[{"level": "Intermediate", "track": "Scale", "abstract": "Digitalization and Industry 4.0 are the current buzzwords in the industrial and\u00a0manufacturing sector. Being a first mover and platform enabler is key to success and to stay ahead of competitors. For this very reason, the ability to unlock the power of data using analytics in creative and effective ways is no longer a nice to have but a must for organisations. This requires forging new partnerships inside and outside your company to accelerate digital innovation. Gopi Krishnamurthy, Data Architect at Bombardier Transporation will showcase how you can stimulate digital innovation by using the power of self-service data preparation and analytics. During this inspirational session, you\u2019ll also get to see real-life use cases in action.\n\nThis talk is sponsored by alteryx.", "title": "Driving digital innovation at Bombardier Transportation through a modern approach to data analytics", "url": "https://2017.berlinbuzzwords.de/17/session/driving-digital-innovation-bombardier-transportation-through-modern-approach-data.html", "speaker": "Gopi Krishnamurthy"}, {"level": "Intermediate", "track": "Search", "abstract": "Search is a core technology that allows Bloomberg to deliver financial news and information quickly and reliably to our clients. The Search Infrastructure team has created a high performance, stable and scalable search ecosystem to support a large, complex and diverse set of search applications.\nProviding search as a service to the thousands of developers in this demanding environment requires us to take a holistic approach. In this talk I'll discuss both the organizational and technical challenges we've encountered and the approach we've taken to solve them. I'll dive into the details of our platform from the way we engage with our tenants, interact with the Solr community, to the infrastructure and tools we use to manage, monitor and scale our platform.\nThis talk is presented by Bloomberg.", "title": "Building a Vibrant Search Ecosystem at Bloomberg", "url": "https://2017.berlinbuzzwords.de/17/session/building-vibrant-search-ecosystem-bloomberg.html", "speaker": "Ken LaPorte"}, {"level": "Advanced", "track": "Scale", "abstract": "Twelve months ago, Cloudability's operations team set out on an ambitious project designed to control AWS spending while simultaneously adopting automation best practices and improving overall efficiency. This talk will highlight lessons learned in optimization of AWS operations resulting in a spending reduction of over 50% while simultaneously doubling the improvement of resource utilization and growing top line company revenue.\nSpecific topics will include Cloudability's use of automation tools including Puppet, Docker and Packer in addition to AWS-specific tools like Lambda functions, Elastic Container Service, the EC2 Spot Market, Auto Scaling Groups and AWS Aurora.\u00a0\nThis talk will cover specific elements of operating a SaaS infrastructure for cloud efficiency but should be applicable to all users of cloud computing.\nThis talk is presented by Cloudability.\n\u00a0", "title": "Optimization of Public Cloud for Efficiency and Scale", "url": "https://2017.berlinbuzzwords.de/17/session/optimization-public-cloud-efficiency-and-scale.html", "speaker": "Erik Onnen"}, {"level": "Beginner", "track": "Search", "abstract": "Community detection in social networks enables to identify densely connected groups of individuals, as well as to dynamically characterise their behavior.\nThis highly innovative field brings new business opportunities for fraud detection, digital marketing and HR analytics.\nAfter an introduction to Graph Theory and community detection algorithms, we will present two scenarios where these concepts are applied to social networks data: Meetup and LinkedIn.\nWill our algorithms manage to identify Data Science communities ?", "title": "Graph Theory : Looking for communities ...", "url": "https://2017.berlinbuzzwords.de/17/session/graph-theory-looking-communities.html", "speaker": "Aur\u00e9lia N\u00e8greAlberto Guggiola"}, {"level": "Intermediate", "track": "Scale", "abstract": "It\u2019s important to know your users\u2019 preferences and behavior in the E-marketplace world. If you can quickly understand who your users are, you can optimize the user journey on the E-marketplace platform by presenting relevant products to the user, and by improving the relevance of search results. One way to leverage user preferences is to calculate a passive user profile based on the user\u2019s interactions with the E-marketplace platform. At mobile.de, Germany's largest online vehicle marketplace, we include in the user profile information such as likelihood of a user to select different car colors, price distribution, mileage distribution, etc.\nThe real challenge is designing a scalable system that can calculate profiles for different users in real-time and serve those profiles via a REST API to other stakeholders.\nAt mobile.de, we reviewed some of the most popular open-source stream processing solutions to consider possible architecture designs for the problem. In a nutshell updating user profile in real time is actually a stateful stream processing system in which the state is the user profile, the state key could be the user ID, and the state update operation can be simple as counter increment or an average/variance update.\nThe main focus of this talk would be: how to store the stream state? Which is one of the most important choices to be made before designing a stateful stream processing system.\nDuring this talk I would compare between a local storage option and an external (global) storage option, and would try to present the different tradeoffs as a result from selecting each of the mentioned options above.\nThis talk is sponsored by our Gold partner ebay tech.", "title": "Design Patterns for Calculating User Profiles in Real Time", "url": "https://2017.berlinbuzzwords.de/17/session/design-patterns-calculating-user-profiles-real-time.html", "speaker": "Igor Mazor"}, {"level": "Beginner", "track": "Scale", "abstract": "Ethical challenges to using data. Currently the gap between data evangelists on one hand, and data sceptics on the other is growing: Can this gap be bridged? How can we build a coherent and constructive argument for the positive use of data science in the world?", "title": "Bridging the gap between data sceptics and data evangelists (at Kino)", "url": "https://2017.berlinbuzzwords.de/17/session/bridging-gap-between-data-sceptics-and-data-evangelists-kino.html", "speaker": "Duncan Ross"}, {"level": "Beginner", "track": "Scale", "abstract": "Viewed in a broader social construct, is free software a movement or should it be? Using free software means making a political and social choice. However, that is not always a choice that is available for everyone. Karen analyses the role of free software and questions if it is just a convenience for the privileged or an essentiality.", "title": "Keynote - Free and Open Source software today (at Kino)", "url": "https://2017.berlinbuzzwords.de/17/session/keynote-free-and-open-source-software-today-kino.html", "speaker": "Karen Sandler"}, {"level": "Intermediate", "track": "Stream", "abstract": "During this Workshop you will learn how to design a scalable IoT application from sensor to data center, including:\nData capture with various sensors\nData transport using various protocol and API, for example Apache Kafka API with MapR Streams\nStream processing using Apache Spark and/or Apache Flink\nData Persistence using Streams, NoSQL Database, and File System\nAnalytics with Apache Drill\nCome with your laptop and start coding your first scalable IoT Application; you will have access to sensors \"the things\", a MapR Cluster to capture, store, process and analyse the data; and sample code to play with the data.\nPrerequisites:\nSome Java Experience\nJava \u00a0Development Kit 1.8 (JDK)\nYour favorite IDE\nMaven\nGit", "title": "Streaming and the Internet of Things: Hands-on Workshop", "url": "https://2017.berlinbuzzwords.de/17/session/streaming-and-internet-things-hands-workshop.html", "speaker": "Tugdual GrallTed Dunning"}, {"level": "Intermediate", "track": "Search", "abstract": "Is your system working? Really? Average response times and throughputs don\u2019t tell the whole story. To really understand what is happening, you probably need measurements like the 99.9%-ile response time. A growing number of systems are using the t-digest to do this. I will explain the algorithm with practical examples, talk about how it is much simpler and faster than before, talk about integration in systems like Elastic, Solar and streamlib, tell some real-world deployment stories and show some pretty pictures.", "title": "Update on the t-digest: Finding Faults in Real Data", "url": "https://2017.berlinbuzzwords.de/17/session/update-t-digest-finding-faults-real-data.html", "speaker": "Ted Dunning"}, {"level": "Beginner", "track": "Stream", "abstract": "There\u2019s a revolution underway in how people work with data. Streaming data is no longer seen as a special use case \u2013 and that\u2019s a good thing because streaming is a better fit to the way life happens. Innovative technologies for robust stream processing are changing what you can reasonably expect to do with stream-based applications, particularly when low latency is required. Apache Flink is one such emerging technology, and its popularity is growing.\nInnovation by those who design and develop new technologies is, however, just one-half of an effective data revolution. It\u2019s not just the people who build the disruptive technologies who must have vision if significant change is to occur: the users of those new technologies must also have vision if the revolution is to have real impact.\nUsing specific examples from a variety of projects involving streaming data, this talk focuses on how innovation with real impact can happen, including the shift in thinking and in engineering culture that underlie successful change in how we work with data at scale. For instance, people are beginning to recognize that stream-first architectures are useful even beyond real-time processing.\u00a0 Another big idea has to do with where data lives: the big data revolution showed us that data structures spanning more than one machine are a good thing -- now a new revolution involves data structures that span more than one continent (geo-distribution) and that go from on-premise to cloud.\nWe\u2019ll also look at how streaming data supports flexible practices such as a microservices style that have huge implications in IoT, in A/B testing, deployment of machine learning models and other large-scale analytical workflows. Finally we will how the right technologies and the right design can make life easier for developers and for system administrators by creating a separation of concerns.\nThis talk should be useful for audiences at all levels of experience.", "title": "The Shape of Revolutions: What Makes a Difference?", "url": "https://2017.berlinbuzzwords.de/17/session/shape-revolutions-what-makes-difference.html", "speaker": "Ellen Friedman"}, {"level": "Intermediate", "track": "Search", "abstract": "Although a lot of online content is written in English there\u2019re tons of non English users out there that still need to retrieve information. When searching, especially for tech related topics, it\u2019s common to compose queries in English; however for such users search results written in their own native language may be preferred.\nWe\u2019ll see how statistical machine translation tools can help in the above scenario to perform text translation at query time, resulting in an improved recall and precision for the search engine queries.\nWe\u2019ll be having a look at how cross language information retrieval can be implemented on top of Apache Lucene with the help of Apache Joshua machine translation toolkit.\nThe audience would gain a better understanding of how to be able to make search queries against a multilingual corpora indexed into Apache Lucene and being able to retrieve all of the relevant search results in different languages.\n", "title": "Embracing diversity: searching over multiple languages", "url": "https://2017.berlinbuzzwords.de/17/session/embracing-diversity-searching-over-multiple-languages.html", "speaker": "Tommaso TeofiliSuneel Marthi"}, {"level": "Beginner", "track": "Scale", "abstract": "Rolling out Enterprise Kubernetes Clouds at SAP\nIn this talk we give insights in how we have set up several Kubernetes clouds for SAP. We tell the whole story, from PoC state, first developer installations, testing until production. We implemented on premises, in internal and external IaaS clouds. We used Docker and Rkt, rolled out database applications and implemented deployment pipelines. Special applications have special needs, so we tweaked parameters. At the end, we implemented several self-installing, self-hosted, self-healing and extendable Kubernetes clusters, which allow application deployment on scale for cutting edge case involving high performance databases and number crunching applications.\n", "title": "Rolling out Enterprise Kubernetes Clouds at SAP", "url": "https://2017.berlinbuzzwords.de/17/session/rolling-out-enterprise-kubernetes-clouds-sap.html", "speaker": "Stefan VetterThomas Fricke"}, {"level": "Advanced", "track": "Scale", "abstract": "This talk focused the topic on how to model data pipelines as retroactive, immutable data structures. It covers the topic of how do you build a data pipelines for a growing organization where different teams depend on each others data and need to be able to re-process data when errors occur upstream. I draw comparisons between the microservice architectures for both stream and batch processings and provide some guiding principals towards building resiliant systems based on experience scaling out infrastructure at SoundCloud.\n", "title": "Mechanics of Data Pipelines", "url": "https://2017.berlinbuzzwords.de/17/session/mechanics-data-pipelines.html", "speaker": "Sean Braithwaite"}, {"level": "Intermediate", "track": "Stream", "abstract": "This talk shares experiences from deploying and tuning Flink steam processing applications for very large scale. We share lessons learned from users, contributors, and our own experiments about running demanding streaming jobs at scale. The talk will explain what aspects currently render a job as particularly demanding, show how to configure and tune a large scale Flink job, and outline what the Flink community is working on to make the out-of-the-box for experience as smooth as possible. We will, for example, dive into - analyzing and tuning checkpointing - selecting and configuring state backends - understanding common bottlenecks - understanding and configuring network parameters\n", "title": "Experiences running Flink at Very Large Scale", "url": "https://2017.berlinbuzzwords.de/17/session/experiences-running-flink-very-large-scale.html", "speaker": "Stephan Ewen"}, {"level": "Intermediate", "track": "Scale", "abstract": "Data scientists love tools like R and Scikit-Learn, as they offer a convenient and familiar syntax for analysis tasks. However, these systems are limited to operating serially on data sets that can fit on a single node and do not allow for distributed execution.\nMahout-Samsara is a linear algebra environment that offers both an easy-to-use Scala DSL and efficient distributed execution for linear algebra operations. Data scientists transitioning from R to Mahout can use the Samsara DSL for large-scale data sets with familiar R-like semantics. Machine Learning and Deep Learning algorithms built with the Mahout-Samsara DSL are automatically parallelized and optimized to execute on distributed processing engines like Apache Spark and Apache Flink accelerated natively by CUDA, OpenCL and OpenMP.\nIn this talk, we will look at Mahout's distributed linear algebra capabilities and demonstrate an EigenFaces classification using Distributed SSVD executing on a GPU cluster. This talk will also demonstrate the ease with which one can roll-out new Machine Learning Algorithms using Mahout-Samsara DSL.\u00a0\nML practitioners will come away from this talk with a better understanding of how Samsara's linear algebra environment can help simplify developing highly scalable, CPU/GPU accelerated ML and DL algorithms by focusing solely on the declarative specification of the algorithm without having to worry about the implementation details of a scalable distributed engine or having to learn to program with native math libraries.\n", "title": "Distributed and Native Hybrid optimizations for Machine Learning Workloads", "url": "https://2017.berlinbuzzwords.de/17/session/distributed-and-native-hybrid-optimizations-machine-learning-workloads.html", "speaker": "Suneel Marthi"}, {"level": "Intermediate", "track": "Scale", "abstract": "Linux containers are increasingly popular with application developers: \u00a0they offer improved elasticity, fault-tolerance, and portability between different public and private clouds, along with an unbeatable development workflow. \u00a0It\u2019s hard to imagine a technology that has had more impact on application developers in the last decade than containers, with the possible exception of ubiquitous analytics. \u00a0Indeed, analytics is no longer a separate workload that occasionally generates reports on things that happened yesterday; instead, it pulses beneath the rhythms of contemporary business and supports today\u2019s most interesting and vital applications. \u00a0Since applications depend on analytic capabilities, it makes good sense to deploy our data-processing frameworks alongside our applications.\nIn this talk, you\u2019ll learn from our expertise deploying Apache Spark and other data-processing frameworks in Linux containers on Kubernetes. \u00a0We\u2019ll explain what containers are and why you should care about them. We'll cover\u00a0the benefits of containerizing applications,\u00a0architectures for analytic applications that make sense in containers, and how to handle external data sources. \u00a0You\u2019ll also get practical advice on how to ensure security and isolation,\u00a0how to achieve high performance,\u00a0and how to sidestep and negotiate potential challenges. \u00a0Throughout the talk, we\u2019ll refer back to concrete lessons we\u2019ve learned about containerized analytic jobs ranging from interactive notebooks to production applications. \u00a0You\u2019ll leave inspired and enabled to deploy high-performance analytic applications without giving up the security you need or the developer-friendly workflow you want.\n\u00a0\n", "title": "The Revolution Will Be Containerized:  Architecting the Insightful Applications of Tomorrow", "url": "https://2017.berlinbuzzwords.de/17/session/revolution-will-be-containerized-architecting-insightful-applications-tomorrow.html", "speaker": "William Benton"}, {"level": "Intermediate", "track": "Stream", "abstract": "In the relational database world, there are different benchmarks to evaluate the correctness and performance of different databases, for example the TPC suites, In this talk we will motivate the need for a benchmark framework to evaluate both stream and batch processing systems.\n\nWe will introduce Nexmark, a framework to evaluate queries over data streams and discuss its implementation on Apache Beam, and the properties that make Apache Beam the perfect tool to develop a benchmark framework. Nexmark was an integration test donated by Google as part of the Apache Beam incubation process and we have been working to evolve it since. Nexmark not only bridges the gap for evaluating data processing frameworks, but also serves as a rich integration test of the correct implementation of both the Beam runners (for systems like Apache Spark, Apache Flink and Apache Apex), and the new features of the Beam SDK that we will also present.", "title": "Using Apache Beam to create a unified benchmarking framework for streaming and batch systems", "url": "https://2017.berlinbuzzwords.de/17/session/using-apache-beam-create-unified-benchmarking-framework-streaming-and-batch-systems.html", "speaker": "Isma\u00ebl Mej\u00eda"}, {"level": "Intermediate", "track": "Search", "abstract": "Information Retrieval (IR) systems are a vital component in the core of successful modern web platforms.\nThe main goal of IR systems is to provide a communication layer that enables customers to establish a retrieval dialogue with underlying data.\nThe immense explosion of unstructured data drives modern search application to go beyond just fuzzy string matching, to invest in deep understanding of user queries through interpretation of user intention in order to respond with a relevant result set.\u00a0\nThe modern architecture of search is a design of a data-driven IR system that covers the following:\u00a0\n\u00a0 \u00a0 \u00a0- Data ingestion pipelines from various sources\n\u00a0 \u00a0 \u00a0- Data retrieval and the lifecycle of a user search query\n\u00a0 \u00a0 \u00a0- Machine learned relevance ranking\u00a0\n\u00a0 \u00a0 \u00a0- Personalised search\n\u00a0 \u00a0 \u00a0- Search performance tracking and quality assessment\nThe talk will discuss the components needed to build an eco-system that is designed to solve the problems of IR in web platforms. What role can Machine learning play in search relevancy? how natural language processing can help provide a solid understanding of search phrases? How data can drive a personalized search experience? What are the challenges of maintaining such a complex system? \u00a0\n\u00a0\n#ml #nlp #ir #search_relevancy #architecture #big_data\n____\nAlae Elhadba\nResearch Engineer @ Zalando\u00a0\n", "title": "The modern architecture of search", "url": "https://2017.berlinbuzzwords.de/17/session/modern-architecture-search.html", "speaker": "Alaa ElhadbaMikio Braun"}, {"level": "Intermediate", "track": "Search", "abstract": "Solr exposes low level internal metrics which applications may consume in their monitoring tools. The talk describes various key metrics which a user should monitor carefully to maintain SLAs, both in terms of ingestion rates and query latency.\nThe second part of the talk presents a cookbook of recipes for admins to use by acting on the metrics. The recipes utilize a set of Solr APIs that will help expand your collection(s) to more nodes, re-shard, add more replicas etc.\nThe third part of this talk covers the work being done in Solr to help users scale\u00a0their cluster in an automated fashion. Ultimately, upon completion of the talk, the user should be able to define a set of rules and provide\u00a0recipes on which Solr may take action when thresholds are hit.\n", "title": "Road to Auto-Scaling", "url": "https://2017.berlinbuzzwords.de/17/session/road-auto-scaling.html", "speaker": "Varun Thacker"}, {"level": "Intermediate", "track": "Stream", "abstract": "There has been so much noise surrounding advances in analytical systems lately that Online Transaction Processing (OLTP) problems may seem a bit overshadowed. Big data, streaming analytics, machine learning, and even deep learning have all changed the way businesses are run and problems are solved. Meanwhile, problems that require low latency, high write throughput and some consistency guarantees haven\u2019t gone away.\nTwo of the biggest \u201cnew\u201d ideas for operational workloads are:\nCommand Query Responsibility Segregation\u00a0suggests splitting operations into parallel streams of queries and modifications.\nSystem like Samza and Kafka Streams propose flipping the persistence mechanism around, using logs to store events and maintaining materializations on top of the logs. This idea is best introduced here.\n\nWhile these approaches blur the line between stream processing and traditional operational databases, the fuzziness is coming to even the most stodgy systems. We now speak of \u201cevents\u201d more than \u201ctransactions\u201d, even if the transaction hasn\u2019t gone away. Systems are becoming more parallel and more and more asynchronous.\nThis talk will do something a little weird: compare streaming systems like Kafka, Flink, DataFlow, and Kinesis with traditional databases, like Postgres and Oracle, or newfangled databases like Cloud Spanner, HBase, MySQL Galera Cluster, or VoltDB. It\u2019s going to be fun.\nWe\u2019ll take example operational problems and show how they would be solved in a database-first environment. Then we\u2019ll flip them around and show how they might be expressed in a streaming-first environment (with or without a stateful database involved). What are the pros and cons? How will I know which approach is best?\nThis talk will contain clear explanations. Sometimes there are also clear answers, and this talk will help you identify those. Other times, the decision will not be obvious, and you\u2019ll have to know what questions to start asking. Good thing you came to this talk!\n\u00a0\n", "title": "The History and Future of the Blurring of Stream Processing & OLTP", "url": "https://2017.berlinbuzzwords.de/17/session/history-and-future-blurring-stream-processing-oltp.html", "speaker": "John Hugg"}, {"level": "Beginner", "track": "Stream", "abstract": "There is a lot to consider when setting up a big data streaming application: How much data will we need to handle? How important are \u201creal time\u201d results? What about constraints on data quality? And how can we deal with various failure scenarios? The open-source world offers numerous big data frameworks that can help process unbounded data, each with its own mechanisms to tackle these problems.\nIn this \"recipe\", I want to show you these frameworks and explain their mechanisms in order to give you some insights on which ingredients you should add to build a big data streaming application that suits your needs.\n", "title": "A Big Data Streaming Recipe", "url": "https://2017.berlinbuzzwords.de/17/session/big-data-streaming-recipe.html", "speaker": "Konstantin Gregor"}, {"level": "Intermediate", "track": "Search", "abstract": "Did you know that Apache Solr is almost 10 years old ? Over the years, a lot of APIs have been added to Solr and as it stands now, they lack consistency. The version 2 APIs in Solr intend to fix this so they look cohesive and consistent. With support for introspection, the new APIs wouldn\u2019t only be intuitive, but would be self documenting.\nIn this presentation, I would provide an overview of the new APIs, what they offer, and also talk about what does it mean for existing systems that use the old APIs.\nThe audience is anyone who uses Apache Solr and wishes to upgrade to newer versions. Attendees can expect a detailed update on the reason behind this effort, the release plan, what does it mean for existing users, and also details about what more do these APIs bring to the table, as compared to the existing APIs for Solr users.\n", "title": "Introduction to V2 APIs in Apache Solr", "url": "https://2017.berlinbuzzwords.de/17/session/introduction-v2-apis-apache-solr.html", "speaker": "Anshum Gupta"}, {"level": "Intermediate", "track": "Search", "abstract": "At the beginning of the year 2017, the Apache Lucene team decided to focus on releasing Apache Lucene 7. Around Berlin Buzzwords, the new version will be available for testing.\nThis talk will present the new and changed features of Lucene 7: As TF-IDF is no longer the default,\u00a0several query special cases like query normalization and the so-called \"coord factor\" were removed. Those were workaround for problems that are specific to TF-IDF like not\u00a0strong enough term frequency saturation, but can be completely ignored with other ranking functions like BM25. The user has to be prepared that scores may differ and the absolute values of scores are meaningless, breaking applications.\u00a0The problem with query normalization and coordination factors was correct query rewriting, but now many more optimizations can be done to handle optional, filtered, and mandatory query clauses:\u00a0Lucene 7 will be faster if it finds duplicate clauses. The talk will also present recent Lucene 6 features like graph token streams and how they are used in Lucene 7.\nThe talk will also present future plans to support the Java 9 module system and the current state of Java 9 support inside Apache Lucene, because it is expected that Lucene/Solr and Elasticsearch users will one of the first communities that will\u00a0migrate to Java 9, because recent hotspot optimizations will execute queries and allow doc values access with much higher performance.\n", "title": "Apache Lucene 7 - What's coming next?", "url": "https://2017.berlinbuzzwords.de/17/session/apache-lucene-7-whats-coming-next.html", "speaker": "Uwe Schindler"}, {"level": "Intermediate", "track": "Scale", "abstract": "Big Data technologies like distributed databases, queues, batch processors, and stream processors are fun and exciting to play with. Making them play nicely together can be challenging. Keeping it fun for engineers to continuously improve and operate them is hard. At ResearchGate, we run thousands of YARN applications every day to gain insights and to power user facing features. Of course, there are numerous integration challenges on the way:\nintegrating batch and stream processors with operational systems\ningesting data and playing back results while controlling performance crosstalk\nrolling out new versions of synchronous, stream, and batch applications and their respective data schemas\ncontrolling the amount of glue and adapter code between different technologies\nmodeling cross-flow dependencies while handling failures gracefully and limiting their repercussions\n\nIn this talk we will discuss how ResearchGate has tackled those problems. We describe our ongoing journey in identifying patterns and principles to make our big data stack integrate well. Technologies to be covered will include MongoDB, Kafka, Hadoop (YARN), Hive (TEZ), Flink Batch, and Flink Streaming.\n", "title": "Integration Patterns for Big Data Applications", "url": "https://2017.berlinbuzzwords.de/17/session/integration-patterns-big-data-applications.html", "speaker": "Michael H\u00e4usler"}, {"level": "Intermediate", "track": "Scale", "abstract": "The first 365 days of a relationship are full of discoveries, learnings and lessons! And that's also true with significant technological shifts. Apache Spark has rapidly transformed the data platform landscape, and has recently reached a 2.0 version, completing an important development cycle. But how to really succeed at rolling Spark into an organization? How to support various use-cases, from machine learning and data products all the way to analytics or reporting? And how to do that step by step, gracefully and efficiently?\nI would like to share my experience and learning from a first and intense year moving Spark into production at GetYourGuide, a Berlin startup of 250+ employees in the travel space, with lots of data to analyse and crunch. Whether the starting point is a legacy data warehouse, a Hadoop infrastructure or anything in between, the \"playbook\" I would like to present aims\u00a0to touch on the following important questions and topics:\nHow to support SQL-based use-cases, as well as complex machine learning on one platform\nHow to integrate an existing data warehouse into the rollout strategy\nWhich platform to choose, and what needed resources to expect?\nCan everything be done with the Dataset API, are RDD still relevant?\nHow to get started with streaming?\nHow to best organize your data, moving from relational tables to a data lake on S3\nHow to integrate with existing BI tools? (e.g. Looker)\n\nThis presentation will be informative for whoever is currently planning or executing a migration to Spark. It will highlight intermediate technical topics for newcomers, as well as tips and advices to have a successfull first year relationship with Spark!\n", "title": "365 days of Spark!", "url": "https://2017.berlinbuzzwords.de/17/session/365-days-spark.html", "speaker": "Mathieu Bastian"}, {"level": "Beginner", "track": "Stream", "abstract": "Pattern matching over event streams is increasingly being employed in many areas including financial services and click stream analysis. Flink, as a true stream processing engine, emerges as a natural candidate for these usecases. In this talk, we will present FlinkCEP, a library for Complex Event Processing (CEP) based on Flink. At the conceptual level, we will see the different patterns the library can support, we will present the main building blocks we implemented to support them, and we will discuss possible future additions that will further enhance the coverage of the library. At the practical level, we will show how the integration of FlinkCEP with Flink allows the former to take advantage of Flink's rich ecosystem (e.g. connectors) and its stream processing capabilities, such as support for event-time processing, exactly-once state semantics, fault-tolerance, savepoints and high throughput.\n", "title": "Complex Event Processing with Flink: the state of FlinkCEP", "url": "https://2017.berlinbuzzwords.de/17/session/complex-event-processing-flink-state-flinkcep.html", "speaker": "Kostas Kloudas"}, {"level": "Intermediate", "track": "Search", "abstract": "Learning to Rank uses machine learning to improve the relevance of search results. In this talk, I discuss how we built a learning to rank plugin for Elasticsearch. But what's more interesting is what happened next. Learning to rank requires new ways of thinking about search relevance, and in this talk I go on to discuss the specific problems faced by production-ready learning to rank systems. We learned these hard way so you don't have to. These systems need to solve a variety of problems including:\nCorrectly measuring, using analytics, what a user deems \"relevant\" or \"irrelevant\"\nHypothesizing which features of users, queries, or documents (or query-user dependent features) might correlate to relevance\nLogging/Gathering hypothesized features using the search engine\nTraining models in a scalable fashion\nSelecting and evaluate models for appropriateness and minimal error\nIntegrating models in a live search system alongside business logic, and other non-relevance considerations\nA/B testing learning to rank models and avoiding future bias of training data\n\nEach of these requires solving pretty tough problems. This talk will discuss our war stories, practical lessons, and the goings-on inside real life search implementations that can help you decide what pitfalls to avoid and decide whether learning to rank is the right direction for your search problem.\n", "title": "We built an\u00a0Elasticsearch Learning to Rank plugin. Then came the hard part.", "url": "https://2017.berlinbuzzwords.de/17/session/we-built-elasticsearch-learning-rank-plugin-then-came-hard-part.html", "speaker": "Doug TurnbullJason Kowalewski"}, {"level": "Intermediate", "track": "Stream", "abstract": "Spam is defined as the unauthorized use of a platform to send unsolicited content, usually in the form of advertising, to its user base with the purpose of generating views and revenue. LOVOO is a dating platform with over 50 millions registered users, making it attractive to spammers. Due to the nature of the service (a dating platform) many of the spammers disguise themselves as real users, using believable images and genuine descriptions on their profiles to lure our users into believing they are real people. To achieve the goal of keeping our platform free of spam, LOVOO\u2019s dedicated anti-spam team relies heavily on machine learning and massive real time data processing. So far, we are winning this fight. As seen in our latest Fake and Spam Transparency Report, on a daily basis, only 0.3 percent of the active users are spammers, and on average it takes around 2.2 hours to detect them.\nIn this presentation I will talk about how the anti-spam platform works, and will present some insights and statistics about our findings.\n", "title": "Anti-spam and Machine Learning at LOVOO", "url": "https://2017.berlinbuzzwords.de/17/session/anti-spam-and-machine-learning-lovoo.html", "speaker": "Juan De Dios Santos Rivera"}, {"level": "Intermediate", "track": "Search", "abstract": "This talk gives an introduction to an often used but little thought about feature of fulltext search systems: did-you-mean (aka spellcheck). Whenever you misspell a query this useful feature kicks in and and will show you the query you really meant - or won't it?\nWe will highlight potential requirements to a did-you-mean feature and identify their difficulties. We will discuss the implementations available in Solr and Elasticsearch. But there are also alternative algorithms and data structures used in real world search projects which we will show. What is the difference between all these approaches and which one should you use when?", "title": "Did-you-mean", "url": "https://2017.berlinbuzzwords.de/17/session/did-you-mean.html", "speaker": "Bastian Mathes"}, {"level": "Intermediate", "track": "Store", "abstract": "We software developers take for granted the notion of \u00a0\"a filesystem\", with its paths, directories, files and operations. Yet when it comes to distributed filesystems, those notions built from years of using desktop systems actually constraining us to a metaphor which is no longer sustainable\nThis talk looks at our foundational preconceptions from the perspective of trying to define a single operation in Hadoop HDFS, rename(), what it takes to implement it in a distributed filesystem \u2014and what has to be done to mimic that behaviour when working with an object store.\nPreconceptions about rename()'s semantics are deeply embedded in large scale applications such as Apache MapReduce, Apache Hive, Apache Spark and the like, being the operation used to atomically commit work \u2014and so do not work the way we think they do on Object Stores like Amazon S3.\nWe have to rethink our strategies for committing distributed work, with Hadoop's new \"S3Guard committer\" being the example of the world we have to move to. The time of renaming files is over.\n(Warning: This talk contains formal set-theoretic specifications of distributed system semantics, though it is disguised of Python code so developers can understand it rather than run away)\n", "title": "What does rename() do?", "url": "https://2017.berlinbuzzwords.de/17/session/what-does-rename-do.html", "speaker": "Steve Loughran"}, {"level": "Beginner", "track": "Scale", "abstract": "How do you measure the behavior of users firing 3,000 requests per second?\nAt bol.com, the biggest online retail platform in the Netherlands and Flanders, there are thousands of users visiting the site daily, and my team provides the measurement infrastructure providing near real time metrics in the interactions of these customers with the platform.\nOur new measuring system, Measuring 2.0 is built using open-source big data technologies like Apache Flink, Kafka, Avro, and Parquet, supported with in-house technology.\nAt Berlin Buzzwords 2016, Niels Basjes introduced this project, for which we can now share technical details of running it in production. During the talk I will guide you through how we kept our Kafka cluster resilient enough to handle our peak loads, how we used Apache Flink to show trending products on our website in real-time, how we handle continuous changes in our measurement data structure using Avro, how we use Parquet to deal with the great quantities of data while also keeping them easily accessible, and share the lessons we learned along the way.\n", "title": "Half a year of Measuring 2.0 in production at bol.com", "url": "https://2017.berlinbuzzwords.de/17/session/half-year-measuring-20-production-bolcom.html", "speaker": "Ivan Budincevic"}, {"level": "Intermediate", "track": "Search", "abstract": "Learning to rank is a technique for automatically improving the quality of results produced by a search engine. It was initially proposed in academia around 17 years ago and\u00a0almost all commercial web search engines\u00a0 employ it in some form or other. At Bloomberg, we decided that it was time for an open source engine to support learning to rank, so we spent more than a year designing and implementing it. The results of our effort have been accepted by the\u00a0community and\u00a0our Learning to Rank plugin is now available in the latest release of Apache Solr (version 6.4). In this talk we will explain how learning to rank works, how we implemented and integrated it into Apache Solr, and how you can use it to improve the quality of the results in your search engine.\n", "title": "Apache Solr Learning to Rank For The Win!", "url": "https://2017.berlinbuzzwords.de/17/session/apache-solr-learning-rank-win.html", "speaker": "Diego CeccarelliMichael Nilsson"}, {"level": "Intermediate", "track": "Scale", "abstract": "For those getting started in AI, finding an interesting yet solvable problem can often be one of the greatest challenges. A project needs a set of data or stimuli that is rich yet easy to collect and a relevant problem that motivates the practitioner. \u00a0If hoping to share the project with a diverse audience, it must not require extensive background industry knowledge. Data from Airbnb\u2019s API is a \u201cone-stop-shop\u201d for new practitioners to experiment with a very diverse set of techniques and methods from the AI/Machine Learning canon. This tutorial introduces the Airbnb API, shows how to collect data from it, and presents a collection of examples of how artificial intelligence tools and algorithms can be used to extract value from the data.\nThis tutorial is motivated by the speaker\u2019s own experience--he has a spare apartment and a guest apartment in the Logan Square neighborhood of Chicago that he began renting on Airbnb. \u00a0Trapped indoors throughout the savage Chicago winter, he spent his time building a bot that seeks to optimize the listing price and apartment description for his listing. The success of this project has led the speaker to consider purchasing another property in the neighboring Avondale neighborhood as an Airbnb investment property.\nThis tutorial will present the interesting highlights of the project, to include the following.\nA Python script collects the listings available for each of the next 60 days. \u00a0Each listing has structured information such as the offered price, number of bedrooms, unit rating, host rating, etc. and unstructured information such as a title, description of the unit, and pictures.\nUsing Apache Spark, the JSON data is loaded into SparkSQL tables. Advanced querying techniques in SparkSQL are leveraged to convert semi-structured JSONs into structured tables for consumption by machine learning algorithms. Additional columns such as \u201crented units\u201d and \u201cprice\u201d are inferred if a unit was \u201cavailable\u201d yesterday, but is not listed today.\nMethods for creating demand curves will be presented, such as a mixed-effects model using R and then Apache Mahout, as well as random-forests and multi-layer perceptrons using SparkML. \u00a0The usefulness of single-node machine learning libraries such as Python\u2019s sklearn are also demonstrated, as well as at what scale such methods become unrealistic, providing motivation for using \u201cBig Data\u201d tools.\nText analysis on listing title and description is performed, as well as image feature recognition in listing images, using the \u201cfixed effects\u201d of previous models as a dependent variable. \u00a0A simple machine is created for helping the user \u2018score\u2019 the quality of their own listing description. \u00a0We will also present a toy application that uses images and facts about the apartment to algorithmically generate \u201coptimal\u201d listing descriptions using a long short-term memory neural network.\nFinally, a bot finds an optimal schedule of prices for the next 60 days for all of our listings, and uses the Airbnb API to update our listing prices accordingly. We will explore various methods of creating such a bot, and how to incorporate machine learning findings.\nThe delayed reward problem is particularly applicable here because there are N days until a listing can possibly be rented. \u00a0This is also a good example of exploration-exploitation as the bot must weigh trade-offs between lowering the price to increase the probability of renting the unit versus raising the price to increase revenue. \u00a0Finally, since each day in the future will only happen once and there are a limited number of days until the rental event happens, this is also a good fit for the generalization problem.\nAttendees will come away from this tutorial with a number of exciting ideas for machine learning and artificial intelligence projects which fit together into a nice macro project, where each sub project can be explored at a depth ranging from beginner to advanced. \u00a0This tutorial is great for autodidacts seeking a plan of study that exposes them to a diverse set of topics in the space, as well as those considering teaching a full course on the subject. \u00a0Working starter code/instructions will be available.\n", "title": "Weekend Project: Real World AirBnB Data Science and Pricing Bot", "url": "https://2017.berlinbuzzwords.de/17/session/weekend-project-real-world-airbnb-data-science-and-pricing-bot.html", "speaker": "Trevor GrantAndrew Weiner"}, {"level": "Intermediate", "track": "Scale", "abstract": "With the rise of data-intensive applications, privacy and personal integrity has become a focus topic. Although companies may have incentive to collect all available data forever, privacy regulations act counter balance. Regulations limit the data that may be stored, for how long it may be stored, how access is given, and give users rights to have their data deleted and get information about the data stored by companies. The regulations put constraints on technical solutions, and makes it challenging to architect and implement systems that allow engineers to efficiently make beneficial use of sensitive data. Unfortunately, failures to properly protect privacy can be very expensive, since the work required to rework core data models and wash tainted data can be massive.\nThis talk provides an engineering perspective on privacy protection. The intended audience is architects, developers, data scientists, and engineering managers that build applications handling user data. We highlight topics that require attention at an early design stage, and go through pitfalls and potentially expensive architectural mistakes. We describe a number of technical patterns for complying with privacy regulations without sacrificing the ability to use data for product features. The content of the talk is based on real world experience from handling privacy protection in large scale data processing environments. \u00a0\n", "title": "Protecting Privacy in Practice", "url": "https://2017.berlinbuzzwords.de/17/session/protecting-privacy-practice.html", "speaker": "Lars Albertsson"}, {"level": "Advanced", "track": "Scale", "abstract": "The beauty of digital advertising is the capability for per-transaction measurement and optimization, but in order to make that a reality, customers need to know what happened in the sea of billions of\u00a0transactions. They want slick visualizations, interactive query times, and drill-down capabilities for fine-grained analysis. \u00a0Learn how we evolved our platform from a fixed set of tabular reports with a basic UI to a\u00a0truly responsive and immersive reporting experience using classic data warehouse techniques, optimized techniques for specialized data sets, and new cloud platforms that address a new reality.\nManaging a global footprint serving 10s of billions of transactions per day is challenging, but moving, aggregating, processing, and reporting on all of that data presents an even larger challenge. \u00a0Gone are the\u00a0days when coarse dimensions, fixed report ranges, and multi-second load times. Customers want to be able to ask relatively open-ended questions across full data sets, customize the reports, drill down, and\u00a0get responses in seconds. \u00a0Join us as we explore our journey from a fixed set of basic reports to a much more interactive experience that better meets our customers needs, all while scaling total volume 10x in\u00a02 years.\nAlong the way, we'll discuss:\nData transport and disaster recovery logistics from world wide data centers using Kafka, Secor, and Camus\nThe evolution of batch processing from Pig to native Map Reduce, and the framework that enabled 6x performance gains\nThe integration of Spark streaming data flows for real time processing, data enrichment, and analytics\nLeveraging new cloud-based data warehouses and technologies to iterate faster and avoid re-inventing the wheel\nRe-imagining data models and processing flows to optimize performance\nHard lessons learned about managing machines, data, and hybrid technology platforms at scale.\n", "title": "Aggregating, reporting, and visualizing 10s of billions of records per day from a global footprint", "url": "https://2017.berlinbuzzwords.de/17/session/aggregating-reporting-and-visualizing-10s-billions-records-day-global-footprint.html", "speaker": "Frank Conrad"}, {"level": "Intermediate", "track": "Stream", "abstract": "This talk will provide a hands on introduction to Apache NiFi and MiNiFi. Focusing on how to use them to securely, efficiently, and reliably collect data from the edge, curate and deliver it to other tiers in a streaming pipeline. This will be a very interactive session to ensure all participants understand the how and why of Apache NiFi and MiNiFi. We will cover the following topics:\nApache NiFi Overview\nDescribe Apache NiFi and its use cases.\nDescribe NiFi Architecture\n\nCore Concepts\nUnderstand Nifi Features and Characteristics.\nUnderstand NiFi user interface in depth.\nUnderstand Processors\nUnderstand Connections\nUnderstand Processor Groups\nUnderstand Remote Processor Groups\nExplain Data Provenance in NiFi\nUnderstand Concepts of NiFi Cluster\n\nDesigning Data Flows\nUnderstand how to build a DataFlow using NiFi\nLearn how to optimize a DataFlow\nLearn how to use NiFi Expression language and its use.\nLearn about Attributes and Templates in NiFi\n\nMiNiFi\nOverview\nUse Cases\nIntegrating with NiFi\n", "title": "Hands-on with Apache NiFi and MiNiFi", "url": "https://2017.berlinbuzzwords.de/17/session/hands-apache-nifi-and-minifi.html", "speaker": "Andrew Psaltis"}, {"level": "Intermediate", "track": "Stream", "abstract": "SQL is undoubtedly the most widely used language for data analytics. It is declarative, many database systems and query processors feature advanced query optimizers and highly efficient execution engines, and last but not least it is the standard that everybody knows and uses. With stream processing technology becoming mainstream a question arises: \u201cWhy isn\u2019t SQL widely supported by open source stream processors?\u201d. One answer is that SQL\u2019s semantics and syntax have not been designed with the characteristics of streaming data in mind. Consequently, systems that want to provide support for SQL on data streams have to overcome a conceptual gap.\nApache Flink is a distributed stream processing system. Due to its support for event-time processing, exactly-once state semantics, and its high throughput capabilities, Flink is very well suited for streaming analytics. Since about a year, the Flink community is working on two relational APIs for unified stream and batch processing, the Table API and SQL. The Table API is a language-integrated relational API and the SQL interface is compliant with standard SQL. Both APIs are semantically compatible and share the same optimization and execution path based on Apache Calcite. A core principle of both APIs is to provide the same semantics for batch and streaming data sources, meaning that a query should compute the same result regardless whether it was executed on a static data set, such as a file, or on a data stream, like a Kafka topic.\nIn this talk we present the semantics of Apache Flink\u2019s relational APIs for stream analytics. We discuss their conceptual model and showcase their usage. The central concept of these APIs are dynamic tables. We explain how streams are converted into dynamic tables and vice versa without losing information due to the stream-table duality. Relational queries on dynamic tables behave similar to materialized view definitions and produce new dynamic tables. We show how dynamic tables are converted back into changelog streams or are written as materialized views to external systems, such as Apache Kafka or Apache Cassandra, and are updated in place with low latency.", "title": "Stream Analytics with SQL on Apache Flink", "url": "https://2017.berlinbuzzwords.de/17/session/stream-analytics-sql-apache-flink.html", "speaker": "Fabian Hueske"}, {"level": "Intermediate", "track": "Stream", "abstract": "In this talk I will present how we enable distributed, Unix style programming using Docker and Apache Kafka. We will show how we can take the famous Unix Pipe Pattern and apply it to a Distributed Computing System. We will demonstrate the development of simple applications with the focus on \"Do One Thing and Do It Well.\" Afterwards we demonstrate how we make these two programs work to together using Apache Kafka. By encapsulating our applications in containers we will also show how that enables us to go from the limited resources of a development machine to cluster of computers in a data center without changing our applications or containers.\n", "title": "Containerizing Distributed Pipes", "url": "https://2017.berlinbuzzwords.de/17/session/containerizing-distributed-pipes.html", "speaker": "Hagen T\u00f6nnies"}, {"level": "Intermediate", "track": "Stream", "abstract": "In a time not too long ago, we just used SQL for\u00a0everything. Thank god that\u2019s over, but while we\u2019re blessed with loads of different NoSQL data stores that all have their use cases, we need to figure out how to keep data between these stores.\nThe premise is easy: You listen for changes in one database, and propagate them to one or more other databases. It turns out that this is one of the problems that is Harder Than It Looks\u2122.\nIn this talk I\u2019ll do a quick intro on Apache Kafka and Kafka Streams, and then start to look at concrete examples why we need something like Kafka Streams to deal with this database diversity.\nFinally, if we follow the thread of this architecture, it gets a bit meta and we end up with concepts like CQRS and Event Sourcing. It will change the way you think about data and replication. Promise.\n", "title": "Embracing Database Diversity", "url": "https://2017.berlinbuzzwords.de/17/session/embracing-database-diversity.html", "speaker": "Frank Lyaruu"}, {"level": "Intermediate", "track": "Scale", "abstract": "Word2vec is an unsupervised algorithm which is able to represent words as vectors, the so-called word embeddings. It computes them so that words with close meanings are close together in the vectorial space. Originally developed for NLP applications, it has recently proven to be extremely relevant in numerous contexts such as biology, speech recognition, recommendation, etc.\nAt Criteo, an ad retargeting company, we use word2vec to compute product embeddings. This allows us to compute similarities between products and consequently improve the relevance of the products we recommend to our users.\nSpark MLlib provides a distributed implementation of Word2vec. However, using it daily on tens of billions of item as we do, raises challenges in terms of scalability and quality. In this talkI will share with you how we moved from POC implementation to ABTest. I will tell you the numerous things we broke during this journey and how we fixed them. Finally, I will expose how we plan to improve our product embeddings and use them in different ways.\n", "title": "Feeding Word2vec with tens of billions of items, what could possibly go wrong?", "url": "https://2017.berlinbuzzwords.de/17/session/feeding-word2vec-tens-billions-items-what-could-possibly-go-wrong.html", "speaker": "Simon Doll\u00e9"}, {"level": "Intermediate", "track": "Search", "abstract": "Learning to Rank (LTR) is gaining popularity as a method to personalize and improve the ranking of search results. I will outline how LTR can be used to improve the ranking in a faceted search system,\u00a0based on an implementation of me and my colleagues - and focus upon some gaps between theory and practice.\nThe most important ones being that virtually all literature about LTR assumes a direct search approach where both the query and document are represented as a bag of words, and that the machine-learned model often suffices being a simple linear function based on language- and retrieval-model features. In case of faceted search applications, where users\u00a0search\u00a0on several different types of dimensions - like brand, location, price, recency etc. - applying this standard approach to LTR will barely go beyond an adjustment of facet field weights.\nI will explain how we integrated LTR in Textkernel's faceted search system that matches vacancies and CV's, where queries and documents may have many dimensions of different types.\u00a0I\u2019ll outline how we use LTR for more than tuning facet field weights by selecting and crafting the right algorithms and features. I\u2019ll also focus upon how to efficiently extract features from queries and documents and apply reranking models with minimal impact on execution times. Finally, I\u2019ll touch upon how a search interface or data from logs can be used to get satisfactory user feedback to keep improving the models.\nNo past experience of machine learning will be required. Expect to leave this talk with an understanding of how you could integrate reranking in your own system and what pitfalls you might encounter.", "title": "Learning to Rank for Faceted Search: Bridging the gap between theory and practice", "url": "https://2017.berlinbuzzwords.de/17/session/learning-rank-faceted-search-bridging-gap-between-theory-and-practice.html", "speaker": "Agnes van Belle"}, {"level": "Intermediate", "track": "Stream", "abstract": "Hops is a new European version of Hadoop that introduces new concepts to Hadoop to enable multi-tenant Streaming-as-a-Service. In particular, Hops introduces the abstractions: projects, datasets and users. Projects are containers for datasets and users, and are aimed at removing the need for users to manage and launch clusters today, as clusters are currently the only strong mechanims for isolating users and their data from one another.\nIn this talk we will discuss the challenges in building multi-tenant streaming applications on both Spark and Flink over YARN using Hops concepts. Our platform, called Hopsworks, is in an entirely UI-driven environment built with only open-source software. We also show how we use the ELK stack (Elasticsearch, Logstash, and Kibana) for logging and debugging running Spark streaming applications, how we use Grafana and InfluxDB for monitoring Spark streaming applications, and finally how Apache Zeppelin can provide interactive visualizations and charts to end-users. We will also show how applications are run within a 'project' on a YARN cluster with the novel property that applications are metered and charged to projects. We will also discuss our experiences running streaming-as-a-service on a cluster in Sweden with over 150 users (as of early 2017). \u00a0\n", "title": "Hops: Multi-Tenancy and Streaming-First in an open-source SaaS platform", "url": "https://2017.berlinbuzzwords.de/17/session/hops-multi-tenancy-and-streaming-first-open-source-saas-platform.html", "speaker": "Jim Dowling"}, {"level": "Beginner", "track": "Stream", "abstract": "Traditionally, big data applications rely on the Lambda Architecture in order to achieve low latency as well as completeness. A streaming layer provides real-time previews while a complementary batch layer retrospectively recomputes the correct results. Using a robust stream processor like Apache Flink, we can do without the latter. But can we take it even one step further? This talk will discuss one of the upcoming features of Apache Flink with the potential to do just that.\nAs a real-world example we have built a prototype for a robust billing system based on Flink and Queryable State. On the one hand, the system exposes the current monthly subtotals in real-time to front-end applications, on the other hand it reports the complete results to downstream systems, e.g. for invoicing. As completeness and correctness are core requirements for a billing system, we will demonstrate the system in multiple failure scenarios, including taskmanager and jobmanager failures as well as unavailability of downstream systems.\nThis talk will give you an idea of how \"Queryable State\" combined with a robust stream processor enables new streaming use\u00a0cases and changes the future of streaming application architecture.\n", "title": "Queryable State or How to Build a Billing System Without a Database", "url": "https://2017.berlinbuzzwords.de/17/session/queryable-state-or-how-build-billing-system-without-database.html", "speaker": "Maximilian BodeKonstantin Knauf"}, {"level": "Intermediate", "track": "Search", "abstract": "A user types \u201cblack clutch\u201d into your search engine. Do they mean the handbag, the automobile part, or something else entirely?\nSearch is about matching the intent of the user with the information they need. For decades, \u201crelevance\u201d in information retrieval systems has meant things like BM25, TFIDF, field boosting, document boosting, etc. These simple heuristics and strategies have served us well, but ultimately fall short because they fail to semantically model intent. Our systems don\u2019t actually understand what users want, they just hope a few magic numbers will get us close enough.\nQuery Understanding is about using real intelligence to put users first. In this session, we\u2019ll talk about what Query Understanding is, why it\u2019s important, and some practical strategies for making your search experience smarter.\n", "title": "Getting Started with Query Understanding", "url": "https://2017.berlinbuzzwords.de/17/session/getting-started-query-understanding.html", "speaker": ""}, {"level": "Intermediate", "track": "Stream", "abstract": "Balancing Heroes and Pokemon in Real TimeA Streaming Variant of Trueskill for Online Ranking\u00a0\nIn this talk we will demonstrate a matchmaking system for online video games that needs to work in a streaming setting. In particular we will demonstrate a solution to the following problems;\u00a0\n- How can you estimate the skill of a video game player in an online setting? Note that this needs to work for one vs. one player games as well as games with a team setting.\u00a0\n- Given these skill estimations, how can you match them such that each player is always playing against a similar skill leven and doesn't need to wait very long. Note that this needs to work in a distributed session as well.\u00a0\nTo demonstrate an easy setting we will demonstrate how we are able to rank pokemon in one vs. one matches.\u00a0 To demonstrate a harder setting we will streaming game logs from heroes of the storm into our algorithm to show how it works. The stack we use is apache flink together with elasticsearch and kibana. We intend to demonstrate a solution to this problem both on an engineering perspective (mainly handled by Fokko) as well as a machine learning perspective (mainly handled by Vincent).\u00a0\nThe skill estimation algorithm can in part be found described on Vincent's blog: http://koaning.io/pokemon-recommendations-part-2.html\n", "title": "Balancing Heroes and Pokemon in Real Time: A Streaming Variant of Trueskill for Online Ranking", "url": "https://2017.berlinbuzzwords.de/17/session/balancing-heroes-and-pokemon-real-time-streaming-variant-trueskill-online-ranking.html", "speaker": "Fokko DriesprongVincent Warmerdam"}, {"level": "Beginner", "track": "Stream", "abstract": "Dealing with real-time, in-memory, streaming data is a unique challenge and with the advent of the smartphone and IoT (trillions of internet connected devices), we are witnessing an exponential growth in data at scale.\nBuilding data layers that can satisfy these requirements can be challenging, but with the help of Infinispan, an in-memory data grid from Red Hat, you can take advantage of state of the art distributed data processing capabilities to tackle these challenges. From classic or full-text queries, to Spark/Hadoop integrations via distributed Java Streams, these wide ranging data processing capabilities make Infinispan the perfect choice for the Big Data era.\nIn this session, we will identify critical patterns and principles that will help you achieve greater scale and response speed. On top of that, you will witness how Infinispan follows these patterns and principles to tackle a big data situation via a live coding demonstration.\n\u00a0\n", "title": "Big Data In Action with Infinispan", "url": "https://2017.berlinbuzzwords.de/17/session/big-data-action-infinispan.html", "speaker": "Galder Zamarre\u00f1o"}, {"level": "Intermediate", "track": "Store", "abstract": "Instead of using ETL Tools, which consume tons of memory on their own system, you will learn how to do ETL jobs directly in and with a database: PostgreSQL.\nPostgreSQL Management of External Data (SQL/MED) is also known as Foreign Data Wrapper (FDW). With FDW, there is nearly no limit of external data, that you could use directly inside a PostgreSQL database.\nThis talk will show you how to use them with examples accessing several data sources.\n", "title": "One Database to  Rule \u2018em all", "url": "https://2017.berlinbuzzwords.de/17/session/one-database-rule-em-all.html", "speaker": "Stefanie Janine St\u00f6lting"}, {"level": "Intermediate", "track": "Search", "abstract": "As the world's leading provider of financial news, Bloomberg LP ingests on the order of 1 million news stories per day from over 100 thousand sources in over 40 languages. To facilitate users' ability to quickly retrieve news tailored to their specific interests, stories are run through a classification system containing hundreds of thousands of rules where they are tagged in real-time with a mean latency of under 50ms. In this talk I'll discuss the migration of the news classification engine from a legacy system to a solution based on Luwak/Lucene while retaining the query language of the existing corpus of rules. \u00a0\n", "title": "Migrating a Real-time News Classification Engine to Luwak/Lucene", "url": "https://2017.berlinbuzzwords.de/17/session/migrating-real-time-news-classification-engine-luwaklucene.html", "speaker": "Marvin Justice"}, {"level": "Intermediate", "track": "Store", "abstract": "The majority of production Hadoop clusters today run in a secure environment. Most of them (from our experience) contain sensitive data that needs to be protected. The Hadoop ecosystem provides lots of choices but there is no single solution, piece of documentation or book to cover everything needed to secure your Hadoop environment. I\u2019m going to talk about a comprehensive security design we created for one of our customers.\nThis security concept covers everything from the Operating System layer, to authentication, authorization, auditing, encryption, data masking, row & column based access, Backup & DR, acquisition processes and more.\nThe technologies involved are fairly standard but I\u2019m going to talk about CDH, HDP and BigInsights/IOP including tools like SPSS, Qlik and Informatica which you'll frequently find in use at your customers or in your own organization.\nIn this talk I\u2019ll teach you all the lessons that we have learned while securing Hadoop environments. It is much more than just enabling Kerberos and includes lots of organizational red tape to get approvals and information. And I hope that you can avoid a few of these pitfalls next time you have to do it after you\u2019ve listened to this talk.\nThis talk is combining business and technical aspects without boring the other half so everyone is welcome and should get something out of it.\n", "title": "Building a fence around your Hadoop environments", "url": "https://2017.berlinbuzzwords.de/17/session/building-fence-around-your-hadoop-environments.html", "speaker": "Lars Francke"}, {"level": "Intermediate", "track": "Scale", "abstract": "We think in words, we talk with words, we understand the world thanks to words. Metaphors take words to the next level explaining concepts that were escaping our understanding before. In 1980 George Lakoff revolutionised the Linguistic and Philosophic worlds when he studied how metaphors affect our thinking, how they influence our actions and even shape who we are. What happens with the metaphors that we use in the Software Industry?\nIn this talk we are going to review the importance that metaphors have in our code quality, in the algorithms we choose, and the products we ship.\u00a0\nAs a practical example we are going to see why Microservices and Containers have been so successful in the past couple of years. We'll try to understand why they have redefined how we package and ship products in our industry.\n", "title": "Metaphors We Compute By", "url": "https://2017.berlinbuzzwords.de/17/session/metaphors-we-compute.html", "speaker": "Alvaro Videla"}, {"level": "Intermediate", "track": "Scale", "abstract": "Apache Spark is one the most popular general purpose distributed systems in the past few years. Apache Spark has APIs in Scala, Java, Python and more recently a few different attempts to provide support for R, C#, and Julia.\nThis talk covers the core concepts of Apache Spark, but then switches gears into how the \"magic\" of Spark can have unintended consequences on your programs. You will gain a better understanding of\u00a0impact of lazy evaluation, the shift away from arbitrary lambda expressions to \"statements\", and of course the \"dreaded\" shuffle (plus a few more concepts). Based on that the presenter will then shamelessly try and self-promote her own book (for one slide I promise :p) and then look at\u00a0(free) resources for learning more about the magic behind Spark.\n", "title": "A Brief Tour of the Magic Behind Apache Spark", "url": "https://2017.berlinbuzzwords.de/17/session/brief-tour-magic-behind-apache-spark.html", "speaker": "Holden Karau"}, {"level": "Intermediate", "track": "Stream", "abstract": "Modern businesses have data at their core, and this data is changing continuously. How can we harness this torrent of information in real-time? The answer is stream processing, and the technology that has since become the core platform for streaming data is Apache Kafka. Among the thousands of companies that use Kafka to transform and reshape their industries are the likes of Netflix, Uber, PayPal, and AirBnB, but also established players such as Goldman Sachs, Cisco, and Oracle.\nUnfortunately, today\u2019s common architectures for real-time data processing at scale suffer from complexity: there are many technologies that need to be stitched and operated together, and each individual technology is often complex by itself. This has led to a strong discrepancy between how we, as engineers, would like to work vs. how we actually end up working in practice.\nIn this session we talk about how Apache Kafka helps you to radically simplify your data architectures. We cover how you can now build normal applications to serve your real-time processing needs \u2014 rather than building clusters or similar special-purpose infrastructure \u2014 and still benefit from properties such as high scalability, distributed computing, and fault-tolerance, which are typically associated exclusively with cluster technologies. We discuss common use cases to realize that stream processing in practice often requires database-like functionality, and how Kafka allows you to bridge the worlds of streams and databases when implementing your own core business applications (inventory management for large retailers, patient monitoring in healthcare, fleet tracking in logistics, etc), for example\u00a0in the form of event-driven, containerized microservices.\nThis talk is presented by our Gold partner Confluent.\n", "title": "Rethinking Stream Processing with Apache Kafka: Applications vs. Clusters, Streams vs. Databases", "url": "https://2017.berlinbuzzwords.de/17/session/rethinking-stream-processing-apache-kafka-applications-vs-clusters-streams-vs-databases.html", "speaker": "Michael Noll"}, {"level": "Beginner", "track": "Stream", "abstract": "Your CEO runs up to you looking scared. Your competitors are recommending related articles based on context and machine learning and the current ML system keeps crashing.\nOur embedded iframe is inside popular news sites with millions of articles and thousands of concurrent visitors, The system\u2019s uptime should at least match these well established companies.\u00a0You have to fix it, now.\n\u00a0\nWhat do you do? Run? Convince the CEO that Machine Learning and Natural Language Processing are passing trends? Or do you reach for open source tools and set out to do something better than your competitors in just a few days?\n\u00a0\nWe went for the third option; using Elasticsearch, as the heart of this system.\nElasticsearch dynamic templating was used for mappings which support specific types like geopoints and dates but still let users dynamically add fields and events.\n\u00a0\nWe wanted simplicity and reliability in an embarrassingly parallel system, and implemented a reactive streams system. This let us build an asynchronous recommendation engine caching recommendation results in the background so they can be promptly served when asked by the frontend, This has proven resilient enough to give us sleep, simple enough to be maintainable and flexible enough to serve millions of users while keeping costs low.\nThese kind of scenarios happen on a daily basis; I will demonstrate how the right design decisions got the product out of the door on time, kept management happy and kept us engineers sane despite the time pressures involved. If you are tired of those nightly dinner \"treats\" here's a solution.\n", "title": "How to build a recommendation system overnight", "url": "https://2017.berlinbuzzwords.de/17/session/how-build-recommendation-system-overnight.html", "speaker": "Raam Rosh Hai"}, {"level": "Beginner", "track": "Store", "abstract": "Back in the days, you had a single machine and you could scroll down the single log file to figure out what is going on. In this Big Data world you need to combine a lot of logs together to figure out what is going on. Data is coming in huge volumes, with high speed so choosing important information and getting rid of noise becomes real challenge. There is a need for a centralized monitoring platform which will aid the engineers operating the systems, and serve the right information at the right time.\nThis talk will try to help you understand all the challenges and you will get an idea which tools and technology stacks are good fit to successfully monitor Big Data systems. The focus will be on open source and free solutions. The problem can be separated in two domains which both are the subject of this talk: metrics stack to gather simple metrics on central place and log stack to aggregate logs from different machines to central place. We will finish up with a combined stack and ideas how it can be improved even further with alerting and automated failover scenarios.\n", "title": "Challenges of Monitoring Distributed Systems", "url": "https://2017.berlinbuzzwords.de/17/session/challenges-monitoring-distributed-systems.html", "speaker": "Nenad Bozic"}, {"level": "Intermediate", "track": "Search", "abstract": "Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene. \u00a0And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance. \u00a0This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it. \u00a0We\u2019ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr. \u00a0We\u2019ll finish with a sneak peak into using deep learning and word2vec in a search context.\n", "title": "BM25 is so Yesterday: Modern Techniques for Better Search Relevance", "url": "https://2017.berlinbuzzwords.de/17/session/bm25-so-yesterday-modern-techniques-better-search-relevance.html", "speaker": "Grant Ingersoll"}, {"level": "Intermediate", "track": "Scale", "abstract": "The complexity and amount of data rises. Modern graph databases are designed to handle the complexity but still not for the amount of data. When hitting a certain size of a graph, many dedicated graph databases reach their limits in vertical or, most common, horizontal scalability.\nIn this talk I'll provide a brief overview about current approaches and their limits towards scalability. Dealing with complex data in a complex system doesn't make things easier... but more fun finding a solution. Join me on my journey to handle billions of edges in a graph database.\n", "title": "Handling Billions Of Edges in a Graph Database", "url": "https://2017.berlinbuzzwords.de/17/session/handling-billions-edges-graph-database.html", "speaker": "Michael Hackstein"}, {"level": "Intermediate", "track": "Search", "abstract": "When you submit a query to Elasticsearch or Apache Solr, underneath all the shiny user interfaces it's a Lucene Query that's doing all the grunt work. \u00a0But what actually happens when your query is processed? \u00a0And how come it's so damn fast?\nThis talk will guide you through the code paths that Lucene takes when executing a query, explaining the data structures and algorithms used, from simple term queries to more complex booleans and custom scoring methods. \u00a0It will be useful for people writing their own queries or similarities, wanting to optimize how their queries are run, or just plain curious about how an awesome piece of software actually works.\n", "title": "How does a Lucene Query actually work?", "url": "https://2017.berlinbuzzwords.de/17/session/how-does-lucene-query-actually-work.html", "speaker": "Alan Woodward"}, {"level": "Intermediate", "track": "Stream", "abstract": "Stream data processing is increasingly required to support business needs for faster actionable insight with growing volume of information from more sources. Apache Apex is a true stream processing framework for low-latency, high-throughput and reliable processing of complex analytics pipelines on clusters. Apex is designed for quick time-to-production, and is used in production by large companies for real-time and batch processing at scale.\nThis session will use an Apex production use case to walk through the incremental transition from a batch pipeline with hours of latency to an end-to-end streaming architecture with billions of events per day which are processed to deliver real-time analytical reports. The example is representative for many similar extract-transform-load (ETL) use cases with other data sets that can use a common library of building blocks. The transform (or analytics) piece of such pipelines varies in complexity and often involves business logic specific, custom components.\nTopics include:\nPipeline functionality from event source through queryable state for real-time insights.\nAPI for application development and development process.\nLibrary of building blocks including connectors for sources and sinks such as Kafka, JMS, Cassandra, HBase, JDBC and how they enable end-to-end exactly-once results.\nStateful processing with event time windowing.\nFault tolerance with exactly-once result semantics, checkpointing, incremental recovery\nScalability and low-latency, high-throughput processing with advanced engine features for auto-scaling, dynamic changes, compute locality.\nWho is using Apex in production, and roadmap.\n\nFollowing the session attendees will have a high level understanding of Apex and how it can be applied to use cases at their own organizations.\n", "title": "From Batch to Streaming ET(L) with Apache Apex", "url": "https://2017.berlinbuzzwords.de/17/session/batch-streaming-etl-apache-apex.html", "speaker": "Thomas Weise"}, {"level": "Intermediate", "track": "Search", "abstract": "Search engines like Lucene have been designed to run full-text queries as fast as possible. You can search for combinations of keywords using boolean operators, and Lucene will give you results in milliseconds. This is possible thanks to the inverted index structure, which gives you a sorted list of ids for every term. Then boolean queries just have to compute the intersection or union of these sorted lists, which is a cheap operation. However in the real world, users often want to run more complicated queries like phrase queries, range queries or queries on scripts, which can't easily get you a sorted list of ids. In this session, we will dive into how Lucene executes queries and in particular recent improvements around execution of slow queries.\n\nNo prior knowledge about Lucene is required, however users who have been exposed to Lucene, Solr or Elasticsearch in the past are more likely to enjoy this session.", "title": "Running slow queries with Lucene", "url": "https://2017.berlinbuzzwords.de/17/session/running-slow-queries-lucene.html", "speaker": "Adrien Grand"}, {"level": "Advanced", "track": "Stream", "abstract": "When I setup my iPhone to use\u00a0HTTP\u00a0proxy\u00a0and pick on the traffic,\u00a0I see lot's of JSON going cross the wire. In fact most of the traffic\u00a0I intercept is encoded as JSON.\u00a0I should not be surprised, JSON became\u00a0a standard for data serialisation. But is it a good standard? In my talk I want to challenge your perception of JSON and outline\u00a0the penalties we willing to accept while using JSON.\n\u00a0\n", "title": "Why are we using JSON?", "url": "https://2017.berlinbuzzwords.de/17/session/why-are-we-using-json.html", "speaker": "Maxim Zaks"}, {"level": "Intermediate", "track": "Scale", "abstract": "Do you have plans to start working with Apache Spark? Are you already working with Spark but you haven\u2019t gotten the expected performance and stability and you are not sure where to look for a fix?\nSpark has a very nice API and it promises high performance for crunching large datasets. It\u2019s really easy to write an app in Spark, unfortunately, it\u2019s also easy to write one which doesn\u2019t perform the way you would expect or just fails for no obvious reason.\nThis talk will consist of multiple common problems you might face when running Spark at full scale and, of course, solutions for solving them. Each of the problems I will cover will come with well-described background and examples so that it will be understood by people with no Spark experience. However, people who are working with Spark are the main audience. The ultimate objective is to give the audience a practical framework for optimizing the most common problems with Spark applications.\nClasses of problems in the presentation:\nDealing with skewed data\nSpark on YARN and its memory model\nCaching\nSizing executors\nLocality\n", "title": "Apache Spark? If only it worked.", "url": "https://2017.berlinbuzzwords.de/17/session/apache-spark-if-only-it-worked.html", "speaker": "Marcin Szymaniuk"}, {"level": "Beginner", "track": "Scale", "abstract": "Maybe you love working with women. \u00a0Maybe you hate working with women. Whatever your perspective, diversity in the workplace is good for business, yet the number\u00a0of women working in technology remains low around the globe. To make an impact on diversity numbers, we need to engage men as partners, sponsors, mentors and champions -- allies. \u00a0Let's put this into action with 10 ways\u00a0to ally for woman in technology. Learn simple ways to support women in the workplace and the\u00a0community - in the office, at conferences, and online. Become more confident in speaking up to support women and be more aware of the challenges facing women in technology after investing 20 minutes in this short session.\n", "title": "10 Ways to Ally for Women in Technology", "url": "https://2017.berlinbuzzwords.de/17/session/10-ways-ally-women-technology.html", "speaker": "Heather VanCura"}]