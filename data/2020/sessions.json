[{"level": "Intermediate", "track": "Stream", "abstract": "This talk is about how adversarial attacks can manipulate our deep learning modules and create drastic variations in the context of data. It focuses on Large\u00a0Textual data and how functions of Natural Language Processing will be trained over wrong information. These attacks compromise the deep learning models and alter the meaning of the data. It is very critical to protect our models from such attacks to protect our data. The talk describes of the measures which we can implement for our Natural Language Processing\u00a0models.\nRefer the presentation:\u00a0http://bit.ly/ppt_berlinbuzzwords", "title": "Adversarial Attacks on Deep Leaning Models in NLP", "url": "https://2020.berlinbuzzwords.de/session/adversarial-attacks-deep-leaning-models-nlp", "speaker": "Sakshi Shukla"}, {"level": "Intermediate", "track": "Search", "abstract": "Join two leading experts to talk about the next generation in search & relevance. Ask them anything about what's coming next - AI-Powered Search!", "title": "Ask Me Anything: AI-Powered Search", "url": "https://2020.berlinbuzzwords.de/session/ask-me-anything-ai-powered-search", "speaker": "Doug Turnbull"}, {"level": "Beginner", "track": "Search", "abstract": "Uwe Schindler is deeply involved in the development of Apache Lucene and is happy to answer all your questions about the upcoming Lucene 9.", "title": "Ask Me Anything: Lucene 9", "url": "https://2020.berlinbuzzwords.de/session/ask-me-anything-lucene-9", "speaker": "Uwe Schindler"}, {"level": "Beginner", "track": "Stream", "abstract": "General Data Protection Regulation (GDPR) is now in place. Are you ready to explain your models? This is a hands-on tutorial for beginners. I will demonstrate the use of open-source H2O platform\u00a0with both Python and R for automatic and interpretable machine learning. Participants will be able to follow and build regression and classification models quickly with H2O's AutoML. They will then be able to explain the model outcomes with various methods.\nPlease note:\u00a0Seats are limited so you need to register separately in our ticket shop here.", "title": "Automatic and Explainable Machine Learning with H2O", "url": "https://2020.berlinbuzzwords.de/session/automatic-and-explainable-machine-learning-h2o", "speaker": "Jo-fai Chow"}, {"level": "All", "track": "Community", "abstract": "Each year at Berlin Buzzwords we like to get together on the Sunday before the main conference schedule begins for a Barcamp and this year\u2019s online conference is no exception!Barcamps are informal sessions, a kind of \"un-conference\", with a schedule decided on the day. It is all driven by the interests and expertise of those who attend so each one is different, but ours are always great!\nAlthough the barcamp doesn't have a strict schedule, it won't be completely devoid of structure! #bbuzz barcamps are dynamic events, focused on the overall Berlin Buzzwords topics, tackling the same challenges but in a different format\nJoin the discussion at berlinbuzzwords.slack.com", "title": "Barcamp", "url": "https://2020.berlinbuzzwords.de/session/barcamp", "speaker": "Varun Thacker"}, {"level": "Beginner", "track": "Store", "abstract": "In this workshop, we will store data originally in tabular format, e.g. csvs, into an open-source graph database. It involves creating an appropriate schema and inserting data as triples into a graph.\nIn the first half of the workshop, we will guide you step by step with example scripts to create a graph of bike journeys with bike hire data. Through this example, you will learn how to create a schema and do proper data wrangling to insert the data. You will also be able to make a query and create an interactive graph visualization. In the second half, you will be able to write your own script to create a graph to analyse the Dublin Council voting data.\nWe will be coding in JavaScript but prior knowledge is not necessary as we will guide you through it.\nPlease note:\u00a0Seats are limited, so you have to register separately in our ticket shop.", "title": "Beginner Workshop for Storing Data as Graph", "url": "https://2020.berlinbuzzwords.de/session/beginner-workshop-storing-data-graph", "speaker": "Cheuk Ho"}, {"level": "Intermediate", "track": "Store", "abstract": "With the proliferation of data in the past years, most business critical decisions are heavily influenced by deep data analysis. As companies rely more on data for their functioning; storing, managing and accessing data intelligently and efficiently is more important than ever before.\u00a0\nAs more business decisions are driven by data in real time, we require strong guarantees such as acceptable latencies, high data quality and system reliability. Moving from a full-reload to a delta model of ingesting quickly became the primary way to ingest large amounts of data at scale. A number of such ingest patterns showcased how a transaction support on such datasets could benefit use-cases immensely.\u00a0\nHudi, an apache project is attempting to introduce uniform data lake standards.\u00a0Hudi is a storage abstraction library that uses Spark as an execution framework. In this talk, we will discuss how Hudi can provide ACID semantics to a data lake. We will discuss some of the basic primitives such as upsert & delete required to achieve acceptable latencies in ingestion while at the same time providing high quality data by enforcing schematization on datasets. Additionally, we will also discuss more advanced primitives such as restore, delta-pull, compaction & file sizing required for reliability, efficient storage management and to build incremental ETL pipelines. We will dig deeper into Hudi\u2019s metadata model that allows for O(1) query planning as well as how it helps support Time-Travel queries to facilitate building feature stores for machine learning use-cases. Apache Hudi builds on open-source file formats; we will discuss how to easily onboard your existing dataset to Hudi format while keeping the same open-source formats so you can start utilizing all the features provided by Hudi without needing to make any drastic changes to your data lake. We will talk about the challenges faced in productionizing large Spark based Hudi jobs\u00a0@scale at Uber and discuss how we addressed them.\u00a0\nFinally, we will make the case for the future, discussing various other primitives that will facilitate in building rich and portable data applications.", "title": "Building large scale, transactional data lakes using Apache Hudi", "url": "https://2020.berlinbuzzwords.de/session/building-large-scale-transactional-data-lakes-using-apache-hudi", "speaker": "Nishith Agarwal"}, {"level": "Advanced", "track": "Scale", "abstract": "Abstract\nAlthough building ML models on small/ toy data-set is easy, most production-grade problems involve massive datasets which current ML practices don\u2019t scale to. In this talk, we cover how you can drastically increase the amount of data that your models can learn from using distributed data/ml pipes.\nDetailed Description\nIt can be difficult to figure out how to work with large data-sets (which do not fit in your RAM), even if you\u2019re already comfortable with ML libraries/ APIs within python. Many questions immediately come up: Which library should I use, and why? What\u2019s the difference between a \u201cmap-reduce\u201d and a \u201ctask-graph\u201d? What\u2019s a partial fit function, and what format does it expect the data in? Is it okay for my training data to have more features than observations? What\u2019s the appropriate machine learning model to use? And so on\u2026\nIn this talk, we\u2019ll answer all those questions, and more!\nWe\u2019ll start by walking through the current distributed analytics (out-of-core learning) landscape in order to understand the pain-points and some solutions to this problem.\nHere is a sketch of a system designed to achieve this goal (of building scalable ML models):\na way to stream instances\na way to extract features from instances\nan incremental algorithm\nThen we\u2019ll read a large dataset into Dask, Tensorflow (tf.data) & sklearn streaming, and immediately apply what we\u2019ve learned about in last section. We\u2019ll move on to the model building process, including a discussion of which model is most appropriate for the task. We\u2019ll evaluate our model a few different ways, and then examine the model for greater insight into how the data is influencing its predictions. Finally, we\u2019ll practice this entire workflow on a new dataset, and end with a discussion of which parts of the process are worth tuning for improved performance.\nDetailed Outline\nIntro to out-of-core learning\nRepresenting large datasets as instances\nTransforming data (in batches) \u2013 live code [3-5]\nFeature Engineering & Scaling\nBuilding and evaluating a model (on entire datasets)\nPracticing this workflow on another dataset\nBenchmark other libraries/ for OOC learning\nQuestions and Answers\nKey takeaway\nBy the end of the talk participants would know how to build petabyte scale ML models, beyond the shackles of conventional python libraries.\nParticipants would have a benchmarks and best case practices for building such ML models at scale.", "title": "Building Petabyte Scale ML Models with Python", "url": "https://2020.berlinbuzzwords.de/session/building-petabyte-scale-ml-models-python", "speaker": "Vaibhav Srivastav"}, {"level": "All", "track": "Search", "abstract": "When looking at personalised e-commerce search, questions on data ethics, privacy and trust come into play. Trust may be regarded as the most important feeling that any search shall aspire to elicit in its users, and trusting search, regardless of its sophistication, is what this workshop is about.\nJoin a group of designers, developers and data scientists to architect and design a personalised commerce search that is conceived to evoke trust in its users.\nDuring the course of these four hours, you will partner with passionate search professionals to compose, share and debate your ideas and to collaboratively arrive at the foundations of what could be the first privacy-by-design personalised commerce search.\nThis workshop requires separate registration (free) in addition to your standard event ticket. Get your ticket from our ticket shop: https://berlinbuzzwords.de/tickets. For more information see\u00a0https://mices.co/mices2020/workshop-trust.html.", "title": "Building trusting connections - the role of emotions in personalized e-commerce search", "url": "https://2020.berlinbuzzwords.de/session/building-trusting-connections-role-emotions-personalized-e-commerce-search", "speaker": "Angel Maldonado"}, {"level": "Intermediate", "track": "Search", "abstract": "(User, Query, Document) - This simple tuple helps give shape and depth to the often flat information retrieval landscape. With a few quick transformations, this dataset can help you suggest query completions, display related queries, propose synonym candidates, generate taste profiles, and forecast demand. This talk will help search engineers and product owners to get a feel for the data relationships created by this useful feedback loop. (Note that while many of these product features can be further optimized using ML, this talk requires no previous knowledge of machine learning applications in search.)", "title": "Click logs and insights: Putting the search experts in your audience to work", "url": "https://2020.berlinbuzzwords.de/session/click-logs-and-insights-putting-search-experts-your-audience-work", "speaker": "Peter Dixon-Moses"}, {"level": "Beginner", "track": "Community", "abstract": "Join us for some parting words\u00a0from the organizers of Berlin Buzzwords, Haystack and MICES.", "title": "Closing session", "url": "https://2020.berlinbuzzwords.de/session/closing-session-0", "speaker": "Nina M\u00fcller"}, {"level": "Beginner", "track": "Search", "abstract": "Carrefour started the journey of digital transformation 6 years ago, transforming the e-commerce business into an omnichannel business using an \u201call in one\u201d e-commerce platform where Search was one more piece of the puzzle.\n\nIn time, Carrefour technical team realized that through an open and flexible Search solution, the e-commerce platform could be constructed around it, making Search (and Navigation) the core as opposed to an add-on or an element on the periphery.\n\nThis approach, at first, implied several challenges, from strategic to technical ones:\nWhat options do we have? Use a SaaS solution, build a search from scratch, Base our search in an existing solution, Look for a partner to help us to build it\u2026\nHow can we keep the intellectual property and the know-how inside the company?\nWhat experience do we want to offer to our customers?\nHow can we deal with a huge assortment of products from different natures and stores?\nWhat insights do we expect to take form the search to help our business?\nHow do we know whether the search is performing well?\nWhat strategy do we have to follow for replacement the old search engine keeping, at the same time, the rest of the \u201call in one\u201d e-commerce platform?\nHow do we develop it and deploy it into production?\nHow do we evolve it?", "title": "From commercial search to owned search", "url": "https://2020.berlinbuzzwords.de/session/commercial-search-owned-search", "speaker": "Jesus de los Bueis"}, {"level": "Beginner", "track": "Scale", "abstract": "Over time, we have all become avid consumers of information. Yet that information is increasingly fragmented and partisan, based on isolated effort instead of consensus. As we look to the future of information production, how can we build solutions that incentivize people to learn from each other, to understand the full complexity and nuance behind an issue instead of just their own perspective?\nFor the past two decades, Wikipedia has been helping humans and machines process the entire spectrum of information, from lengthy research and graduate level dissertations at one end to soundbites and trivia questions dictated to voice assistants at the other. We can provide a valuable model for how technology can help the next generation of knowledge-seekers decode, decipher and analyze information. This talk will focus on knowledge in the next evolution of the internet, and how we must enable not just more access to information, but also critical thinking at global scale.", "title": "A crisis of critical thinking - what Wikipedia can teach us about the future of information and technology\u2019s role", "url": "https://2020.berlinbuzzwords.de/session/crisis-critical-thinking-what-wikipedia-can-teach-us-about-future-information-and", "speaker": "Grant Ingersoll"}, {"level": "Intermediate", "track": "Search", "abstract": "The increase is data and traffic accompanied with the feature set offered by Solr has led to an increased usage of Solr in recent times. As a lot of these applications are critical, a reliable disaster recovery mechanism has become essential.\n\u00a0\nWhile Solr already offers a way to accomplish disaster recovery using cross-dc replication out of the box, there are other ways to accomplish the same. Copying data is the obvious part of this system but a lot of supporting features or systems like monitoring, self-defense mechanism etc. are needed for a complete cross-dc solution.\n\u00a0\nA simple solution, like the one that Solr offers out of the box works 90% of the times, but the lack of supporting features, monitoring tools, and also self-defense mechanism make it a difficult choice in a lot of places. Using such a system might catch you off-guard and leave you with a false sense of DR availability.\n\u00a0\nDuring this talk, I would like to talk about a few approaches of achieving cross-dc availability in Solr, something that I have tried and used over the years and highlight the pros and cons of each of them. I would also talk about all the satellite features and applications that are needed to ensure consistency of these clusters in addition to the self-healing, and self-defense mechanisms that are needed to reliably run and use the DR clusters.\n\u00a0\nAt the end of this talk, attendees would have a much better understanding of achieving cross-dc for Solr and their reliability levels. They would also have learnt about the supporting systems that are needed to have a solid cross-dc story in Solr, one that scales and works reliably.", "title": "Cross DC replication in Apache Solr - Beyond just forwarding data", "url": "https://2020.berlinbuzzwords.de/session/cross-dc-replication-apache-solr-beyond-just-forwarding-data", "speaker": "Anshum Gupta"}, {"level": "Intermediate", "track": "Scale", "abstract": "Deep learning achieves great performance in many areas, and it\u2019s especially useful for computer vision tasks. However, using deep learning in production is challenging: it requires a lot of effort for developing and running the infrastructure to serve deep learning models at scale.\nIn this talk, we present a system for classifying images on one of the largest online classified advertising platforms. The main requirement for this system is to classify tens of millions of images daily and be able to operate reliably even during peak hours.\nIt took a year and lots of trial and error to arrive at the system we currently use. We present the details of this journey and tell our story: how we approached it initially, what worked and what didn\u2019t, how it evolved and how it\u2019s working right now.\nOf course, we also walk you through the technical details and show how to implement a similar system using Python, AWS, Kubernetes, MXNet, and TensorFlow.", "title": "Deep learning in production: Serving image models at scale", "url": "https://2020.berlinbuzzwords.de/session/deep-learning-production-serving-image-models-scale", "speaker": "Alexey Grigorev"}, {"level": "Intermediate", "track": "Stream", "abstract": "Pravega is storage for streams. Pravega exposes stream as a core storage primitive, which enables applications continuously generating data (e.g., from sensors, end users, servers, or cameras) to ingest and store such data permanently. Applications that consume stream data from Pravega are able to access the data through the same API, independent of whether it is tailing the stream or reprocessing historical data. Pravega has some unique features such as the ability of storing an unbounded amount data per stream, while guaranteeing strong delivery semantics with transactions and scaling according to changes to the workload.\nPravega streams build on segments: append-only sequences of bytes that can be open or sealed. Segments are indivisible units that form streams and that enable important features. Segments enable Pravega to spread the load of streams across servers, to scale streams, to implement transactions efficiently, and to implement state replication.\nIn this presentation, we overview the architecture of Pravega and its core features. To enable reliable ingestion of events, we discuss how to use transactions along with state synchronization to guarantee that events from memoryful sources are ingested exactly once. Such a property is important to guarantee correctness when processing the data.\nPravega is an open-source project hosted on GitHub, and aims to be a community-driven project. It is under active development and encourages interested contributors to check it out and interact with the developers.", "title": "Delivering stream data reliably with Pravega", "url": "https://2020.berlinbuzzwords.de/session/delivering-stream-data-reliably-pravega", "speaker": "Flavio Junqueira"}, {"level": "Advanced", "track": "Search", "abstract": "This talk builds on work by Simon Hughes and others to apply genetic algorithms (GA) and random search for finding optimal parameters for relevance ranking. While manual tuning can be useful, the parameter space is too vast to be confident that one has found optimal parameters without overfitting.We'll present Quaerite (https://github.com/tballison/quaerite), an open source toolkit that allows users to specify experiment parameters and then run a random search and/or a GA to identify the best settings given ground truth.We'll offer an overview of mapping parameter space to a GA problem in both Solr and Elasticsearch, the importance of the baked-in n-fold cross-validation, and the surprises and successes found with deployed search systems.", "title": "Evolving Relevance", "url": "https://2020.berlinbuzzwords.de/session/evolving-relevance", "speaker": "Tim Allison"}, {"level": "Intermediate", "track": "Scale", "abstract": "Modern solutions to search and recommendation require evaluating machine-learned models over large data sets with low latency. Producing the best results typically require combining fast (approximate) nearest neighbour search in vector spaces to limit candidates, filtering to surface only the appropriate subset of results in each case, and evaluation of more complex ML models such as deep neural nets computing over both vectors and semantic features. Combining these needs into a working and scalable solution is a large challenge as separate components solving for each requirement cannot be composed into a scalable whole for fundamental reasons.\nThis talk will explain the architectural challenges of this problem, show the advantages of solving it on concrete cases and introduce an open source engine - Vespa.ai - that provides a scalable solution by implementing all the elements in a single distributed execution.", "title": "Fast and scalable evaluation of machine learned models over large data sets using open source", "url": "https://2020.berlinbuzzwords.de/session/fast-and-scalable-evaluation-machine-learned-models-over-large-data-sets-using-open-source", "speaker": "Jon Bratseth"}, {"level": "Intermediate", "track": "Store", "abstract": "Engineering features for machine learning is hard. Before you start, you need to know: are you developing the features for training the model (Python?) or for serving the model (Java/javascript/etc), and if both - how do you ensure consistency of your features between training and inferencing? Could anybody else in your organization find the feature useful in their model(s)? If you are using a traditional data warehouse, how do you\u00a0retrieve the value of a feature from last year (that has now been overwritten with more recent data) to test my model on data from last year?\u00a0\u00a0How do you efficiently join features originating from different backend systems.\nIn this talk, we will answer these questions in the context of the Feature Store. We will show how a Feature Store can provide a natural interface between Data Engineers, who create reusable features from diverse data sources, and Data Scientists, who experiment with predictive models, built from the same features. We will dive into the only fully open-source Feature Store for machine learning, Hopsworks, to better understand the potential of Feature Stores.", "title": "The Feature Store: where Data Engineering meets Data Science", "url": "https://2020.berlinbuzzwords.de/session/feature-store-where-data-engineering-meets-data-science", "speaker": "Jim Dowling"}, {"level": "Intermediate", "track": "Stream", "abstract": "Apache Beam is an open source unified model for defining data processing pipelines (Batch and strEAM), which allows you to write your pipeline in your language of choice and run it with minimal effort on the execution engine (ex: Apache Spark, Apache Flink, Google Cloud Dataflow) of choice.\u00a0\u00a0In this practical session we will get hands-on writing Beam pipelines, and as well as discuss the fundamentals of Beam programming model, and SDKs (Python, Go, Java).\nPrerequisites\nYou will need to install IntelliJ IDEA and/or PyCharm with the EduTools plugin, and with the kata(s) installed in the language of their choice to work through exercises in the online platform.\nInstructions can be found on the blogpost here: https://beam.apache.org/blog/beam-kata-release/", "title": "First Steps with Apache Beam: Writing Portable Pipelines using Java, Python, Go", "url": "https://2020.berlinbuzzwords.de/session/first-steps-apache-beam-writing-portable-pipelines-using-java-python-go", "speaker": "Austin Bennett"}, {"level": "Beginner", "track": "Community", "abstract": "2020 has heightened our attention for multiple difficult problems without clear solutions including the COVID-19 pandemic, a global economic downturns, and police violence.\u00a0 These problems fit the pattern of \"Wicked Problems\" as originally described by Rittel and Weber in 1973.\u00a0 Wicked Problems, are\u00a0systemic problems without a clear formulation, and without clear solutions.\u00a0 Despite the difficulties in understanding and solving these problems, some organizations are better equipped than others for making progress on them.\u00a0 Surprisingly, many of these organizations share a common hallmark: the ability to promote women into positions of leadership.\nI will explain what Wicked Problems are, look at examples where we are succeeding at mitigating these problems, and examine commonalities that arise across successful approaches.\u00a0 Then I will present two examples in which Women in Open Source, Open Data, and Open Standards are helping to make the world a better place both in the context of COVID-19, and in the context of the broader problems humanity\u00a0faces.", "title": "How Women in Open Source Lead in Times of COVID-19", "url": "https://2020.berlinbuzzwords.de/session/how-women-open-source-lead-times-covid-19", "speaker": "Myrle Krantz"}, {"level": "All", "track": "Community", "abstract": "We welcome you to joint virtual MICES and Haystack conferences 2020.", "title": "Joint opening session MICES and Haystack", "url": "https://2020.berlinbuzzwords.de/session/joint-opening-session-mices-and-haystack", "speaker": "Ren\u00e9 Kriegler"}, {"level": "Intermediate", "track": "Search", "abstract": "Some hard technical challenges are better solved by changing the foundations. We had a use case of searching many fields with strong constraints on memory and performance. We needed a massive number of fields to support field level security at scale and open the path to machine learned ranking models.\nA custom PostingsFormat allowed for a solution with greater efficiencies than our prior solution. We developed a new Lucene PostingsFormat called UniformSplit, we deployed it at a very large scale, and we open-sourced it. We learned a lot during the journey, especially about micro-benchmarking, java memory consumption, compact data representation and high performance Lucene indices.\nThis presentation is a good medium to share what we learned with step backwards, the learnings on the Lucene mechanisms, the tips and the pitfalls we encountered. And as we continued the development, we will share the latest works and production measures.", "title": "A Journey to Write a New Lucene PostingsFormat", "url": "https://2020.berlinbuzzwords.de/session/journey-write-new-lucene-postingsformat", "speaker": "Bruno Roustant"}, {"level": "Beginner", "track": "Stream", "abstract": "Modern data processing environments resemble factory lines, transforming raw data to valuable data products. The lean principles that have successfully transformed manufacturing are equally applicable to data processing, and are well aligned with the new trend known as DataOps. In this presentation, we will explain how applying lean and DataOps principles can be implemented as technical data processing solutions and processes in order to eliminate waste and improve data innovation speed. We will go through how to eliminate the following types of waste in data processing systems:\n* Cognitive waste - unclear source of truth, dependency sprawl, duplication, ambiguity.\n* Operational waste - overhead for deployment, upgrades, and incident recovery.\n* Delivery waste - friction and delay in development, testing, and deployment.\n* Product waste - misalignment to business value, detach from use cases, push driven development, vanity quality assurance.\nWe will primarily focus on technical solutions, but some of the waste mentioned requires organisational refactoring to eliminate.", "title": "The lean principles of DataOps", "url": "https://2020.berlinbuzzwords.de/session/lean-principles-dataops", "speaker": "Lars Albertsson"}, {"level": "All", "track": "Search", "abstract": "Join us for a session of lightning talks and networking!\nDo you have a five-minute talk you could give? Maybe a crazy new idea, a project you'd like to introduce, a brief getting started guide or a proposal? Sign up here\u00a0https://forms.gle/tY1YzGkmG7BqaPh79\u00a0and we'll be in touch!", "title": "Lightning Talks and Social", "url": "https://2020.berlinbuzzwords.de/session/lightning-talks-and-social", "speaker": "Atita Arora"}, {"level": "All", "track": "Search", "abstract": "Join us for another round of lightning talks and networking!\nDo you have a five-minute talk you could give? Maybe a crazy new idea, a project you'd like to introduce, a brief getting started guide or a proposal? Sign up here\u00a0https://forms.gle/tY1YzGkmG7BqaPh79\u00a0and we'll be in touch!", "title": "Lightning Talks and Social", "url": "https://2020.berlinbuzzwords.de/session/lightning-talks-and-social-0", "speaker": "Charlie Hull"}, {"level": "Beginner", "track": "Scale", "abstract": "As software engineers, we take pride in our code quality. As data scientists, in the quality of our models and analyses. As a data engineer, I take pride in the quality of the datasets I provide access to.\nEveryone in IT works with data one way or another, be it producing, managing or using it. Yet like water for fish, we often fail to notice data because it is all around us. And, just like fish in the water suffer bad water quality, we suffer if our data quality decreases. Unlike the fish in the water though, we can actually all contribute to addressing data quality issues.\nIn my talk, I want to encourage you to become more aware of data quality concerns. I will discuss what data quality is, how we can identify data quality issues and some strategies for addressing them. As a practical example, I will share our experiences with monitoring data quality using Amazon Research's deequ framework.", "title": "Mind your data! - Data quality for the rest of us", "url": "https://2020.berlinbuzzwords.de/session/mind-your-data-data-quality-rest-us", "speaker": "Ellen K\u00f6nig"}, {"level": "Beginner", "track": "Search", "abstract": "Imagine a shopper searches for \"coffee\". The search results mindlessly repeat brand name, ground coffee over and over. Technically relevant. But not very useful. Not mixed in are results appealing to a different kind of \"coffee\" searcher: \u00a0organic coffee, whole bean, espresso, or related coffee equipment.\u00a0\nBeyond relevant results is Result Set Diversity: providing not just technically correct answers, but a diverse (though still relevant) view of what's possible in the search system. Showing our grocery searcher that we might, after all, have something they want.\nIn this panel, we'll discuss Result Set Diversity in search. We'll hear from teams working to summarize for users the diverse array of options available to them given their query. An ideal search experience helps provide next steps, broaden the user's understanding of the content related to keywords, and cover the user's possible, unspoken intent.\nJoin host Doug Turnbull and search practitioners, Andreas Wagner, Felipe Besson, and Lev Gershenzon, as we hear experiences measuring & solving Results Diversity, with a panel discussion on the Result Search Diversity problem.", "title": "Mixing and Matching: Diversifying Search Results", "url": "https://2020.berlinbuzzwords.de/session/mixing-and-matching-diversifying-search-results", "speaker": "Andreas Wagner"}, {"level": "Intermediate", "track": "Search", "abstract": "Natural language is hard when your target is not a single organization, but the tremendous number of Salesforce customers, each with their own unique customization.\u00a0\nIn this privacy-first world that Salesforce always operated in, it is even harder to do without having access to customer data to train machine learning models.\nIn this talk, we will describe how we implemented, deployed and nurtured Salesforce flavour of Natural language queries. Our approach is a battle-tested mix of deep-learning, solr search indexes, and basic engineering techniques.\u00a0\nThis talk is presented by SalesForce.", "title": "Natural Language queries at Salesforce scale", "url": "https://2020.berlinbuzzwords.de/session/natural-language-queries-salesforce-scale", "speaker": "Marc Brette"}, {"level": "All", "track": "Search", "abstract": "Over the last two years we have developed a Neural IR model for complementing the traditional search system of a large fashion e-commerce company. The ML model has been rolled out in 16 countries and is used for increasing recall of low-result queries as well as for query-dependent ranking powering 20% of traffic today.\nIn this talk we will present the different stages of the model development, the feature representations we have chosen, how we generate massive amounts of training data, and how we manage the tradeoff between learning from big data and staying efficient. We will demonstrate the use cases and present successful results from offline and online testing.", "title": "Neural Search in Practice", "url": "https://2020.berlinbuzzwords.de/session/neural-search-practice-0", "speaker": "Daniel Weinland"}, {"level": "Advanced", "track": "Search", "abstract": "Personalization in IR is one of the hottest topics in the AI-takes-all economy: we should not aim to be \"just\" semantically relevant, but also tailor results to users' preferences and intent. However, personalization in digital commerce is easier said than done: most shoppers visit a given store no more than twice a year, and bounce rates across verticals show that it is important to personalize as early as possible. In this talk we share effective strategies to tackle the challenge of \"in-session personalization\" in NLP and IR tasks, starting from the straightforward case of \"one-shop\" personalization and then generalizing to more. On the business side, we argue using industry benchmarks and data from our network that in-session personalization is a fundamental part of any relevance journey in the hyper-competitive e-commerce market. On the tech side, we build on the latest deep learning trends to show how increasingly sophisticated representations of real-time intent can power personalization in product search. Once dense architectures are in place, we are ready to tackle the challenge of \"transfer learning\": can shopper's intent be transferred from one shop to another without annotated data? Using insights from lexical learning, sessions from different shops can be projected in the same space to power \"zero-shot\" personalization for downstream NLP services; finally, we prove with quantitative and qualitative benchmarks that zero-shot predictions are a significant improvement over industry baselines.", "title": "\"Not all those who browse are lost\": few-shot and zero-shot personalization for digital commerce using deep architectures.", "url": "https://2020.berlinbuzzwords.de/session/not-all-those-who-browse-are-lost-few-shot-and-zero-shot-personalization-digital-commerce", "speaker": "Jacopo Tagliabue"}, {"level": "All", "track": "Scale", "abstract": "We know open source software, hardware and data are good, but sometimes they are also implemented and utilised for good purposes.\nAmanda will talk about some of her experiences in tech, particularly Open technology being used to bank the unbanked and democratise poverty through various projects including implementation of open payment platforms, her work as Chair of the UN Technology Innovation Labs Open Source and IP Advisory and setting up the disaster response tools, Open Tech Response.", "title": "Open For Good", "url": "https://2020.berlinbuzzwords.de/session/open-good", "speaker": "Amanda Brock"}, {"level": "All", "track": "Community", "abstract": "Welcome to the first Berlin Buzzwords online conference. Learn more about our program structure, networking possibilities and more.", "title": "Opening Session", "url": "https://2020.berlinbuzzwords.de/session/opening-session", "speaker": "Nina M\u00fcller"}, {"level": "Beginner", "track": "Stream", "abstract": "Apache Flink supports SQL as a unified API for stream and batch processing. SQL is easier to use than Flink\u2019s lower-level APIs and covers a wide variety of use cases. In this hands-on tutorial you will learn how to run SQL queries on data streams with Apache Flink. We will look at the concepts behind continuous queries and dynamic tables and will show you how to solve different use cases with streaming SQL, including enriching and joining streaming data, computing windowed aggregations, and maintaining materialized views in external storage systems.Prerequisites\nNo prior knowledge of Apache Flink is required. We assume basic knowledge of SQL.\nYou will need a computer with at least 8 GB RAM and Docker installed.\u00a0To save time during the event, we would also like to ask you to set up the tutorial environment beforehand by following the instructions at https://github.com/ververica/sql-training/wiki/Setting-up-the-Training-Environment\nPlease note:\u00a0This workshop is not included in the Standard Ticket. You have to register separately in our\u00a0ticket shop.", "title": "Querying Data Streams with Flink SQL \u2013 Part I [\u20ac]", "url": "https://2020.berlinbuzzwords.de/session/querying-data-streams-flink-sql-part-i-eu", "speaker": "Fabian Hueske"}, {"level": "Beginner", "track": "Stream", "abstract": "Apache Flink supports SQL as a unified API for stream and batch processing. SQL is easier to use than Flink\u2019s lower-level APIs and covers a wide variety of use cases. In this hands-on tutorial you will learn how to run SQL queries on data streams with Apache Flink. We will look at the concepts behind continuous queries and dynamic tables and will show you how to solve different use cases with streaming SQL, including enriching and joining streaming data, computing windowed aggregations, and maintaining materialized views in external storage systems.Prerequisites\nNo prior knowledge of Apache Flink is required. We assume basic knowledge of SQL.\nYou will need a computer with at least 8 GB RAM and Docker installed.\u00a0To save time during the event, we would also like to ask you to set up the tutorial environment beforehand by following the instructions at https://github.com/ververica/sql-training/wiki/Setting-up-the-Training-Environment\nPlease note:\u00a0This workshop is not included in the Standard Ticket. You have to register separately in our\u00a0ticket shop.", "title": "Querying Data Streams with Flink SQL \u2013 Part II [\u20ac]", "url": "https://2020.berlinbuzzwords.de/session/querying-data-streams-flink-sql-part-ii-eu", "speaker": "Fabian Hueske"}, {"level": "Beginner", "track": "Search", "abstract": "When we set out to build a managed service we made a number of assumptions. Who cares what cloud we run in? Why will customers care about the system health\u00a0of their nodes if we're managing them? Kubernetes will solve\u00a0all our scale challenges, right?\n\u00a0\nThose were all wrong assumptions and I want to share why, from a product perspective. In this talk, I will share my experience as the Product Manager of Lucidworks Managed Search and as an active community member. As an introduction, customers care and they have a good reason to care. If they've loaded millions, or even billions, of assets into one cloud they don't want their search application to run in another. If search powers your core business \u2014 you care. And no, Kubernetes does not solve all your scale challenges, but Go goes a long way as a tool. Let me share our journey of building the most feature-rich, up-to-date, and scalable Solr solution on the market. We work on a variety of clouds, in many regions, and this workshop will explain how we got here.\n\n\u00a0\nPlease note: Registration for this workshop is now closed. Attendees who have registered for this workshop will receive an email with login details to the workshop platform.", "title": "Running a Managed Solr Service: High Availability and a High Tolerance for Pain", "url": "https://2020.berlinbuzzwords.de/session/running-managed-solr-service-high-availability-and-high-tolerance-pain", "speaker": "Marcus Eagan"}, {"level": "Intermediate", "track": "Scale", "abstract": "Elasticsearch is highly scalable, but some recent additions make it easier to tie everything together. In this talk, we discuss\u00a0Lifecycle Management or how to automate having a multi-tier architecture as well as automated snapshots and how this actually works in the background. In addition, this talk presents some challenges both during the initial implementation and unexpected failures in production.\nThe goal of this talk is to make your cluster management less complex and more cost-effective.\nPlease note: Registration for this workshop is now closed. Attendees who have registered for this workshop will receive an email with login details to the workshop platform.\nThis workshop is presented by elastic.", "title": "Scale Your Elasticsearch Cluster", "url": "https://2020.berlinbuzzwords.de/session/scale-your-elasticsearch-cluster", "speaker": "Philipp Krenn"}, {"level": "Intermediate", "track": "Scale", "abstract": "In the last few years, deep learning has achieved dramatic success in a wide range of domains, including computer vision, artificial intelligence, speech recognition, natural language processing and reinforcement learning.\nHowever, good performance comes at a significant computational cost. This makes scaling training expensive, but an even more pertinent issue is inference, in particular for real-time applications (where runtime latency is critical) and edge devices (where computational and storage resources may be limited).\nThis talk will explore common techniques and emerging advances for dealing with these challenges, including best practices for batching; quantization and other methods for trading off computational cost at training vs inference performance; architecture optimization and graph manipulation approaches.", "title": "Scaling up Deep Learning by Scaling Down", "url": "https://2020.berlinbuzzwords.de/session/scaling-deep-learning-scaling-down", "speaker": "Nick Pentreath"}, {"level": "Beginner", "track": "Search", "abstract": "EBSCO Health's Search Team applies user intent analysis to increase precision and reduce the time it takes to answer critical medical questions. Learn about the key areas of our search ecosystem that come together to deliver semantic search results for clinicians. We will kick off the discussion with a deep dive into our content enrichment process. We will explore how our Medical Knowledge Graph semantically enhances our content and enables us to disambiguate context. We will then review how we leverage implicit and explicit user signals to help us understand intent. These signals allow us to dynamically generate runtime specific search strategies against Elasticsearch. Finally, we will discuss how we gather judgement lists from subject matter experts and leverage evaluation tools to tune our search results. Please join us as we explore the challenges of building semantic search for the medical domain.", "title": "Semantic Search Ecosystem", "url": "https://2020.berlinbuzzwords.de/session/semantic-search-ecosystem", "speaker": "Erica Lesyshyn"}, {"level": "All", "track": "Scale", "abstract": "Sophie Watson and William Benton explore a way to answer interesting queries about truly massive datasets almost instantly and with a fixed amount of space.\nIt sounds like magic, but you\u2019ll go hands-on to practice sketching data structures that work this magic and the key trick that makes them possible. Sophie and William introduce truly scalable techniques for several fundamental problems like set membership, set and document similarity, counting kinds of events, and counting distinct elements. You\u2019ll learn how and when to use these structures as well as how they work. You\u2019ll see how the same techniques work for parallel, distributed, and stream processing at scale. And you\u2019ll leave able to put these techniques to work in real data engineering and machine learning applications like join processing, document classification, and content personalization.\nPlease note: Registration for this workshop is now closed. Attendees who have registered for this workshop will receive an email with login details to the workshop platform.", "title": "Sketching Data and Other Magic Tricks [\u20ac]", "url": "https://2020.berlinbuzzwords.de/session/sketching-data-and-other-magic-tricks-eu", "speaker": "Sophie Watson"}, {"level": "Beginner", "track": "Search", "abstract": "Companies today are challenged in many different ways to provide a conclusive e-commerce search: in understanding constantly changing customer requirements and translating them to a holistic solution consisting of channels, organisation, processes, systems and data. But how can the multi-layered challenges of search be overcome and the associated opportunities exploited?\nThe Exploded View model from foryouandyourcustomers provides answers to these questions. It has been continuously refined and has been proven in practice. The many possible uses of the Exploded View have shown that it can present and explain decisive interrelations for the development of an organisation and that the people involved \u2013 from managers to employees with little involvement \u2013 receive an overall picture that supports them in digital and analog change.", "title": "Solving the E-Commerce Search Challenge: An effective and lasting approach based on the foryouandyourcustomers' Exploded View model", "url": "https://2020.berlinbuzzwords.de/session/solving-e-commerce-search-challenge-effective-and-lasting-approach-based", "speaker": "Fabian Engeln"}, {"level": "Beginner", "track": "Stream", "abstract": "Take your first steps to understanding and start working with stream processing! By the end of the course, you will be able to build and run distributed streaming pipelines in Java:\nExplain when to use streaming\nDesign a streaming application from the building blocks\nTransform, match, correlate and aggregate continuous data\nScale, deploy, and operate streaming apps\nWe will also cover the advantages and disadvantages of the stream processing technologies available when approaching real-world problems.\n\u00a0\nPart 1: Stream Processing Overview\nStreaming: what is it and where did it come from\nHow streaming fits into the architecture\nContinuous data pipelines\nUse-Cases\nThe architecture of current streaming frameworks\nPart 2: Transforming a Stream of Data (Lab)\nConnectivity\nTransforming and filtering\nPart 3: Enrichment (lab)\nLocal and remote lookup services\nCaching for performance\nPart 4: Aggregations (lab)\nStateful Streaming\nBatch x windowed aggregations\nTime-series data and late events\nPart 5: Scaling and Operations (lab)\nGoing distributed\nEmbedded and Remote cluster setups\nElasticity and fault tolerance\nUpgrading the running job\nMonitoring and diagnostics\nPart 6: Q&A and Conclusion\n\u00a0\nPlease note: Seats are limited so please register separately in our ticket shop here.", "title": "Stream Processing Essentials", "url": "https://2020.berlinbuzzwords.de/session/stream-processing-essentials", "speaker": "Nicolas Fr\u00e4nkel"}, {"level": "Intermediate", "track": "Stream", "abstract": "Orchestration frameworks like Kubernetes have made dealing with stateless applications very easy. But for stateful applications, we are still clinging to the ancient wisdom that state shall be someone else's problem: just put it in a database! Because of that, we are still struggling with the same issues of data consistency and complex failure semantics as decades ago. Developing stateful applications in a scalable and resilient way is still hard, especially when they span multiple (mirco)services.\nStream Processors, like Apache Flink, have solved similar problems in the area of event-processing. By rethinking the relationship between state, messaging, and computation, stream processing applications are out-of-the-box scalable and consistent.\n\u00a0\nIs it possible to bring some of these ideas to the space of general-purpose applications and (micro) services?\n\nThe Apache Flink project has recently added a new subproject called \"Stateful Functions\" (https://statefun.io/) that tries to achieve exactly that. In Stateful Functions, the Flink effectively becomes an event-driven database that works together with containerized event-driven functions to form a new building block for scalable and consistent applications. In this talk, we present the Stateful Functions project. We show how its small change in responsibilities between database and applications goes surprisingly far in solving the problem of consistency and failure semantics for applications, and additionally makes it blend in very with current serverless technologies, like AWS Lambda, knative, etc.", "title": "From Stream Processor to Event-driven Database with Stateful Functions", "url": "https://2020.berlinbuzzwords.de/session/stream-processor-event-driven-database-stateful-functions", "speaker": "Stephan Ewen"}, {"level": "Intermediate", "track": "Scale", "abstract": "What if you were given 2 weeks to prepare for running your first marathon, and had to be able to keep up with the fastest runner?\nThis is the story of a small team that jumped head first into building a top tier stream processing (Apache Flink) service on a tight timeline, and how they prepared for being able to keep up with the\u00a0fastest components.\u00a0\nThis talk will be of interest to those who want\u00a0to build larger, more complex stream processing applications (and quickly), but will also benefit anyone\u00a0who is looking to adopt a new streaming technology (or use it in a way that is new to their company), at any scale. Other takeaways include pragmatic steps to\u00a0leverage\u00a0unique Flink features\u00a0to more effectively solve\u00a0common, real world\u00a0data streaming problems.", "title": "Streaming, Fast and Slow", "url": "https://2020.berlinbuzzwords.de/session/streaming-fast-and-slow", "speaker": "Caito Scherr"}, {"level": "Advanced", "track": "Search", "abstract": "The world of information retrieval is changing. BERT, Elmo, and the Sesame Street gang are moving in, shouting the gospel of \"thought vectors\" as a replacement for traditional keyword search. Meanwhile many search teams are now automatically extracting graph representations of the world, trying their best to also provide more structured answers in the search experience. Poor old keyword search seems so outdated by comparison - is it dead, dying, or simply misunderstood? Contrary to popular belief, embeddings and thought vectors only solve a small subset of search problems, and each of these three tools (keyword search, thought vectors, and knowledge graphs) actually serves a critical role in building the next generation of search experiences. In this talk, we'll define and highlight the strengths and weaknesses of each of these search methodologies, discuss the role each should play in a modern search solution, and demonstrate where each fails to get the job done and also how they can best complement each other to optimize relevance. We'll walk through interactive, open source demos showing each of these three types of search in action, demonstrating how to balance the strengths, weaknesses and tradeoffs between them for different user intents and query types.", "title": "Thought Vectors, Knowledge Graphs, and Curious Death(?) of Keyword Search", "url": "https://2020.berlinbuzzwords.de/session/thought-vectors-knowledge-graphs-and-curious-death-keyword-search", "speaker": "Trey Grainger"}, {"level": "Intermediate", "track": "Search", "abstract": "One of the things I like, working as a consultant, is working on different challenges, find parallels between the various solutions, and learn a lot along the way. I worked for customers among the most prominent e-commerce, travel, and food industries in The Netherlands. In this talk, I want to share the top 10 lessons learned from doing search projects at these customers for the past ten years. Without giving away spoilers, the lessons vary in areas like organizational structure, KPIs, technology, content, and user experience. After the talk, you have some ideas and inspirations for improvements to your search projects.", "title": "Top 10 Lessons learned in search projects the past 10 years", "url": "https://2020.berlinbuzzwords.de/session/top-10-lessons-learned-search-projects-past-10-years", "speaker": "Jettro Coenradie"}, {"level": "Beginner", "track": "Search", "abstract": "More and more search teams in the e-commerce space want to own their search: they want to understand how exactly the retrieval works and optimise it according to their specific needs, both from the user and from the seller perspective. Implementing search using open source search engines, such as Solr and Elasticsearch, seems like a perfect match.\nUnfortunately, the open source solutions out of the box aren\u2019t anywhere near reaching parity with a commercial solution, especially when it comes to optimizing search relevance and managing individual queries as a merchandiser.\nIn this session we will introduce an initiative to combine open source tools and libraries like Querqy (query rewriting, for Solr and Elasticsearch), SMUI (search management), Quepid and RRE (search relevance assessment and tuning) into a project template to accelerate the development of your own e-commerce search, allowing you to shift from setting up basic search functionality to domain specific optimisations a lot faster.\u00a0\nJoin the creators and maintainers of these open source tools to discuss how we can build a powerful and\u00a0freely available platform for testing and tuning e-commerce search.\nThis\u00a0session does not require separate registration - your standard conference ticket will get you in.", "title": "Towards an open source tool stack for e-commerce search", "url": "https://2020.berlinbuzzwords.de/session/towards-open-source-tool-stack-e-commerce-search", "speaker": "Eric Pugh"}, {"level": "Beginner", "track": "Scale", "abstract": "The community is the heart of every open-source project. Without a proper functioning community, the project is bound to have a heart attack, which could be the end: Think of unsatisfied users, project forks, maintainer burnouts, or very annoying arguments.\nBuilding a community can be a daunting task but it doesn't have to be. All it takes is a critical mass of people who have a long-term interest in growing the community. Be it companies or individuals.\nAs it turns out, this group can systematically foster the community by building a framework for community interaction: writing down guidelines, helping users, onboarding new contributors, etc. The next step is to break out of the daily business through organizing conferences, meetups, or regular face-to-face meetings. Ultimately, the community is held together by relationships which are best strengthened via face time.\nIn the past years, I've seen two different communities grow: Apache Flink and Apache Beam. Both have found a way to grow their community. I'd like to present and analyze the paths they chose to build the strong communities they have today.", "title": "When Code Is Not Enough: A Guide to Building an Open-Source Community", "url": "https://2020.berlinbuzzwords.de/session/when-code-not-enough-guide-building-open-source-community", "speaker": "Maximilian Michels"}, {"level": "Intermediate", "track": "Stream", "abstract": "So you got the job of creating this great new streaming analysis on top of an existing data stream. To avoid overloading your laptop you start the project by listening to the data from the test environment which gives you a manageable volume. You get the data into Apache Flink or Beam and you see raw data coming in. Yet your first attempts at doing a very simple analysis on this data results in \u2026 nothing coming out. Then you simply take the raw data and use the advised way to write it to something like HBase ... and it takes 30 minutes for the records to appear in the database.\nWhat is going wrong?\nThe reality of streaming analytics is that the analytics works great on a continuous and big enough stream. Testing streams are very often \u201ctoo dry\u201d causing all kinds of basic systems in stream processing frameworks to behave differently from what you want. But not only the streams in the test environment are a problem, also streams that are used to process incoming files (i.e. batches on a stream) can be quite a problem to handle correctly.\nIn this talk I will go into some of the practical problems we ran into while building streaming applications with Apache Kafka, Apache Flink and similar tools over the last years. And I will show the solutions we use that allow people to\u00a0successfully\u00a0build analytics/processing solutions on\u00a0these \"not so streaming\"\u00a0streams.", "title": "When your stream doesn\u2019t stream very well.", "url": "https://2020.berlinbuzzwords.de/session/when-your-stream-doesnt-stream-very-well", "speaker": "Niels Basjes"}, {"level": "Beginner", "track": "Store", "abstract": "Apache Cassandra\u2122 is scalable NoSql database allowing you requesting over petabytes in real-time. During this session we will first review key architecture principles to identify proper use cases fitting this tool. We will then walk through data modelling methodology with Cassandra and recommended best practices.\nFrom there, fasten your seatbelt belt, as we will build a fully working application from scratch in front of you and deploy it in the cloud.\nPlease note: Registration for this workshop is now closed. Attendees who have registered for this workshop will receive an email with login details to the workshop platform.", "title": "From Zero to Cloud with Apache Cassandra\u2122", "url": "https://2020.berlinbuzzwords.de/session/zero-cloud-apache-cassandratm", "speaker": "Cedrick Lunven"}]