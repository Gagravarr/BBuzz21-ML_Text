[{"level": "Intermediate", "track": "Search", "abstract": "Building a chat system relies on recognizing known entities being spoken.\u00a0\u00a0 The Solr Tagger, a powerful unique capability of Apache Solr, provides performant tagging of known, concrete items in free text. Using the Solr Tagger as a first stage of query interpretation provides rich entity metadata that can be leveraged to hone in on user intent.\nThis session will introduce the Solr Tagger and its myriad of use cases culminating with a chat application that creatively uses the tagger for query interpretation.\n\u00a0\nThis talk is presented by Lucidworks", "title": "Chatting with Solr", "url": "https://2019.berlinbuzzwords.de/19/session/chatting-solr.html", "speaker": "Erik Hatcher"}, {"level": "Intermediate", "track": "Scale", "abstract": "In the last decades many systems have been used that were described as \"queues\" (AQ, ActiveMQ, RabbitMQ, etc.), yet from a computer science perspective these are not queues at all. Many of us have learned to work quite effectively with these messaging systems and we all understand that we cannot expect to receive the messages in any particular order and that we get all messages exactly once (which we can expect with a queue). With the arrival of Kafka and Flink a new class of applications became possible. In this talk I will go into several real applications from the bol.com context that all revolve around low latency behavioral analytics. I will talk about the entire end-to-end pipeline from the webbrowser and application server to application and discuss many of the things to think about when creating your analysis application. I will also touch upon using state machines as a way of doing this type of behavioral analysis using very simple software and show example algorithms from our context.", "title": "Measuring 2.0 - How to handle 100k events per second", "url": "https://2019.berlinbuzzwords.de/19/session/measuring-20-how-handle-100k-events-second.html", "speaker": "Niels Basjes"}, {"level": "Intermediate", "track": "Search", "abstract": "This talk will walk through the latest developments in Elasticsearch and related changes in Apache Lucene. We will look into significant query performance improvements, frozen indices, usage of the new soft-deletes feature for cross data-center replication and changes APIs and usability changes that makes search-as-you-type almost trivial.\nThe talk also presents the results of some long standing changes that we made in Lucene which are now paying off in geo-search use-cases and how they are integrated in Elasticsearch. Folks that are interested in distributed aspects of Elasticsearch will also hear about improvements in Zen 2 and Index-lifecycle-Management.\n\u00a0", "title": "What's evolving in Elasticsearch and Lucene?", "url": "https://2019.berlinbuzzwords.de/19/session/whats-evolving-elasticsearch-and-lucene.html", "speaker": "Simon Willnauer"}, {"level": "Beginner", "track": "Store", "abstract": "What defines a Legacy System and why is it considered Legacy? Perhaps you are writing one right now, it's just a matter of time to figure it out.\nDo buzzwords also become Legacy? In this talk we are going to review one historical buzzword Episode, see how trends repeat over and over, and what can be done to avoid pitfalls. Our behaviour is also related to it, so how are we doing on overall when compared to other areas?\nThis talk is presented by heycar", "title": "50 Shades of Legacy", "url": "https://2019.berlinbuzzwords.de/19/session/50-shades-legacy.html", "speaker": "Gregorio Kusowski"}, {"level": "Intermediate", "track": "Search", "abstract": "Search is fundamental feature of mobile.de platform and we as Data Team work hard to improve user experience by bringing personalization and relevancy into search flow. Hardest challenge is to cater to user interests while still displaying sponsored ads and obeying dealer interests. To solve it our team created and combined a number of data-driven products such as user profiling based on browsing history, car price ratings based on historical data from mobile.de inventory and ML model predicting click probability based on user tracking data. I will describe each of the products, highlighting the architecture and show how each of them fits into a big picture and integrated with existing search flow. Also I will share main learnings and outcomes.\nThis talk is presented by ebay tech.", "title": "Architecture of relevancy search at mobile.de", "url": "https://2019.berlinbuzzwords.de/19/session/architecture-relevancy-search-mobilede.html", "speaker": "Richard Knox"}, {"level": "Beginner", "track": "Scale", "abstract": "Non-code contributions, like project and community management, are essential to the healthy function of an Open Source project, however often times, they contributors who take on these tasks arent recognized nor made part of the key decision makers in a project. In this talk I'll cover success stories in not only recognizing but encouraging and empowering the recognition of contributions such as documentations, advocacy, community management and contributor mentoring. Ill also discuss best practices to incentivize these contributions and to use them as a bridge to welcome more diversity in your community.", "title": "Non-Code contributions: The hidden gem in Open Source Projects", "url": "https://2019.berlinbuzzwords.de/19/session/non-code-contributions-hidden-gem-open-source-projects.html", "speaker": "Griselda Cuevas"}, {"level": "Beginner", "track": "Scale", "abstract": "In my journey as a new participant in the Open Source world, Ive seen that Commercial Vendors often times contribute to OSS projects and get burned, sometimes for bragging to much about contributions and others for not preparing their employees to be good citizens when they play the game. Ive worked with my employer to develop tools and platform that help our employees be good OSS citizens but also effective employees, starting to enjoy contributing to Open Source and integrating their efforts to a healthy career development.", "title": "Playing fair in the Open Source World", "url": "https://2019.berlinbuzzwords.de/19/session/playing-fair-open-source-world.html", "speaker": "Griselda Cuevas"}, {"level": "Beginner", "track": "Scale", "abstract": "\"We have won\" - that's what I once heard about the trust that is being put into free and open source projects. Looking back at ten years of Berlin Buzzwords, it's hard to imagine how things could be any different: Projects developed at the Apache Software Foundation are basis on which tons of data analytics systems are built. But as a downstream user likely betting their core business on open source - how well do you understand how these projects are being governed, how direction is being set, how well they are equipped to survive the next decade? After all, there's more to open source than the source code itself.\nThe ASF provides a home to several hundred OSS projects - a lot of them relevant to the search, store, scale, stream ecosystem that Berlin Buzzwords targets. If you have been digging a bit deeper, you may have encountered the phrase \u201cThe Apache Way\u201d - often used in relation to how projects at the ASF should work.\nThis talk tries to give a glimpse behind the scenes - with a strong focus on what the implications of best practices advertised at the ASF are for downstream users of it\u2019s projects. (As a tiny hint: At the end of the day, it's all about pulling you in to become active and contribute yourself.)", "title": "10 editions of Berlin Buzzwords - A user's perspective on open source", "url": "https://2019.berlinbuzzwords.de/19/session/10-editions-berlin-buzzwords-users-perspective-open-source.html", "speaker": "Isabel Drost-Fromm"}, {"level": "Intermediate", "track": "Search", "abstract": "If you have ever searched for \u201cswimsuit\" on an ecommerce store and were shown a row of snow boots as products that \"customers frequently bought with,\u201d you have witnessed one instance where AI will fail for a long time without \u00a0human guidance and interventionn. Artificial Intelligence, without human oversight, will fail extraordinarily.One problem with visualizing artificial intelligence, particularly machine learning, is often the deep knowledge of mathematics needed to understand the language of its output. I will offer you a technical strategy and solution for visualizing your AI, outside of purely technical context, and in a way that is consumable by team members throughout the organization and quick to implement. Your business needs to deploy AI, or it will lag behind competition and lose market share until it dies. But your AI needs an audit trail. Through the use of open source visualization tools, you can transform complex mathematical data, like word vectors used in collaborative filtering for ecommerce, to ensure that your AI is one you can rely on and trust.\u00a0\n\u00a0", "title": "Visualizing the Output of Your Search AI", "url": "https://2019.berlinbuzzwords.de/19/session/visualizing-output-your-search-ai.html", "speaker": "Marcus Eagan"}, {"level": "Intermediate", "track": "Scale", "abstract": "To achieve high performance or interactive analytics on a big data set, most massively parallel processing (MPP) solutions resort to putting a large proportion of the dataset into memory and launch as many CPU cores as possible to deliver query results in time. By nature, MPP solutions could easily hit throughput bottlenecks at high concurrency or budget issues when datasets grow too large to fit in memory.\nApache Kylin proposed another solution to speed up analytical queries with pre-built OLAP Cube (which is essentially groups of aggregate tables). The Cubes are built with MapReduce/Spark on commodity hardware so that a large volume of datasets can be handled at a reasonable cost.\nThis talk will have the following detailed topics:\n- Apache Kylin background\n- Why OLAP Cube is needed for big data\n- How Kylin build the Cube on Hadoop\n- Performance benchmark\n- Use cases", "title": "Accelerate big data analytics with Apache Kylin", "url": "https://2019.berlinbuzzwords.de/19/session/accelerate-big-data-analytics-apache-kylin.html", "speaker": "shishaofeng Shi"}, {"level": "Intermediate", "track": "Scale", "abstract": "Although ARM processors are almost always viewed as having been designed for the embedded market, several vendors are making a bet and building server CPUs that contend with x86 in cloud deployments. With the presence of the Java ARM port and a wide variety of applications in the Java ecosystem able to run on ARM CPUs, the real question is which workloads are best suited to the ARM servers niche and which metrics can be optimized for using ARM servers.\nThis presentation explores the status of Java and the Java ecosystem on ARM, together with the Java ARM port features and performance of specific workloads. Some focus is on the recent changes in the Java ARM port, to which the speaker\u2019s company actively contributes.\n", "title": "Java on ARM. Theory, Applications, and Workloads", "url": "https://2019.berlinbuzzwords.de/19/session/java-arm-theory-applications-and-workloads.html", "speaker": "Dmitry Chuyko"}, {"level": "Intermediate", "track": "Scale", "abstract": "Machine learning is typically viewed as simply training a model on data. However, the \u201clast mile\u201d of deploying models to production systems is often overlooked and yet is one of the most critical aspects of real-world machine learning systems. Despite this, currently there are few widely accepted, open and standard solutions available that cover deployment of end-to-end ML pipelines.\nIn this talk, we explore the current state of ML deployment using open-source, standardized formats. The talk will cover the various available options, including\u00a0PMML,\u00a0PFA\u00a0and\u00a0ONNX, and how these fit in with the most popular and widely used ML libraries (including scikit-learn, Spark ML, TensorFlow, Keras and PyTorch).\n", "title": "Open Standards for Machine Learning Deployment", "url": "https://2019.berlinbuzzwords.de/19/session/open-standards-machine-learning-deployment.html", "speaker": "Nick Pentreath"}, {"level": "Beginner", "track": "Stream", "abstract": "An important underlying concept behind location-based applications is called geofencing. Geofencing is a process that allows acting on users and/or devices who enter/exit a specific geographical area, known as a geo-fence. A geo-fence can be dynamically generated\u2014as in a radius around a point location, or a geo-fence can be a predefined set of boundaries (such as secured areas, buildings, boarders of counties, states or countries).\nGeofencing lays the foundation for realizing use cases around fleet monitoring, asset tracking, phone tracking across cell sites, connected manufacturing, ride-sharing solutions and many others.\nGPS tracking tells\u00a0constantly and in real time where a device is located and forms the stream of events which needs to be\u00a0analyzed\u00a0against the much more static set of geo-fences.\u00a0Many of the use cases mentioned above require low-latency actions taken place, if either a device enters or leaves a geo-fence or when it is approaching such a geo-fence. That\u2019s where streaming data ingestion and streaming analytics and therefore the Kafka ecosystem comes into play.\nThis session will present how location analytics applications can be implemented using Kafka and KSQL & Kafka Streams. It highlights the exiting features available out-of-the-box and then shows how easy it is to extend it by custom defined functions (UDFs). The design of such solution so that it can scale with both an increasing amount of position events as well as geo-fences will be discussed as well.\u00a0\n", "title": "Location Analytics - Real-Time Geofencing using Kafka", "url": "https://2019.berlinbuzzwords.de/19/session/location-analytics-real-time-geofencing-using-kafka.html", "speaker": "Guido Schmutz"}, {"level": "Intermediate", "track": "Search", "abstract": "Natural language is gaining more and more relevance as an interface between man and machine. Already today, we are able to carry out simple task by talking to our smartphone or smart speaker, like Google Home or Alexa. An important challenge for any kind of dialog agent or chatbot is to include external knowledge into the conversation with the user. Therefore, such systems need to be able to interact with resources like relational databases or unstructured resources, like search engines. However, the complexity of natural language makes it hard to capture diverse utterances with a set pre-defined rules. Instead, we present an approach that leverages Deep Learning to learn how to query an Elasticsearch given natural language questions. As our model learns to follow the inherent logic of querying, it is even possible to switch to other systems and query languages. This carries a great potential for future applications of Elasticsearch and related NoSQL solutions.\n", "title": "Querying Elasticsearch with Deep Learning to Answer Natural Language Questions", "url": "https://2019.berlinbuzzwords.de/19/session/querying-elasticsearch-deep-learning-answer-natural-language-questions.html", "speaker": "Sebastian BlankHans-Peter Zorn"}, {"level": "Advanced", "track": "Scale", "abstract": "At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.\nThis talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.\n", "title": "Apache Beam pipelines at 100TB+ scale using Apache Spark.", "url": "https://2019.berlinbuzzwords.de/19/session/apache-beam-pipelines-100tb-scale-using-apache-spark.html", "speaker": "David Moravek"}, {"level": "Intermediate", "track": "Scale", "abstract": "Do you know the Abbey of the Crime?\nThe abbey is an 8-bit game (for spectrum and CPC) that became the first RPG game in 3D (2.5D) in 1987.\nThis game is a marvel from a technological point of view: in only less than 120k it is capable of storing the sound, the images, all the logic of the program and the data.\nDid you manage to finish the game without help?\nI do not know any human being who has passed it without help. It is one of the most complicated games that have been developed, like a 1000x compared to the revenge of Montezuma from Atari. The complexity is around (10 ** 10000)\nIn the talk we will tell how we design and build an AI capable of playing alone and learn to complete the game.\n\u00a0\n", "title": "How we use Reinforcement Learning to solve the abbey of the crime", "url": "https://2019.berlinbuzzwords.de/19/session/how-we-use-reinforcement-learning-solve-abbey-crime.html", "speaker": "Juantom\u00e1s Garcia Molina"}, {"level": "Intermediate", "track": "Scale", "abstract": "It was a cold day in December of 2018, I was sitting with my developer friends at a local pub in Oslo. Inspired by our conversations, I got the idea to finally fix the \"Tinder problem\". We're all so tired of swiping, it's about time we automated this thing! Never did I imagine what I'd get myself into, the intrigues and the drama that would follow. Sure, investigating the Tinder API was easy enough. Sure, building an Azure Function and deploying it to the cloud to fully automate Tinder was not a big deal. But eventually getting my account locked by Facebook and finding a security threat in the OAuth 2.0 protocol?! The Tinder service coincidentally going down and seeing this event unfold all over the media?! Never did I imagine I'd have so much fun and learn so much along the way.\nSit back, relax and come watch my talk. You're in for a wild ride! \u00a0\n", "title": "Not automating Tinder in the Cloud and the day I didn't break Tinder by accident", "url": "https://2019.berlinbuzzwords.de/19/session/not-automating-tinder-cloud-and-day-i-didnt-break-tinder-accident.html", "speaker": "Sirar Salih"}, {"level": "Intermediate", "track": "Search", "abstract": "You already have your\u00a0search engine\u00a0in place, users have started using it and now see that\u00a0your results could do better. You hire some engineers to get you through this and they included a synonym here, an exception there, and asked you to create a\u00a0taxonomy to organize your items. After a few months, \u00a0everything seems to work\u00a0fine except for a few minor issues that you got your team working on.\u00a0\nNow, your company needs to expand to meet the new demands of your business, so you get a wider range of products in your catalog. Your search results start to show some issues and your team is having a hard time controlling all of your search parameters and maintaining rules and exceptions. You know that improving your results will be the key to the success of your company. You now get yourself the biggest decision to make: should I hire more relevance engineers to handle it?\nThe first part of this talk will show why it is so complicated to scale an engineering approach of relevance based only on synonyms, taxonomies, rules, and exceptions. As a consequence of these limitations, the second part will focus on the latest advancements of natural language processing and information retrieval to show that all you need are good data scientists responsible for providing new data-driven solutions to your needs without causing any change into your search stack.\u00a0\nWith the vast\u00a0amounts of data collected, you can find applications of it for all your search needs. The talk will guide you through topics such as\u00a0language modeling for autocompletion, deep neural networks for named-entity recognition, network embeddings for relevance score, and\u00a0query-product embeddings to improve the discoverability of your most exquisite\u00a0items. All of this, optimized by your learning-to-rank algorithm and enabling you to personalization capabilities.\nWith all of these data-driven approaches, your search engine will be actively learning from the interaction of your users and there will be no need to hire an entire department of relevance engineers.\u00a0\n", "title": "Why data-driven methods will shape the future of relevance search", "url": "https://2019.berlinbuzzwords.de/19/session/why-data-driven-methods-will-shape-future-relevance-search.html", "speaker": "Pedro Balage"}, {"level": "Beginner", "track": "Search", "abstract": "The principal difficulty in implementing a code search engine is the\u00a0difference in syntax between the natural-language query and computer-language target. Because of this, engines based on traditional IR techniques often have difficulty returning relevant code snippets. In this talk we discuss DeepCS, presented by Gu et al at ICSE last year, which uses a deep learning based model to map method definitions and their corresponding textual descriptions to nearby locations in the same feature space. In so doing, this system\u00a0is also able to map natural-language queries to points in this feature space close to relevant code. This deep learning-based technique performs significantly better than Lucene-based systems, and even out-performs the state-of-the-art system\u00a0CodeHow.\n", "title": "DeepCS: a Code Search Tool Powered by Deep Learning", "url": "https://2019.berlinbuzzwords.de/19/session/deepcs-code-search-tool-powered-deep-learning.html", "speaker": "Robert Rodger"}, {"level": "Intermediate", "track": "Stream", "abstract": "Ray Tracing is an embarrassingly parallel way to render high quality images, but distributing work between machines is hard. Apache Beam is a model for efficient distributed data processing. In this talk I map Ray Tracing onto the Apache Beam Go SDK.\nApache Beam SDKs portably abstract computations and data into DoFns and PCollections, and allow you to construct a graph of how data flows and connects. Then any compatible Runner can optimize that graph for computation, and distribute work to\u00a0the runners.\nI'll give a overview of Ray Tracing\u00a0 and introduce the\u00a0Apache Beam model, it's history,\u00a0 demonstrating mapping one to the other. I'll explain the benefits and limits of the model and how to get the most out of your pipelines, and show it running on compatible runners like Apache Flink and Google Cloud Dataflow. Further, I'll show using the same pipeline running in batch and streaming modes, with minimal changes.\n", "title": "Writing a Distributed Ray Tracer with Apache Beam, Abridged", "url": "https://2019.berlinbuzzwords.de/19/session/writing-distributed-ray-tracer-apache-beam-abridged.html", "speaker": "Robert Burke"}, {"level": "Intermediate", "track": "Search", "abstract": "Berlin Buzzwords 2019 celebrates its 10th anniversay and every year Uwe presented a talk about news in Apache Lucene/Solr. Some of those contained funny details about heavy committing,\u00a0bugs in certain\u00a0Java versions,\u00a0enormous testing and understanding garbage collectors. This talk will present the details of 10 years Apache Lucene/Solr development starting with Lucene 2.9 / 3.0, released around the first Berlin Buzzwords conference. It will walk through the cool steps that led to what we have today like the start of numeric queries, merge of the Lucene and Solr projects, near realtime search, tokenstream attributes, flexible indexing (codecs), finite state automatons, doc values, bugs in Java 7, continuous hacking around\u00a0memory mapping, force merge confusion aka optimize, randomized testing and code verification with policeman's tools and introduction of Solr cloud.\nThis talk will not go too deep into details -\u00a0but as always -\u00a0Uwe will try to speak as fast as possible to give you an entertaining heavy committing policeman session!\n", "title": "10 (funny) years of Apache Lucene hacking", "url": "https://2019.berlinbuzzwords.de/19/session/10-funny-years-apache-lucene-hacking.html", "speaker": "Uwe Schindler"}, {"level": "Intermediate", "track": "Search", "abstract": "The User Experience on a mobile phone is different of that of a tablet and still different of that of a computer screen. You have to adapt your graphical interface, the information displayed but also the calls to your services.\nWhen the \u201cUser Interface\u201d is a \u201cvoice\u201d, the User Experience and the services your application provides need to adjust.\nIn this presentation we will adapt, step by step, an Enterprise Search Engine to Google Assistant and, more generally, to a Virtual Voice Assistant. The challenge will be to find the one and only meaningful result with a minimum of round trips while taking into account all the constraints of this type of User Interaction (among which also: \"being funny with the answers\").\n", "title": "Integrate your Search Engine with a Voice Assistant", "url": "https://2019.berlinbuzzwords.de/19/session/integrate-your-search-engine-voice-assistant.html", "speaker": "Lucian PrecupMargaux Wagner"}, {"level": "Intermediate", "track": "Scale", "abstract": "With the abundance of Remote Sensing satellite imagery available on public clouds(AWS), the possibilities are endless as to the kind of insights that can be derived from them. One such use is to determine the use of satellite imagery for predicting natural hazards like forest fires and the risk to residential localities like the recent forest fires in California.\nIn this talk, we\u2019ll be looking at leveraging the Near Infrared (NIR) bands from Landsat satellite imagery data to determine the health of vegetation and predicting the likelihood of next big forest fire based on the chlorophyll content in the vegetation that\u2019s captured in NIR bands of Landsat images. This, when combined with MLS data of properties in the region, can be used to determine the risk to property. \u00a0\nFor this talk, we\u2019ll be looking at building Deep Learning models built with Tensorflow to determine vegetation health. We will be showing how to ingest satellite data along with MLS datasets as different streams, and predicting the risk to a residential property due to a potential natural calamity.\nWe\u2019ll be leveraging streaming pipelines built on Apache Beam and TFX (Tensorflow Transforms) for preprocessing, model training and inference in real-time at scale. Developers will come away with a better understanding of how to analyze satellite imagery available on public clouds like AWS and leveraging Apache Beam Python pipelines on Apache Flink runner for real-time predictions.\n", "title": "Measuring Vegetation Health to Predict Natural Hazards", "url": "https://2019.berlinbuzzwords.de/19/session/measuring-vegetation-health-predict-natural-hazards.html", "speaker": "Suneel Marthi"}, {"level": "Advanced", "track": "Search", "abstract": "The current implementation of SpanNearQuery in Lucene sacrifices precision and completeness in favor of performance. This tradeoff significantly limits Lucene's usefulness for some potential high-value use cases. This talk introduces a stable patch that makes SpanQuery matching precise and complete, while maintaining performance comparable to that of the extant implementation.\nThe patch will be discussed in the context of the issue LUCENE-7398, describing the central problem and how it differs from the problem addressed by the new IntervalsSource API.\nWe will discuss details of the patch implementation, in an effort to familiarize the audience with techniques involved (and hopefully inspire further innovation). Details covered will include:\nThe main word-lattice data structure (linked, circular, resizable-array-backed, 2-dimensional queue with stable node references and support for efficient binary seek, serial traversal, and arbitrary node removal).\nPerformance/GC management for linked data structures (as opposed to the array-based data structures that are more common in the Lucene codebase)\nRecording in the index: positionLength, nextStartPosition lookahead, and information about possible decrease of endPosition (de-\"sausagization\" of the index)\n\nPossible implications/applications of robust support for precise, performant matching over complex indexed token graphs are significant:\nIndex-time synonyms that leverage document context (the current recommendation for supporting graph-based phrase queries supports synonyms exclusively at query-time)\nCJK search: index support for orthographic variation\nMore thorough scoring (e.g., variant forms, implicit boosting for exact orthographic matches, etc.)\nPotential for use in domains that prioritize precision, where \"phrase\" search could be used over data represented by complex token schemes (e.g., time-series data (scheduling, complex event processing), overlapping shingles, etc.)\n", "title": "Complete, precise graph-based phrase query with SpanNearQuery", "url": "https://2019.berlinbuzzwords.de/19/session/complete-precise-graph-based-phrase-query-spannearquery.html", "speaker": "Michael Gibney"}, {"level": "Intermediate", "track": "Search", "abstract": "Elasticsearch cluster coordination system, called Zen Discovery, got a rewrite in version 7.0. Starting from a formal model, the coordination layer was rebuilt to address multiple issues discovered over the years. This talk shows the main improvements of the new implementation: Master elections are much faster, the infamous\u00a0minimum_master_nodes setting has been removed,\u00a0growing and shrinking clusters becomes safer and easier, and leaves less room to misconfigure the system. Let us join the new, more Zen way of cluster coordination.\n", "title": "Reaching Zen in Elasticsearch's Coordination", "url": "https://2019.berlinbuzzwords.de/19/session/reaching-zen-elasticsearchs-coordination.html", "speaker": "Philipp Krenn"}, {"level": "Intermediate", "track": "Scale", "abstract": "Kubernetes has been widely accepted as the de-facto deployment orchestrator for managing and scaling containers in the cloud. While it's capabilities have been well acknowledged in the deployment space, its ability to manage business-specific workflows is yet to see a wide application.\nThis talk gives an insight into how we at Unbxd used 'Kubernetes Jobs' to build a Workflow Orchestration Engine that helps to configure and manage complex sequence of processes where the output of each step is used as an input for the next node. This architecture can also use\u00a0existing\u00a0microservices running on any platform as a node in the workflow and the data routing intelligence remains with the workflow orchestration layer (in the form of\u00a0fault-tolerant DAGs). \u00a0\u00a0\nThis\u00a0dynamic and configurable workflow also helps in scaling\u00a0the architecture well, as the inter-node data flow is controlled by the Orchestrator from within a Kubernetes pod (using a distributed message queue).\n", "title": "Managing Distributed Workflows at Scale - Kubernetes Jobs in Action", "url": "https://2019.berlinbuzzwords.de/19/session/managing-distributed-workflows-scale-kubernetes-jobs-action.html", "speaker": "Abhishek Kumar Singh"}, {"level": "Beginner", "track": "Store", "abstract": "Machine learning is rapidly being democratized and is becoming something that developers at large can use effectively. To do so, however, developers need to add appropriate data skills to their coding skills. It\u2019s not the advanced algorithms and modelling they must learn -- that may be left to the data scientists. Instead, the most important part of a machine learning system turns out to be the data preparation itself and the importance of the data is often underestimated, especially by people new to these approaches. Data preparation is essential to learn, and clever techniques for data exploration, feature extraction and data versioning can be applied across a wide variety of machine learning projects. In fact, in some cases, data exploration and feature extraction actually reveal solutions that can actually make the machine learning part of a machine learning system optional. Honing data preparation skills is useful for both experienced data scientists and for newcomers.\nThis presentation will cover practical techniques for data engineering with specific examples. We will focus on three areas of data preparation: data exploration, feature extraction and data management for machine learning systems, and we will give examples from real world stories of where these approaches are effective. The approaches we examine are powerful yet simple enough for people new to machine learning to easily understand, and they may surprise more experienced data scientists as well. We include the rationale behind the approaches along with specific implementation to make it easier for the audience to apply these techniques to their own situations.\n", "title": "Doing Data: The Critical Process of Data Preparation for Machine Learning and More", "url": "https://2019.berlinbuzzwords.de/19/session/doing-data-critical-process-data-preparation-machine-learning-and-more.html", "speaker": "Ellen Friedman"}, {"level": "Beginner", "track": "Stream", "abstract": "Flink ML can be used to apply Machine Learning models on a stream of data and make decisions based on the output of the model. The same can also be applied to the problem of cloud security as well.\nCloud data collaboration platforms provide immense flexibility to store, share and manage access control. However, a lack of heuristic-based data breach detection leads to a serious security loophole in the current systems. With the amount of data growing at a staggering rate every day and increasing number of users accessing the resources, the problem gets even more critical.\nThis talk will provide an overview of how Apache Kafka and Flink ML can be used to apply various Machine Learning models to flag malicious access patterns mined from cloud activity events in Near Real-Time and how it can be scaled to high throughput systems.\n", "title": "Analysing Real time Activity Streams for Security: Flink ML to the Rescue.", "url": "https://2019.berlinbuzzwords.de/19/session/analysing-real-time-activity-streams-security-flink-ml-rescue.html", "speaker": "Rashmi Singh"}, {"level": "Intermediate", "track": "Stream", "abstract": "Bias in NLP 101 talks about the biases present in Texts based for Men and Women. For this, a dataset based on movies reviews from India is used and worked on by IBM.\u00a0The\u00a0intention is on having various situations where we can check if the model predicts a biased result as based on the stereotypical notions developed in the society for both men and women. Since movies replicate the reality in the most significant way their reviews being less in length would get biased. To de-bias\u00a0the system, a model called DeCogTeller is used which results in the correct form of Gender.\u00a0This project is a Talk Session on Machine Learning and Natural Language Processing and also brings out the challenges companies like Google, Amazon and Facebook faced while recruiting women.\u00a0\nLink to the presentation:\u00a0http://bit.ly/ppt_buzzwords\n", "title": "Bias in NLP 101", "url": "https://2019.berlinbuzzwords.de/19/session/bias-nlp-101.html", "speaker": "SAKSHI SHUKLA"}, {"level": "Beginner", "track": "Search", "abstract": "There are some great Open Source text search / information retrieval systems, such as Apache SOLR and ElasticSearch. But could an AI / ML powered solution do better? And how would you even go about building one?\nBased on our experiences building a knowledge base / Q&A system, we'll guide you through the process. Learn how to get your text into a format that AI / ML techniques can work on, and how to build a simple model and recommender. Then it's Deep Learning and Neural Networks, and finally updating the models with real user feedback. Oh, and comparing it to a traditional search engine, to see if it's actually any better...\nNo AI or hard-core-search experience needed, we'll show you the code and techniques required to create your own!\n", "title": "Building an AI/ML powered text search system", "url": "https://2019.berlinbuzzwords.de/19/session/building-aiml-powered-text-search-system.html", "speaker": "Nick Burch"}, {"level": "Intermediate", "track": "Scale", "abstract": "Hive tables are an integral part of the big data ecosystem, but the simple directory-based design that made them ubiquitous is increasingly problematic. Netflix uses tables backed by S3 that, like other object stores, don\u2019t fit this directory-based model: listings are much slower, renames are not atomic, and results are eventually consistent. Even tables in HDFS are problematic at scale, and reliable query behavior requires readers to acquire locks and wait.\nI will present an overview of Apache Iceberg, a new open source project that defines a new table layout addresses the challenges of current Hive tables, with properties specifically designed for cloud object stores, such as S3. Iceberg is joined Apache Incubator last year. It specifies the portable table format and standardizes many important features, including:\nAll reads use snapshot isolation without locking.\nNo directory listings are required for query planning.\nFiles can be added, removed, or replaced atomically.\nFull schema evolution supports changes in the table over time.\nPartitioning evolution enables changes to the physical layout without breaking existing queries.\nData files are stored as Avro, ORC, or Parquet.\nSupport for Spark, Hive, and Presto.\n", "title": "Introducing Apache Iceberg: Tables Designed for Object Stores", "url": "https://2019.berlinbuzzwords.de/19/session/introducing-apache-iceberg-tables-designed-object-stores.html", "speaker": "Owen O'Malley"}, {"level": "Beginner", "track": "Stream", "abstract": "In this workshop, you will become familiar with how to build serverless applications using Serverless Framework (serverless.com). Serverless Framework enables you to quickly define and configure your application as well as the resources it needs using simple yaml files, and then deploy it to your cloud platform of choice. In this workshop we will be using AWS Lambda. The application itself can be written using a variety of programming languages, such as JavaScript, Python or Java. We will cover how to create a REST API to call your functions, as well as how to use other triggers such as file uploads to S3, modifications in DynamoDB tables, or Kinesis event streams.\n", "title": "Building applications with Serverless Framework and AWS Lambda", "url": "https://2019.berlinbuzzwords.de/19/session/building-applications-serverless-framework-and-aws-lambda.html", "speaker": "Fredrik Vraalsen"}, {"level": "Intermediate", "track": "Stream", "abstract": "With the rise of Tensorflow and libraries like Numpy, Python has become a popular choice for data processing. Applications built with Python are commonly single-node applications and need to be parallelized in order to scale for big amounts of data. Turns out, JVM-based languages are often the only choice to leverage the power of large-scale data processing tools like Apache Flink or Apache Spark.\nThis talk introduces Apache Beam, an open-source data processing framework for large-scale batch and stream processing which is designed with portability in mind. Apache Beam lets you use languages like Python, Go, Java, and Scala for data processing. Even better, the resulting programs can be run on the execution engine of your choice.\nWe will show how easy it is to run data processing jobs on Apache Beam and provide insight into different aspects of Apache Beam's portability architecture. In particular how Beam programs\nexecute on top of different execution engines like Apache Spark, Apache Flink, or Google Cloud Dataflow\nsupport multiple languages like Python, Go, and Java\n\nApache Beam's portability avoids being locked into a single execution engine or programming language. Moreover, portability enables completely new use cases, e.g. to create data processing jobs which mix multiple languages, to reuse Java IO connectors for loading/storing data from a Python job, or to use libraries (e.g. for machine learning) that do not exist in the main language of the data processing job.\nPlease join us to learn more about the future of data processing where users are free to choose their programming language and execution engine.\n\u00a0\n", "title": "Python, Java, or Go: It's Your Choice with Apache Beam", "url": "https://2019.berlinbuzzwords.de/19/session/python-java-or-go-its-your-choice-apache-beam.html", "speaker": "Maximilian MichelsIsma\u00ebl Mej\u00eda"}, {"level": "Beginner", "track": "Search", "abstract": "This talk is about building Slack\u2019s Search Infrastructure. Solr powers search across messages, files and many use-cases at Slack.\nWe\u2019ll first talk about how we can build our indexes billions of documents regularly.\nHow do we support our language internationalization efforts and what that means to search quality and latency.\nLastly the talk will discuss some of the biggest challenges we face today and how we plan on tackling them\n", "title": "Evolution of Slack Search", "url": "https://2019.berlinbuzzwords.de/19/session/evolution-slack-search.html", "speaker": "Varun Thacker"}, {"level": "Intermediate", "track": "Store", "abstract": "In the space of building products with data, either by dealing with huge amounts of data or by applying machine learning, many different ecosystems meet. Larger volumes of data have to be passed between these systems. The handling of the data is not only down to divide between systems written in Java that need to pass it on to the machine learning model in Python. When you take into account that you want to integrate with the existing business infrastructure, you also need to cater for legacy systems as well do you need to bring the large volumes of data to the user via UIs.\nSwitching between each of these ecosystems is often expensive. You either have a simple but slow adapter or have spent a lot of human effort in building an efficient interchange. Apache Arrow started three years ago as a Apache top-level project to eliminate a lot of overhead in this sector. As part of this talk, we would like to show how to move large amounts of data from Java to Python and then onto JavaScript without leaving the comfort zone of each ecosystem. An important aspect is that everyone should be able to use their usual tools without a deep knowledge of the other's ecosystem while still providing data exchange at high speed.\u00a0This should enable developers to remove the overhead of writing conversion code from their data pipelines and focus on the actual business requirements.\n", "title": "Taming the language border in data analytics and science with Apache Arrow", "url": "https://2019.berlinbuzzwords.de/19/session/taming-language-border-data-analytics-and-science-apache-arrow.html", "speaker": "Uwe Korn"}, {"level": "Intermediate", "track": "Search", "abstract": "Modern search engines leverage Natural Language Processing (NLP) and Machine Learning (ML) to improve relevance of results. In this presentation, we focus on the specific field of \u2018Enterprise Search\u2019, whose primary goal is to make domain specific company data and documents readily accessible to employees to improve their productivity and promote collaboration. Indeed, any large organization produces vast amounts of documentation about their specific systems, technologies and processes. The question then is - \u201cHow can we speed up search-driven activities and enhance user experience for the enterprise?\u201d\nFacebook AI research team has developed and open sourced a tool to answer questions by reading Wikipedia articles, called DrQA (Chen, et al. 2017). DrQA is based on a 2-stage Q&A pipeline: (i) Retriever: retrieve the top-k relevant documents (pages), followed by (ii) Reader: determining the most relevant answer span among the retrieved documents (pages). We applied it on an enterprise use-case to search over machine manuals used by factory operators.\nWe present an architecture integrating ElasticSearch in the DrQA pipeline, which has been contributed upstream and is now available from the official DrQA github repository (https://github.com/facebookresearch/DrQA). The end result is a very scalable search engine that can be deployed on any document repository in your enterprise containing Microsoft Office docs, ppts, emails, pdf documents, etc. Simply point it to your ElasticSearch index and it will be able to provide \u2018very precise\u2019 answers based on your documents, thanks to the pre-trained Deep Learning Q&A model of DrQA. We discuss the learnings along the creation and the limitations of such an engine, e.g. scenarios where it excels by identifying precise answers and how it performs compared to a non-ML approach or a typical keyword based search.\n", "title": "Building an enterprise Natural Language Search Engine with ElasticSearch and Facebook\u2019s DrQA", "url": "https://2019.berlinbuzzwords.de/19/session/building-enterprise-natural-language-search-engine-elasticsearch-and-facebooks-drqa.html", "speaker": "Debmalya BiswasLouis Baligand"}, {"level": "Intermediate", "track": "Search", "abstract": "The Search Infrastructure team at Bloomberg runs over a thousand Solr clouds spread across hundreds of machines. This scale creates significant challenges in managing hardware resources. Beyond just managing where clouds are allocated and the resources available across the cluster, tasks that affect multiple clouds, such as upgrading OS versions or taking a machine down for maintenance, can grow into serious undertakings.\nKubernetes is a system designed to help orchestrate large scale applications. However, it has room for improvement in use cases such as running on physical hardware or managing stateful applications. In this talk, Houston Putman will detail how the team has addressed these issues and begun to run production Solr clouds on Kubernetes. He will also share his experience and the performance characteristics when running Solr on Kubernetes vs. on bare metal.\n", "title": "Running Solr within Kubernetes at Scale", "url": "https://2019.berlinbuzzwords.de/19/session/running-solr-within-kubernetes-scale.html", "speaker": "Houston Putman"}, {"level": "Intermediate", "track": "Search", "abstract": "Over the last few years, Apache Solr has evolved to encompass a wide range of new, advanced functionality to make it one of the most widely used solutions for Information Retrieval and Analytics. With this new responsibility, deployments have grown substantially in size and cover multiple use cases. These larger deployments\u00a0now ingest substantial quantities of data and perform increasingly complex queries.\nAt large companies most use-cases are generally consolidated and hosted by a single multi-tenant platform. All of these use cases generally span different teams, with different data volumes, throughput, and service expectations. As with most complex systems, where requests are often generated by the clients programmatically, the requests are more likely to be harmful for the health of the system. This is only amplified by the fact that Solr is currently unbounded by design. As we and our users push it to the limits, it has become increasingly easy to cause catastrophic failures that cannot easily be recovered from.\nThe failures often lead to degraded performance and even extended downtimes for systems that are playing increasingly critical roles in businesses. In addition, recovering from these situations almost always have high operational cost, distracting the platform owners from building new things.\nDuring this talk, I would like to discuss common, catastrophic vulnerabilities encountered after running Solr at scale for multiple years and the newly introduced features to protect against those. These vulnerabilities include, but are not limited to, issues encountered when either too many fields are added to Solr, leading to out of memory\u00a0exceptions and a stalled system or cores growing too large due to an unexpected indexing spike. At the end of this talk, the attendees would be better equipped with not only hosting a more stable multi-tenant search platform built on Solr, but even applying the same ideas to almost all scalable, distributed platforms.\n", "title": "Keeping your search platform up and running - new self-defense features in Solr", "url": "https://2019.berlinbuzzwords.de/19/session/keeping-your-search-platform-and-running-new-self-defense-features-solr.html", "speaker": "Anshum Gupta"}, {"level": "Intermediate", "track": "Stream", "abstract": "Data Scientists using one set of tools, predominantly in Python or R with lots of file based data. \u00a0Engineers deploying production systems using different programming languages and primarily online databases. \u00a0\u00a0This is a common pattern, and leads to silos between these two groups.\n\u00a0\nIn this talk, Eric will share what he\u2019s learned in creating a project structure that will feel at home for both Data Scientists and Engineers using Apache Zeppelin and Docker. \u00a0\u00a0The project structure he\u2019ll share is heavily influenced by the Cookie Cutter Data Science Project (https://drivendata.github.io/cookiecutter-data-science/) that is \u201cA logical, reasonably standardized, but flexible project structure for doing and sharing data science work\u201d, but embracing the richness of 20+ programming languages and data stores that Apache Zeppelin connects to. \u00a0All code will be available via Github for you to play with.\n\u00a0\nHe\u2019ll demonstrate using Zeppelin to expose web analytics rollups, to do ETL processing, and to enrich datasets with NLP processing.\n\u00a0\nThis talk will serve as as a great intro to Apache Zeppelin, and if you are already using Jupyter, will encourage you to take a look at this competitor! \u00a0If you are already using Zeppelin, then you\u2019ll be interested in how to use Zeppelin for more than just the core task of interactive data analytics, and indeed it is a great environment for rapid prototyping of the backend of many intensive data processing projects. \u00a0\u00a0\n", "title": "It\u2019s a Balloon! A Blimp! No, a Dirigible! Apache Zeppelin: An ETL Engine and Data Science Notebook That Connects Programmers to Data Scientists and Business Analysts!", "url": "https://2019.berlinbuzzwords.de/19/session/its-balloon-blimp-no-dirigible-apache-zeppelin-etl-engine-and-data-science-notebook.html", "speaker": "Eric Pugh"}, {"level": "Advanced", "track": "Store", "abstract": "As the world migrates to Kubernetes on the cloud, companies have also been migrating their data processing jobs. The big advantage of the cloud and Kubernetes is that you can add and remove resources as needed. This also allow different strategies of handle the problems. However, there are challenges involved with stateful and long-running applications in cluster managers. This talk will discuss how to leverage tools to run big data jobs, such as Spark, Flink, and HDFS, in an on-demand way on top of Kubernetes. We also look at possible pitfalls, and ways of addressing them.\nTopics will include:\ndifferent opportunities in the cloud\nWhat tools can be used to help with deployment and management\nCluster auto-scaling\nDeployment of big data jobs\nTuning and optimization on big data and kubernetes side\u00a0\n", "title": "Dynamic Big Data in the Cloud with Kubernetes", "url": "https://2019.berlinbuzzwords.de/19/session/dynamic-big-data-cloud-kubernetes.html", "speaker": "Frank Conrad"}, {"level": "Intermediate", "track": "Stream", "abstract": "You wouldn\u2019t dream of developing a modern data processing system without defining schemas for your data to properly annotate whether data elements are dates, strings, integers or reals. But annotating values as numeric leaves a lot unsaid. Is my throughput in queries per second or queries per minute? Is this airline flight distance in miles or kilometers? Is that fluid volume in cubic centimeters or liters?\nPractically every numeric value we manipulate in our software has an implied unit. We learn how to check our unit analysis in high school science, but our development tools provide no support for keeping track of units or enforcing their correctness! In modern distributed systems operating at scale, errors caused by simple unit mistakes can cost you valuable engineering time and generate weeks of corrupted data.\nIt doesn\u2019t have to be this way. A unit is really a kind of data type. What if our compilers could check units along with all the other data types we use every day? Languages with modern type systems, such as Scala, Haskell and Rust, open up new possibilities for representing units as compiler checkable types with constructs like type-classes and dependent types.\nIn this talk, we will explain the challenges around supporting units as static data types, and approaches to solving them with modern type systems. We will discuss a range of compelling use cases, from compiler verified schema to distributed data science pipelines to type safe internationalization. Concepts will be illustrated with concrete code, using a Scala library for compile-time unit analysis. The audience will discover what our software systems could look like in a world with first class unit types.\n", "title": "Miles or Kilometers? Why Your Data Schema Should Include Units", "url": "https://2019.berlinbuzzwords.de/19/session/miles-or-kilometers-why-your-data-schema-should-include-units.html", "speaker": "Erik Erlandson"}, {"level": "Intermediate", "track": "Store", "abstract": "Hops is a European open-source, next-generation distribution of Apache Hadoop that is being repurposed for the cloud. In this talk, we will walk through some of recent technical developments in Hops, including solving the small files problem by stuffing them in metadata using NVMe disks, free-text search of file system with extended metadata (this is great for automated annotation of millions of images and then finding them in milliseconds with consistent), and most interestingly data-center level HA for HopsFS with millions of filesystem operations per second on real industrial workloads. So yes, we will tell you why a POSIX-style hierarchical filesystem with indexed extensible metadata is superior to an object store. Finally, we can show you what else you can do with Hops, and how we built Hopsworks, a horizontally scalable secure platform for Data and AI, using Hops' extended metadata.\n", "title": "Hops in the Cloud", "url": "https://2019.berlinbuzzwords.de/19/session/hops-cloud.html", "speaker": "Jim Dowling"}, {"level": "Intermediate", "track": "Stream", "abstract": "Race conditions and intermittent failures, daylight saving time, time zones, leap seconds, and\u00a0overload conditions - time is a factor in many of the most annoying problems in computer systems. Data engineering is not exempt from problems caused by time, but also has a slew of unique problems. In this presentation, we will enumerate the time-related problems that we have seen cause trouble in data processing system components, including data collection, batch processing, workflow orchestration, and stream processing. We will provide examples of time-related incidents, and\u00a0also tools and tricks to avoid timing issues in data processing systems.\n\u00a0\n", "title": "Eventually, time will kill your data pipeline", "url": "https://2019.berlinbuzzwords.de/19/session/eventually-time-will-kill-your-data-pipeline.html", "speaker": "Lars Albertsson"}, {"level": "Intermediate", "track": "Search", "abstract": "It\u2019s 3am and your phone wakes you up. A service you own is having a problem. This is an all too familiar issue for application and infrastructure teams alike. When that team is Bloomberg's Search Infrastructure team and the global financial markets are relying on the services you provide, you have to get up and fix it right away.\nBut how did you learn about the problem or how severe it is in the first place? And how do you scale that for hundreds and thousands of services? What can you do to ensure your services are performing within your SLAs - despite peak load (a question that becomes even more interesting when many of those services are managed using Kubernetes)?\nBloomberg's Search Infrastructure team has created a holistic, extensible and configurable monitoring solution for large scale distributed systems. Our solution allows us to scale monitoring both horizontally (the types of services) and vertically (the number of services). In this talk, I will discuss how our approach has evolved as the number of services we monitor has increased dramatically. I will detail how we leveraged Kafka\u00a0to improve our reliability and unlink our monitoring and alarming solutions. Finally, I\u2019ll demonstrate how ChatOps have helped us all get a good night's sleep.\n", "title": "Monitoring Solr at Scale", "url": "https://2019.berlinbuzzwords.de/19/session/monitoring-solr-scale.html", "speaker": "Ken LaPorte"}, {"level": "Intermediate", "track": "Search", "abstract": "Good engineering documentation not only drives adoption (48% of businesses say that documentation was the deciding factor in choosing to adopt an open source technology) and supports developers; it's also critical to supporting community contributors. Clear documentation of a project's processes, such as contributor guides and codes of conduct, are especially valued by under-represented groups and are vital to building inclusive communities. But in practice, 93% of OSS devs complain about the lack of good, reliable documentation, making it the #1 problem they face when working with open source software.\nHow can we fix this? In Q1 2019, Google's Open Source Strategy team commissioned a user research study to better understand why users do (or do not) contribute back to the project, and how documentation can help remove roadblocks to that project's adoption and usage - and encourage and enable community contributions.\nIn this talk, we'll cover the methodology and participant profiles of the study, and describe the three essential user personas we identified - along with their critical user journeys. And we'll also distill the research to provide concrete recommendations and best practices for creating documentation that helps your community flourish.\n", "title": "From User to Contributor: How documentation enables vibrant open source communities", "url": "https://2019.berlinbuzzwords.de/19/session/user-contributor-how-documentation-enables-vibrant-open-source-communities.html", "speaker": "Aizhamal Nurmamat kyzyRiona MacNamara"}, {"level": "Intermediate", "track": "Search", "abstract": "After many years running its own in-house C++ search engine, Amazon is exploring moving its customer facing e-commerce product search to Apache Lucene (tm), serving millions of customers each day worldwide. Solr, Elasticsearch and other Lucene derivatives have been used widely for many years at Amazon, but until now the .com product search has been powered by a proprietary in-house engine. We'll discuss why we decided to adopt open source for this vital technology and dive deep into the technical challenges we faced in replicating our legacy engine's behavior, pointing out novel uses of Lucene along the way. Highlights will include:\nOur open-source contributions: concurrent deletions and updates, custom term frequencies, improvement to taxonomy faceting\nIndex replication on S3: a near-real-time segment-based index replication strategy backed by cheap cloud storage that provides excellent scalability and durability.\nMerging: fully merging all index segments may be harmful! We'll discuss why our query latencies increase when the index is fully merged.\nRanking: stringent performance requirements together with complex machine-learned ranking models make for an uneasy marriage. We'll explain how index sorting with early termination combined with multiphase ranking make it possible to have both.\nOffers/families: how we model offers for a single product, and families with multiple products, using index-time joins.\nScoring: custom term frequencies based on various machine-learned signals feeding into extensive custom scoring function library including compiled expressions giving performance that is competitive with C++ functions.\nGarbage collection: we had 8 second stop-the-world pauses and had to dive deep to understand why and correct it. We're using Lucene 7.5.0, JDK11 with the deprecated concurrent garbage collector, CMS. We tried FSTPostingsFormat, then had to revert it. We reduced RAM used by our complex hierarchical index configuration.\nQuery caching: we tried enabling Lucene's query cache, and it didn't help, but we saw nice gains from indexing commonly occurring sub-queries and optimizing the corresponding clauses at search time.\n", "title": "E-Commerce search at scale on Apache Lucene (tm)", "url": "https://2019.berlinbuzzwords.de/19/session/e-commerce-search-scale-apache-lucene-tm.html", "speaker": "Michael SokolovMike McCandless"}, {"level": "Intermediate", "track": "Search", "abstract": "With the advances in deep learning and the corresponding increase in machine learning frameworks in recent years, a new class of software has emerged: model servers. These promise, among other things, performance and scalability. There is however a large class of applications where such model servers are inadequate. For instance, search and recommendation applications must efficiently evaluate models over potentially many thousands of data points as part of handling a query. In such cases the amount of data transferred to the model servers can quickly saturate the network and thus decrease total system throughput and degrade quality of service.\nIn this talk we present a solution to this problem which is to evaluate the models where data is stored rather than moving data to where the model is hosted. We base our solution on Vespa, an open-sourced platform developed at Yahoo for building scalable real-time data processing applications over large data sets. Vespa has\u00a0 native features to import ONNX and TensorFlow models and represent the computational graphs in its internal tensor language.\u00a0In this talk we will show how this achieves model evaluation performance at web-scale, and that even if one does not take advantage of specialized hardware such as GPUs and TPUs, the total system throughput can scale much better.\n\u00a0\n", "title": "Scaling ONNX and TensorFlow model evaluation in search", "url": "https://2019.berlinbuzzwords.de/19/session/scaling-onnx-and-tensorflow-model-evaluation-search.html", "speaker": "Lester Solbakken"}, {"level": "Beginner", "track": "Stream", "abstract": "Every day, companies\u00a0rely\u00a0on data to guide every single business process and decision. Missing or incorrect information seriously compromises the customer experience and any decision process downstream. Therefore, a crucial, but tedious task for every team involved in data processing is to verify the quality of their data. In this talk, we will show how to continuously verify data quality by defining metrics and constraints, resulting in better testing for data pipelines and machine learning applications.\n\u00a0\nLink to open source repo: https://github.com/awslabs/deequ.\n\u00a0\n\u00a0\n", "title": "Large-scale Data Quality Verification - How to Unit-test Your Data with deequ", "url": "https://2019.berlinbuzzwords.de/19/session/large-scale-data-quality-verification-how-unit-test-your-data-deequ.html", "speaker": "Philipp Schmidt"}, {"level": "Advanced", "track": "Stream", "abstract": "Integrating Apache Kafka with other systems in a reliable and scalable way is often a key part of a streaming platform. Fortunately, Apache Kafka includes the Connect API that enables streaming integration both in and out of Kafka. Like any technology, understanding its architecture and deployment patterns is key to successful use, as is knowing where to go looking when things aren\u2019t working.\nThis talk will discuss the key design concepts within Kafka Connect and the pros and cons of standalone vs distributed deployment modes. We\u2019ll do a live demo of building pipelines with Kafka Connect for streaming data in from databases, and out to targets including Elasticsearch. With some gremlins along the way, we\u2019ll go hands-on in methodically diagnosing and resolving common issues encountered with Kafka Connect. The talk will finish off by discussing more advanced topics including Single Message Transforms, and deployment of Kafka Connect in containers.\n", "title": "From Zero to Hero with Apache Kafka's Connect API", "url": "https://2019.berlinbuzzwords.de/19/session/zero-hero-apache-kafkas-connect-api.html", "speaker": "Robin Moffatt"}, {"level": "Beginner", "track": "Scale", "abstract": "Buoyed by expensive industrial research efforts, amazing engineering breakthroughs, and an ever-increasing volume of training data, machine learning techniques have recently seen successes on problems that seemed largely intractable twenty years ago. \u00a0However, beneath awe-inspiring demos and impressive real-world results, there are cracks in the foundation: ordinary organizations struggle to get real insight or value out of their data and wonder how they\u2019ve missed out on the promised democratization of AI and machine learning. \u00a0\nThis talk will diagnose how we got to this point. \u00a0You\u2019ll see how the incentives and rhetoric of software and infrastructure vendors have led to inflated expectations. \u00a0We\u2019ll show how internal political pressures can encourage teams to aim for moonshots instead of realistic and meaningful goals. \u00a0You\u2019ll learn why contemporary frameworks that have enjoyed prominent successes on perception problems are almost certainly not the best fit for gleaning insights from structured business data. \u00a0Finally, you\u2019ll see why many of the solutions the industry has offered to real-world machine learning woes are essentially \u201cbandages\u201d that cover deep problems without addressing their causes.\nThis talk won\u2019t merely offer a diagnosis without a prescription; we\u2019ll conclude by showing that the way to avoid disappointing machine learning initiatives in the future isn\u2019t a patchwork of superficial fixes to help us ignore that we\u2019re solving the wrong problems. \u00a0Instead, we need to radically simplify the way we approach learning from data by embracing broader definitions of \u201cAI\u201d and \u201cmachine learning.\u201d Organizations should prioritize results over emulating research labs and practitioners should focus first on fundamental techniques including summaries, sketches, and straightforward models. \u00a0These techniques are unlikely to attract acclaim on social media or in the technology press, but they are broadly applicable, allow practitioners to realize business value quickly, produce interpretable results, and truly democratize machine intelligence.\n", "title": "Band-Aids don\u2019t fix bullet holes: Repairing the broken promises of ubiquitous machine learning", "url": "https://2019.berlinbuzzwords.de/19/session/band-aids-dont-fix-bullet-holes-repairing-broken-promises-ubiquitous-machine-learning.html", "speaker": "William Benton"}, {"level": "Intermediate", "track": "Stream", "abstract": "Flink currently features different APIs for bounded/batch (DataSet) and streaming (DataStream) programs. And while the DataStream API can handle batch use cases, it is much less efficient in that compared to the DataSet API. The Table API was built as a unified API on top of both, to cover batch and streaming with the same API, and under the hood delegate to either DataSet or DataStream.\nIn this talk, we present the latest on the Flink community's efforts to rework the APIs and the stack for better unified batch & streaming experience. We will discuss:\n- The future roles and interplay of DataSet, DataStream, and Table API\n- The new Flink stack and the abstractions on which these APIs will build\n- The new unified batch/streaming sources\n- How batch and streaming optimizations differ in the runtime, and what the future interplay of batch and streaming execution could look like\n", "title": "Towards Flink 2.0: Rethinking the stack and APIs to unify Batch & Stream", "url": "https://2019.berlinbuzzwords.de/19/session/towards-flink-20-rethinking-stack-and-apis-unify-batch-stream.html", "speaker": "Stephan Ewen"}, {"level": "Advanced", "track": "Search", "abstract": "Yelp\u2019s core business search was one of the oldest systems at Yelp designed well before the advent of Elasticsearch. While this system served Yelp well for a few years, we were getting close to a point where the original search infrastructure architecture was not sufficient to solve modern day search problems.\nThis talk will detail\nHow Yelp\u2019s search engineers decoupled the search infrastructure from search relevance.\nThe challenges associated with transferring complex custom lucene based ranking and text analysis functionality to elasticsearch.\nThe benefits of offloading search infrastructure to elasticsearch and\nFinally the dividends it paid by allowing us to further leverage our technological investment in elasticsearch, by hosting machine learning models in elasticsearch by using Learning to Rank plugin, and making contributions to it.\n", "title": "Evolution of Yelp search to a ranking platform", "url": "https://2019.berlinbuzzwords.de/19/session/evolution-yelp-search-ranking-platform.html", "speaker": "Umesh Dangat"}, {"level": "Beginner", "track": "Scale", "abstract": "Off-the-shelf recommenders are a necessary building block for developing a personalised recommendation system, but they are not sufficient to solve personalisation problems by themselves. Such systems are designed to use either explicit or implicit data, but never both. They are unable to identify anomalous user activity, and cannot determine when one account is put to multiple distinct uses, for example combined personal and business purchases, or many users in a household sharing streaming media.\n\u00a0\nThis talk will look at the pain points of recommendation algorithms and cover ways to overcome them in practise. We will dive into the example of data drift in recommendation using data from a music streaming service. A user\u2019s behaviour is likely to change over time for multiple reasons: users might discover a new genre they like, they may associate negative memories with a song they used to love, or their taste may simply change as they grow up. Given a profile of a user\u2019s tastes over time, it\u2019s relatively straightforward to recommend content that will spark nostalgia. \u00a0However, \u201cnostalgia\u201d comes from Greek words meaning both \u201creturning home\u201d and \u201cpain,\u201d and the songs we loved once may not bring us joy today!\n\u00a0\nWe\u2019ll talk about how to identify changing tastes, find content which will feel like returning home without the pain, and give some simple suggestions for how to incorporate these findings into off-the-shelf recommenders to give a more robust user experience.\n\u00a0\n", "title": "Dealing with pain points of recommenders in the real world", "url": "https://2019.berlinbuzzwords.de/19/session/dealing-pain-points-recommenders-real-world.html", "speaker": "Sophie Watson"}, {"level": "Beginner", "track": "Search", "abstract": "Chances are, if you have shopped online, you\u2019ve searched for the item you want to buy, placed that item in your digital cart, paid for that item, and had it delivered in record time. Each of those steps you took to enjoy your shiny, new item wouldn\u2019t be possible without the help of search engines. Search engines help us find products, help merchants confirm your order, and ship it on time. How do we initially get all this data from slower, traditional databases into fast search engines?\nIn this talk, Conor Landry focuses on how Shopify indexes product, customer, order, and merchant data from MySQL to Elasticsearch in near real-time and how to reindex over 50 terabytes of data in less than 24 hours and the roadblocks we\u2019ve encountered. Conor describes the challenges faced when handling data which is critical to the livelihoods of small business owners and well-known brands as well as strategies used by Shopify when scaling a search indexation system for the long term.\n\u00a0\n", "title": "Reindexing in Record Time: How Shopify Indexes Over 800,000 Merchants' Data in Under 24 Hours", "url": "https://2019.berlinbuzzwords.de/19/session/reindexing-record-time-how-shopify-indexes-over-800000-merchants-data-under-24-hours.html", "speaker": "Conor Landry"}, {"level": "Beginner", "track": "Search", "abstract": "You\u00a0are\u00a0a\u00a0multi-national\u00a0company\u00a0having\u00a0customers\u00a0in\u00a0different\u00a0countries\u00a0where the business\u00a0language\u00a0is\u00a0not\u00a0necessarily\u00a0English.\u00a0How\u00a0would\u00a0you\u00a0build\u00a0a\u00a0centralized\u00a0search\u00a0system\u00a0catering for the needs\u00a0of\u00a0all\u00a0your\u00a0users?\u00a0Apache\u00a0Lucene/Elasticsearch/Apache\u00a0Solr all provide\u00a0different\u00a0language-based\u00a0text\u00a0analyzers\u00a0to\u00a0analyze\u00a0the text, but which one should you use and when? We found overselves in exactly this situation.\u00a0\nWe\u00a0are\u00a0an\u00a0email-security\u00a0company\u00a0having\u00a0around\u00a0300\u00a0billion\u00a0emails\u00a0archived\u00a0with\u00a0us,\u00a0resulting\u00a0in\u00a0indices\u00a0in\u00a0petabytes of indexed data.\u00a0Earlier\u00a0we\u00a0used\u00a0whitespace\u00a0analyzers\u00a0from\u00a0Lucene\u00a0to\u00a0be\u00a0able\u00a0to\u00a0serve\u00a0the\u00a0searches\u00a0in\u00a0different\u00a0languages,\u00a0but\u00a0this\u00a0approach, although\u00a0simplistic,\u00a0presented\u00a0many\u00a0limitations\u00a0once\u00a0we\u00a0started\u00a0to\u00a0serve\u00a0in\u00a0different\u00a0languages\u00a0(e.g.\u00a0German).\u00a0I\u00a0will\u00a0explain\u00a0how\u00a0we\u00a0overcome\u00a0these\u00a0problems\u00a0by\u00a0first\u00a0identifying\u00a0the\u00a0language\u00a0of\u00a0the\u00a0content\u00a0through\u00a0our\u00a0own\u00a0language\u00a0detection\u00a0model\u00a0which\u00a0in\u00a0turn\u00a0served\u00a0as the guide for the selection of an analyzer to analyze the email in various languages.\u00a0\nThis\u00a0talk\u00a0will\u00a0walk\u00a0you\u00a0through\u00a0how\u00a0to\u00a0build\u00a0multilingual\u00a0search\u00a0systems\u00a0and\u00a0explore different possible approaches.\u00a0It\u00a0will\u00a0also\u00a0discuss\u00a0different\u00a0problems\u00a0one\u00a0may\u00a0run\u00a0into\u00a0when\u00a0these\u00a0language-based\u00a0analyzers\u00a0are\u00a0used\u00a0and\u00a0what\u00a0are\u00a0the\u00a0ways\u00a0to\u00a0improve\u00a0the\u00a0search\u00a0results\u00a0in\u00a0these cases.\u00a0\nIn\u00a0particular\u00a0the\u00a0talk will\u00a0focus\u00a0on\u00a0the\u00a0query-log\u00a0analysis\u00a0as\u00a0an\u00a0effective\u00a0way\u00a0to\u00a0improve\u00a0the\u00a0multilingual\u00a0search\u00a0by\u00a0providing\u00a0the\u00a0feedback\u00a0to\u00a0fine\u00a0tune\u00a0the\u00a0analyzers\u00a0used\u00a0for\u00a0stemming and lemmatization,\u00a0thereby\u00a0increasing\u00a0not\u00a0only\u00a0the\u00a0recall but also the precision (relevance) of the search results.\u00a0\n", "title": "Multilingual search system - how to build and improve!?", "url": "https://2019.berlinbuzzwords.de/19/session/multilingual-search-system-how-build-and-improve.html", "speaker": "Paresh Paradkar"}, {"level": "Advanced", "track": "Stream", "abstract": "IoT data poses several challenges to data processing systems. The volume of machine-generated data is huge, users expect timely reactions as soon as real-world events are detected by remote sensors, and connections to edge devices often suffer from varying and often high transfer latencies, resulting in data arriving out-of-order. Apache Flink is an open-source stream processor, that addresses the challenges that IoT data presents. Flink applications run in production at a massive scale at many enterprises and companies, including Alibaba, Netflix, and Uber. In this talk, we will discuss seven reasons why Apache Flink is well-suited for your IoT data project and present how we built a system for real-time RFID asset tracking that is backed by Apache Flink.\n\u00a0\n\u00a0\n", "title": "7 Reasons to use Apache Flink for your IoT Project - How We Built a Real-time Asset Tracking System", "url": "https://2019.berlinbuzzwords.de/19/session/7-reasons-use-apache-flink-your-iot-project-how-we-built-real-time-asset-tracking-system.html", "speaker": "Jakub PiaseckiFabian Hueske"}, {"level": "Beginner", "track": "Stream", "abstract": "Effective real-time analysis and visualization of collected and correlated data to get insights is the high need for businesses. Streaming Expressions introduced in Apache Solr v 6.0 provides powerful stream language for distributed mode, Solrcloud. This session will begin with challenges faced in building near-real-time analytics applications on large datasets. We introduce Streaming Expressions in Apache Solr, discuss the concept and key components it is built upon briefly. The session moves on to discuss various real-life use-cases build on top of Streaming Expressions with statistical functions available in latest versions, along with their performance complexity. The session concludes with how newest additions in Streaming Expressions is a viable solution for Big Data processing.\n", "title": "Building Analytics Applications with Streaming Expressions in Apache Solr", "url": "https://2019.berlinbuzzwords.de/19/session/building-analytics-applications-streaming-expressions-apache-solr.html", "speaker": "Amrit Sarkar"}, {"level": "Intermediate", "track": "Stream", "abstract": "Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.\nEnablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.\nTopics covered in this talk include:\nOverview of use cases and platform architecture\nStreaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping\nStateful streaming computation with scalability, high availability and low latency processing on Apache Flink\nDevelopment frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python\nPython with\u00a0Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink\nKubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications\n", "title": "Streaming your shared ride", "url": "https://2019.berlinbuzzwords.de/19/session/streaming-your-shared-ride.html", "speaker": "Thomas Weise"}, {"level": "Beginner", "track": "Store", "abstract": "Kubernetes is going to run more and more in security critical environments.\nWhat does this mean to run microservices in a \"secure\" way?\u00a0 The good news is: Kubernetes has everything build in to run microservices on the highest security level. The bad news: it is hard to sort out which level is appropriate for your application.\nSeveral examples are discussed like doing DevSecOps with access logs in medical environments, traffic control systems, energy in critical infrastructure, trains, telephony. We report the feedback from security audits.\nThe art of the microservice architecture is to find the appropriate level of security starting with running distributed databases correctly, setting up roles the right way for the level of multitenancy, applying a network policy implementing network layer or using TLS sidecar proxies and Istio in a zero trust infrastructure.\u00a0\n", "title": "Containers, Kubernetes and Practical DevSecOps", "url": "https://2019.berlinbuzzwords.de/19/session/containers-kubernetes-and-practical-devsecops.html", "speaker": "Thomas Fricke"}, {"level": "Beginner", "track": "Search", "abstract": "Open Source projects are a medium to expose engineers to innovation, other cultures and fast-track ways to learn more about the new leading technologies. Representation in the world of Open Source contribution has the opportunity to benefit not only the participants but the broader society. While there are many intersectional aspects to diversity, this talk will focus on diversity of region of contributors and gender as these are easier to analyze and we have more personal experience with them. Together we will explore data from projects in the Apache Software Foundation, Linux Foundation, and Jupyter family of projects.\n\u00a0\nTopics include:\nDiversity of representation of gender, working hours, and region among the different levels of a given project\u2019s leadership (committers, PMC, etc.)\nSource of project origination\nThe existence of codes of conduct\nAnd how creating a welcoming environment helps diversity\nLanguage & time of communication in comments, code, and mailing lists\nThe rate & sources of promotions for project participants\nThe importance of recognizing non-code contributions and how projects with better process are more diverse\n\nTopics of out of scope:\nOur employer\u2019s view on diversity\nAnything implying a causal link: all we can state is correlations\n", "title": "State of diversity in open source communities: A bi  exploration of correlations, draw your own inference", "url": "https://2019.berlinbuzzwords.de/19/session/state-diversity-open-source-communities-bi-exploration-correlations-draw-your-own.html", "speaker": "Holden Karau"}, {"level": "Beginner", "track": "Stream", "abstract": "So, you've migrated your application to Reactive Microservices to get the last ounce of performance from your servers.\u00a0Still want more? Perhaps you forgot about the logs: logs can be one of the few roadblocks on the road to ultimate performance.\nAt Exoscale, we faced the same challenges as everyone: the application produces logs, and they need to be stored in our log storage - Elasticsearch, with the minimum of fuss and the fastest way possible.\nIn this talk, I'll show you some insider tips and tricks taken from our experience put you on the track toward fast(er) log management. Without revealing too much, it involves async loggers, JSON, Kafka and Logstash.\n", "title": "Fast log management for your apps", "url": "https://2019.berlinbuzzwords.de/19/session/fast-log-management-your-apps.html", "speaker": "Nicolas Frankel"}, {"level": "Advanced", "track": "Search", "abstract": "Applying machine learning in online applications requires solving the problem of model serving: Evaluating the machine-learned model over some data point(s) in real time while the user is waiting for a response. Solutions such as TensorFlow Serving are available to solve this problem where the model only needs to be evaluated over a one data point per user request, but what about the case where a model needs to be evaluated over many data points per request, such as in search and recommendation systems?\u00a0\nThis talk will show that this is a bandwidth constrained problem, and outline an architectural solution where computation is pushed down to data shards in parallel. It will demonstrate how this solution can be put into use with Vespa.ai - the open source big data serving engine - to achieve scalable model serving of TensorFlow and ONNX and show benchmarks comparing performance and scalability to TensorFlow Serving.\u00a0\nModel serving with Vespa is used today for some of the worlds largest recommendation systems, such as serving personalized content on all Yahoo content pages, personalized ads in the worlds third largest ad network, and image search and retrieval by similarity in Flickr. These systems evaluate models over millions of data points per request for hundreds of thousands of requests per second.\nSummary:\nOnline evaluation of machine-learned models (model serving) is difficult to scale to large data sets.\nVespa.ai is an open source solution to this problem in use today on some of the largest such systems in the world, such as the content pages on the Yahoo network and the worlds third largest ad network.\nThis talk will explain the problem and architectural solution, show how Vespa can be used to implement the solution to achieve scalable serving of TensorFlow, ONNX and hand-written models, and present benchmarks comparing performance and scalability to TensorFlow Serving.\n", "title": "Scalable machine-learned model serving", "url": "https://2019.berlinbuzzwords.de/19/session/scalable-machine-learned-model-serving.html", "speaker": "Jon Bratseth"}, {"level": "Intermediate", "track": "Search", "abstract": "With all the hype about deep learning and \"AI\", it is not well publicized that for structured/tabular data widely encountered in business applications it is actually another machine learning algorithm, the gradient boosting machine (GBM) that most often achieves the highest accuracy in supervised learning tasks. In this talk we'll review some of the main GBM implementations available as R and Python packages such as xgboost, h2o, lightgbm etc, we'll discuss some of their main features and characteristics, and we'll see how tuning GBMs and creating ensembles of the best models can achieve the best prediction accuracy for many business problems.\n", "title": "Better than Deep Learning: Gradient Boosting Machines (GBM)", "url": "https://2019.berlinbuzzwords.de/19/session/better-deep-learning-gradient-boosting-machines-gbm.html", "speaker": "Szilard Pafka"}]