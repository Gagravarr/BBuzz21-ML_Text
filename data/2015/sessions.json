[{"level": "Intermediate", "track": "Search", "abstract": "The 1.0 release of Apache Drill does SQL on Hadoop, but with some big differences.\u00a0\nThe biggest difference is that Drill changes SQL from a strongly typed language into a late binding language without losing performance.\u00a0 This allows Drill to process complex structured data in addition to relational data.\u00a0 By dynamically generating code that matches the data types and structures observed in the data, Drill can be both agile as well as very fast.\u00a0 Drill can analyze complex data directly with no ETL steps.\nDrill also introduces a view-based security model that uses file-system permissions to control access to data at an extremely fine-grained level that makes secure access easy to control.\nThese changes have huge practical impact when it comes to writing real applications.\nI will give several practical examples of how Drill makes it easier to analyze data.\u00a0 This will include examples of how to use Drill to analyze real complex data.", "title": "What and Why and How: Apache Drill 1.0", "url": "https://2015.berlinbuzzwords.de/session/what-and-why-and-how-apache-drill-10.html", "speaker": "Ted Dunning"}, {"level": "Beginner", "track": "Scale", "abstract": "More and more people prefer the independent life style of freelancing and increased mobility. At the same, IT workers and machine learning and data experts in particular are in great demand, giving them the luxury of choice. In this talk I'll discuss my 10+ years of industry experience, working first as an independent data mining consultant and later leading a team of such work-for-hire data science experts. I'll share some practical tips for those interested in walking the same path, or for companies wanting to contract such teams.", "title": "So you want to be a Data Science consultant (or hire one)? 10 things you should know.", "url": "https://2015.berlinbuzzwords.de/session/so-you-want-be-data-science-consultant-or-hire-one-10-things-you-should-know.html", "speaker": "Radim \u0158eh\u016f\u0159ek"}, {"level": "Intermediate", "track": "Search", "abstract": "In this session we like to share our experiences from analyzing streams of Twitter data with Apache Spark Streaming in near real-time, leveraging Apache Kafka as a HA messaging backbone plus storing and searching for Tweets in Elasticsearch at a large scale. Key design aspects are short end-end processing delays, sub-second search responses and a highly available system that does not rely on hardware redundancy.\n\u00a0", "title": "Analyzing and Searching Streams of Social Media at Scale using Spark, Kafka and Elasticsearch", "url": "https://2015.berlinbuzzwords.de/session/analyzing-and-searching-streams-social-media-scale-using-spark-kafka-and-elasticsearch.html", "speaker": "Markus Lorch"}, {"level": "Intermediate", "track": "Scale", "abstract": "In this workshop I will discuss and teach the more advanced CUDA APIs and concept. CUDA is Nvidia's general purpose computing framework for graphic cards and allows you to scale your computations thanks to its multi-processor paradigm. Recent versions of CUDA has a number of new features and APIs which can potentially save developers time as well as provide highly optimized and\u00a0 fast algorithms if used properly. In this workshop we will teach you how to get the most out of these accelerator cards.\n\u00a0", "title": "New CUDA Concepts and APIs", "url": "https://2015.berlinbuzzwords.de/session/new-cuda-concepts-and-apis.html", "speaker": "Kashif Rasul"}, {"level": "Intermediate", "track": "Scale", "abstract": "Machine Learning is at the heart of the Criteo platform : Sourcing the inventory and generating the banners for 2 billions daily displays require 15 millions predictions per second. In this talk, we will present the infrastructure that allow us to train and deploy 1700 models a day as well as the tooling we set up to continuously monitor and improve this pipeline.", "title": "Prediction @ Criteo: learning at scale on Hadoop", "url": "https://2015.berlinbuzzwords.de/session/prediction-criteo-learning-scale-hadoop.html", "speaker": "Olivier Toromanoff"}, {"level": "Beginner", "track": "Scale", "abstract": "Behavioral retargeting consists of displaying online advertisements that are personalized according to each user\u2019s browsing history. At this point, the selection of the products to display in the banner needs to be fast and accurate. At Criteo, we built a recommender system which is able to choose a dozen of relevant products from over two billion products in a few milliseconds. In this talk, we will expose the problems we faced whilst building this system and how we solved them thanks to a mix of online and offline computations.\n\u00a0", "title": "Recommendation at scale", "url": "https://2015.berlinbuzzwords.de/session/recommendation-scale.html", "speaker": "Simon Doll\u00e9"}, {"level": "Intermediate", "track": "Store", "abstract": "I'd like to do a short talk about Cassandra Reaper\u00a0[1]. Cassandra Reaper is a service making it easy to manage repairs in Cassandra. The motivation to start building the Reaper came from our long-term pain we had with runningrepairs. Nobody in the Cassandra community seemed to be looking into that (despite facing similar pain as we), so we went ahead.\nWe've been building the Reaper as an open-source project from day 1. We already got a few minor contributions, as well as a larger one [2].\nNow we've been running the Reaper for a few months. It's been working really well for us, and rumours have it other people were quite please with it as well. However, the greatest benefit we've reaped (hoho) is that the Reaper allowed people, who are not Cassandra experts, to deal with the repairs themselves.\nThe aim of my talk should be to raise awareness of the project. I'd like to go for a healthy balance of technicalities and background stories.\n\n[1] https://github.com/spotify/cassandra-reaper\n[2] https://github.com/spodkowinski/cassandra-reaper-ui", "title": "Automating Cassandra Repairs", "url": "https://2015.berlinbuzzwords.de/session/automating-cassandra-repairs.html", "speaker": "Radovan Zvoncek"}, {"level": "Intermediate", "track": "Scale", "abstract": "This talk will introduce Mazerunner, an integration of Apache Spark with Neo4j that can be used to offload expensive global graph compute algorithms to a scalable cluster. Mazerunner uses a Docker based orchestration to set up the different components and reads and writes transactionally from a running Neo4j instance to the Spark cluster using an persistent queue. We will discuss the general architecture, walk through the setup and demo two different graph algorithms PageRank and Betweenness Centrality on the DBPedia\ndataset.", "title": "Using Apache Spark for Graph Computation with Neo4j", "url": "https://2015.berlinbuzzwords.de/session/using-apache-spark-graph-computation-neo4j.html", "speaker": "Michael Hunger"}, {"level": "Beginner", "track": "Search", "abstract": "(Ab)Using the ELK stack to analyze information from the Norwegian government owned alcohol monopoly, making it help us answer important questions such as: What beer gives the most alcohol for the money? What terms characterize Belgian beers? (aka Belgianness) What beer contains the maximum Belgianness?", "title": "Kibana 4 - towards a beer analytics engine", "url": "https://2015.berlinbuzzwords.de/session/kibana-4-towards-beer-analytics-engine.html", "speaker": "Christoffer Vig"}, {"level": "Intermediate", "track": "Search", "abstract": "Family is important. \u00a0Work is important. \u00a0Data is important. \u00a0We are constantly inundated with messages telling us what is important, but how do we really determine what is important? \u00a0Focusing on the work and data sides of this challenge, we\u2019ll explore technologies and approaches that can help us better understand what is important as well as some key challenges to be tackled in this realm. \u00a0We\u2019ll also look at how the combination of larger data sets, faster computing and new ways of thinking can help us unlock what really matters, making us more effective as developers, creators and consumers.", "title": "Understanding What\u2019s Important", "url": "https://2015.berlinbuzzwords.de/session/understanding-whats-important.html", "speaker": "Grant Ingersoll"}, {"level": "Beginner", "track": "Search", "abstract": "Starting with heterogeneous data sets like JSON, XML, HTML and binary data, we will illustrate the journey of building a web application that unveils interesting new insights about the world\u2019s biggest single sporting event: the FIFA World Cup 2014.\nWe will focus on the importance of agile data processing and how to interrogate the data to find answers about the tournament and players etc. that previously were not available using other technologies. The demo will show the flexibility in the data layer combined with extensive APIs which are key features for building Big Data solutions.\nWe are looking forward to the discussion with you.", "title": "Teaming up heterogeneous sports data - Building a 360\u00ba view of the FIFA World Cup", "url": "https://2015.berlinbuzzwords.de/session/teaming-heterogeneous-sports-data-building-360o-view-fifa-world-cup.html", "speaker": "Jochen J\u00f6rgNiko Schmuck"}, {"level": "Intermediate", "track": "Scale", "abstract": "Data Science has enabled companies to establish predictive models about their sales, forecast their needs in human resources, enhance theirjavascript:void(0); customer knowledge and so much more. But what is the afterlife of these models ? Are they doomed to perform one-shot predictions and then fade away? After the training and testing steps, the final part of an end-to-end Data Science project should be deploying the constructed model in a production environment in order to reuse easily its results.\nToday this step can be quite time-consuming when it involves rewriting completely the machine learning model in another language or combining specific skills in machine learning and production coding. In this talk, we will present several techniques to automate the deployment of any R model in two complementary production environments : a big Data cluster and a web service.", "title": "Model as code - How to automate the deployment of R models in production environment", "url": "https://2015.berlinbuzzwords.de/session/model-code-how-automate-deployment-r-models-production-environment.html", "speaker": "Isabelle RobinMatthieu Vautrot"}, {"level": "Intermediate", "track": "Scale", "abstract": "Online learning has recently caught a lot of attention, following some competitions, and especially after Criteo released 11GB for the training set of a Kaggle contest.\nOnline learning allows to process massive data as the learner processes data in a sequential way using up a low amount of memory and limited CPU ressources. It is also particularly suited for handling time-evolving date.\nVowpal Wabbit has become quite popular: it is a handy, light and efficient command line tool allowing to do online learning on GB of data, even on a standard laptop with standard memory. After a brief reminder of the online learning principles, we present how to run Vowpal Wabbit on Hadoop in a distributed fashion.", "title": "Online learning, Vowpal Wabbit and Hadoop", "url": "https://2015.berlinbuzzwords.de/session/online-learning-vowpal-wabbit-and-hadoop.html", "speaker": "Heloise Nonne"}, {"level": "Beginner", "track": "", "abstract": "I spent years ignoring ergonomics, operating under an arrogance that the health issues associated with long hours behind a keyboard were something that happened to others, and Not Me.\u00a0 Unsurprisingly, I would learn the hard way that I was wrong.\nYears of surgery, physical therapy, medication, and a healthier set of work habits have taught me the value of an ounce of prevention.\u00a0 In this presentation I relate a cautionary tale of mistakes made, consequences incurred, and the lessons learned along the way.\n\u00a0", "title": "It's All Fun And Games Until...: A Tale of Repetitive Stress Injury", "url": "https://2015.berlinbuzzwords.de/session/its-all-fun-and-games-until-tale-repetitive-stress-injury.html", "speaker": "Eric Evans"}, {"level": "Intermediate", "track": "", "abstract": "The Internet of things is all the rage, as it promises to produce a massive amount of various data and represents real challenges for both technology and business.\nBuilding applications that leverage this coming flood of data will require a different approach, compared to how we build common data applications today. Most if not all of the data acquired from connected and intelligent devices is modeled as Time Series. And extracting value from time stamped data, require some thoughtful design in storage, processing and the type of algorithms necessary to extract insights from the said data.\nIn this talk we intend to describe how with tools we find in today's big data ecosystem we can build powerful intelligent applications from connected devices.", "title": "Analytics in the age of the Internet of Things", "url": "https://2015.berlinbuzzwords.de/session/analytics-age-internet-things.html", "speaker": "Ludwine Probst"}, {"level": "Intermediate", "track": "Search", "abstract": "Hundreds of billions of tweets have been sent since Twitter was founded. Last year, we made all of them searchable using Apache Lucene.\nThis session will present the architecture of this massive search engine. We'll talk about how it was developed, the challenges we faced and how the system scales as more and more tweets are composed.\n", "title": "A complete Tweet index on Apache Lucene", "url": "https://2015.berlinbuzzwords.de/session/complete-tweet-index-apache-lucene.html", "speaker": "Michael Busch"}, {"level": "Beginner", "track": "Search", "abstract": "If you have one or two files, you can take the time to manually work out what they are, what they contain, and how to get the useful bits out (probably....). However, this approach really doesn't scale, mechanical turks or no! Luckily, there are open source projects and libraries out there which can help, and which can scale!\nIn this talk, we'll first look at how we can work out what a given blob of 1s and 0s actually is, be it textual or binary. We'll then see how to extract common metadata from it, along with text, embedded resources, images, and maybe even the kitchen sink! We'll see how to use things like Apache Tika to do this, along with some other libraries to complement it. Once that part's all sorted, we'll look at how to roll this all out for a large-scale Search or Big Data setup, helping you turn those 1s and 0s into useful content at scale!\n", "title": "What's with the 1s and 0s? Making sense of binary data at scale", "url": "https://2015.berlinbuzzwords.de/session/whats-1s-and-0s-making-sense-binary-data-scale-0.html", "speaker": "Nick Burch"}, {"level": "Beginner", "track": "Search", "abstract": "Domains such as financial trading, advertising and marketing have been fertile breeding grounds for cutting edge data-driven applications. Yet the systems that power these services are still running on decades old technology. Traders can run algorithms that make millions of dollars in milliseconds but the underlying systems are still serviced by tools that provide mere searching and graphing. Even the most powerful IT tools only look at small fractions of data, leaving most IT professions to hunt around in the dark. In this talk we will explore the application of machine learning in the domain of IT operations. In the datacenter there are thousands of sources of event data. By modeling the datacenter as a source of multiple data streams, we can apply techniques from other domains, modified to address the environments that now power much of \u00a0our economy. Our focus here is around correlated anomaly detection across multiple time series data sets. By filtering streams of events that deviate from previously established patterns we can surface likely correlations of these streams for further exploration and analysis. Anomalies may be surfaced by anything from simple heuristics such as a reduced count of events, reference points such as specific types of event counts crossing Bollinger bands, \u00a0or by more complex techniques such as probabilistic graphical models. Attendees will learn how techniques used in other domains of machine learning can be applied to help IT operators keep quality of service high. Attendees should be familiar with either machine learning techniques or with IT operations and data center architecture. \u00a0\n", "title": "Predictive Insights for IT Operations", "url": "https://2015.berlinbuzzwords.de/session/predictive-insights-it-operations.html", "speaker": "Omer Trajman"}, {"level": "Advanced", "track": "Search", "abstract": "netarchive.dk maintains a historical archive of Danish net resources. We are indexing its 500TB of raw data into Solr. One of the requirements is to provide faceting on several fields, the largest having billions of unique String values. Stock Solr is not capable of doing that with satisfiable performance on our hardware. Inspection of Solr's core faceting code has led to multiple performance improvements for high cardinality faceting.\nLess memory overhead, using packed counters\nLess garbage collection, reusing counters\nBetter performance for small result sets, using sparse counters\nBetter performance overall with distribution, rewriting fine-counting logic\n\nPerformance gains relative to stock Solr varies with result size. A rule of thumb is 2x for single shard indexes and 4x for multi shard. The principles behind the improvements will be presented and their influence on the faceting performance curve will be discussed and visualized with data from tests and production systems.\nSparse faceting is Open Source and available at http://tokee.github.io/lucene-solr/", "title": "Solr sparse faceting", "url": "https://2015.berlinbuzzwords.de/session/solr-sparse-faceting.html", "speaker": "Toke Eskildsen"}, {"level": "Intermediate", "track": "Store", "abstract": "Data is becoming one of the main decision-makers in an organisation. The more data we have the more challenges we face every day. Every decision we make will have long-term implications. In the talk we will go through different approaches to the data pipelines: from a simple in-house built, with comparison to open source solutions like Apache Kafka and finally hosted auto scaling solutions based Amazon(S3, Kinesis, Lambda) or Google. The talk covers the main aspects of data collecting processes altogether with further implications for data processing, highlighting appropriate solutions and architectures for the main use-cases.\u00a0\n", "title": "Building data pipelines: from simple to more advanced - hands-on experience", "url": "https://2015.berlinbuzzwords.de/session/building-data-pipelines-simple-more-advanced-hands-experience.html", "speaker": "Sergii Khomenko"}, {"level": "Beginner", "track": "Scale", "abstract": "Your first thought may be: Why would I want to talk to non-coders? Buzzwords is a developers\u2019 conference, and most users of open source software also are developers. But there\u2019s a huge advantage to be gained by being able to describe what you do \u2013 the capabilities and the reasonable limitations \u2013 in powerful, non-technical ways that let you communicate effectively with project managers, those with useful domain knowledge, and in the case of some open source projects, the users. Take the new Apache Drill offering, for example. The community of Drill users comprise widely different groups. It includes developers who will appreciate the flexibility and extensibility of Drill as they incorporate it into their own projects plus business analysts with less deep technical developer knowledge but with strong experience and serious goals analyzing big data with BI tools. It helps for those developing Drill to be able to clearly see their needs and talk about how Drill may address them.\u00a0\n\u00a0\nDescribing technical work in non-technical terms does not mean \u201cdumbing it down\u201d. Much to the contrary, it means having sufficiently clear conceptual understanding of your own work that you can cut to the heart of the essential aspects and communicate them to people with a wide range of different background expertise.\u00a0 This approach is particularly useful with machine learning projects in which clear communication with business clients and domain experts about the applicability of available data sources can make or break a project.\n\u00a0\nThere\u2019s another advantage to developing the skill of conceptual communication: it improves your own thinking in terms of seeing the critically important aspects of your work and in leaving you open to innovation.\u00a0 This talk will examine concrete steps to take to learn how to best communicate the strength of your work to other groups and best conceptualize your own roadmap.\n", "title": "Talk the Talk: How to Communicate with the Non-Coder", "url": "https://2015.berlinbuzzwords.de/session/talk-talk-how-communicate-non-coder.html", "speaker": "Ellen Friedman"}, {"level": "Intermediate", "track": "Search", "abstract": "The t-digest is a state-of-the-art algorithm for computing approximate quantiles with adjustable accuracy limits and very few limitations.\nImplementations of t-digest algorithm are easy to use and have been integrated in all kinds of software from ElasticSearch to Apache Mahout. Certain kinds of queries such as finding the top 99.999th %-ile can be accelerated by several orders of magnitude by using t-digest.\nI will describe the basic algorithm and demonstrate the effect of some variations of the algorithm. I will also show how to use the algorithm in your code or your queries.\n", "title": "Practical t-digest Applications", "url": "https://2015.berlinbuzzwords.de/session/practical-t-digest-applications.html", "speaker": "Ted Dunning"}, {"level": "Intermediate", "track": "Search", "abstract": "When you want to make search fast, 80% of the job involves organizing your data so that it can be accessed with as little work as possible. This is the exact reason why Lucene is based on an inverted index.\n\nBut there are some very interesting algorithms and data structures involved in that last 20% of the job. In this talk, you will gain insights into some internals of Lucene and Elasticsearch, and see how priority queues, finite state machines, bit twiddling hacks and several other algorithms and data structures help make them fast.", "title": "Algorithms and data-structures that power Lucene and Elasticsearch", "url": "https://2015.berlinbuzzwords.de/session/algorithms-and-data-structures-power-lucene-and-elasticsearch.html", "speaker": "Adrien Grand"}, {"level": "Beginner", "track": "Search", "abstract": "Modern search engines\u00a0can\u00a0store billions of records containing both text and structured data, but as the amount of data being searched grows, so do the requirements for disk space and memory. \u00a0Various compression techniques are used to decrease the necessary storage, but still allow fast access for search. \u00a0\nWhile Lucene has always used compression for its inverted index, compression techniques have improved and been generalized to other parts of the index, like the built-in document and\u00a0column-oriented data stores.\u00a0In this presentation, Ryan Ernst will give an introduction to how compression is used in Lucene, including recent improvements for Lucene 5.0.\n", "title": "Compression in Lucene", "url": "https://2015.berlinbuzzwords.de/session/compression-lucene.html", "speaker": "Ryan Ernst"}, {"level": "Intermediate", "track": "Search", "abstract": "A wide range of end user and industrial applications rely on accurate\n3D scene representation.\nAutomated 3D modelling from optical sensors such as LIDAR scanners, stereo or RGBD cameras became inevitable since this enables to replace the time consuming manual 3D modelling process.\nHowever, optical sensors deliver extremely high amount of data which is required to be processed to information.\nAn 8 bit stereo system with 9MPx colour images delivers 4GB of data per second.\nBeing able to make sense out of this\nenormous flow of data is a huge and ongoing research and development task.\nThis talk will give an overview of\nthe challenges w Title hich naturally involve efficient search, storage and scalable algorithms for information extraction from 3D data.\nAlso an overview of practical implications on 3D reconstruction of cities,\nautonomous driving in public and industrial facilities, or inspection and monitoring as part of security strategies will be presented.", "title": "Data Challenges with 3D Computer Vision", "url": "https://2015.berlinbuzzwords.de/session/data-challenges-3d-computer-vision.html", "speaker": "Eugen FunkMartin Scholl"}, {"level": "Intermediate", "track": "Scale", "abstract": "How to start a company based on a machine learning idea? And how to scale it into the \"Big Data\" region?\nIn this talk I want to share some insights that I gathered during the last 3 years while founding and successfully scaling a real-time bidding (RTB) company from a two-person startup to a leading technology provider in the field:\nFrom fancy algorithms to production-proof algorithms.\nFrom thousands of model evaluations per day to trillions.\nFrom megabytes to petabytes.\nFrom real-time to batch to real-time.\nFrom two people to entire teams of data scientists and engineers.\n\nI want to present real world examples of pitfalls we were facing, bad technology decisions we made and other things that can and will go wrong. And how to make the best out of it!\nBuzzwords involved: Hadoop, Kafka, Spark, Impala, Redis, Aerospike, \u2026\nSlides:\u00a0https://speakerdeck.com/ctavan/from-machine-learning-startup-to-big-data-company\n", "title": "From Machine Learning Startup to Big Data Company", "url": "https://2015.berlinbuzzwords.de/session/machine-learning-startup-big-data-company.html", "speaker": "Christoph Tavan"}, {"level": "Intermediate", "track": "Scale", "abstract": "Sequence numbers assign a unique increasing number to every document change. They lay the foundations for higher level features such as a changes stream, or bringing a lagging replica up to speed quickly. Implementing them in a distributed system implies dealing with challenges far beyond the capabilities of a simple AtomicLong. They have to be robust enough to deal with problems like faulty servers, networking issues or sudden power outages. On top of that, they need to work in the highly concurrent indexing environment of systems like Elasticsearch.\u00a0\u00a0This talk will take you through the journey of designing such a system.\nWe will start by explaining the requirements. Then we'll evaluate solutions based on existing consensus algorithms, like ZooKeeper's ZAB and Raft, and why they are (in)sufficient for the task. Next we'll consider some alternate approaches, and finally end up with our proposed solution.\nYou don't need to be a consensus expert to enjoy this talk. Hopefully, you will leave with a better appreciation of the complexities of distributed systems and be inspired to learn more.\n", "title": "Designing Concurrent Distributed Sequence Numbers for Elasticsearch", "url": "https://2015.berlinbuzzwords.de/session/designing-concurrent-distributed-sequence-numbers-elasticsearch.html", "speaker": "Boaz Leskes"}, {"level": "Intermediate", "track": "Scale", "abstract": "Today if a byte of data were a gallon of water, in only 10 seconds there would be enough data to fill an average home, in 2020 it will only take 2 seconds. \u00a0The Internet of Things is driving a tremendous amount of this growth, providing more data at a higher rate then we\u2019ve ever seen. With this explosive growth comes the demand from consumers and businesses to leverage and act on what is happening right now. Without stream processing these demands will never be met, and there will be no big data and no Internet of Things. \u00a0Apache Spark, and Spark Streaming in particular can be used to fulfill this stream processing need now and in the future. In this talk I will peel back the covers and we will take a deep dive into the inner workings of Spark Streaming; discussing topics such as DStreams, input and output operations, transformations, and fault tolerance.\u00a0\u00a0After this talk you will be ready to take on the world of stream processing using Apache Spark.\n", "title": "Going deep with Spark Streaming", "url": "https://2015.berlinbuzzwords.de/session/going-deep-spark-streaming.html", "speaker": "Andrew Psaltis"}, {"level": "Beginner", "track": "Search", "abstract": "Bluekiwi.de, Unister's meta search engine, offers an explicit search over news articles. But what about the user who wants to inform herself on what is actually new and important? Watching hundreds of news sources, we collect thousands of news every day - far too many to read or present concisely. So the question becomes how to aggregate, summarize and present the important news of the day.\nIn this talk, we want to present some strategies we used to aggregate news, figure out what is important and not just common, and what techniques can be used to come up with descriptive teasers. These teasers could be used in turn to trigger useful search queries, thus providing the user with the full news stories behind them. Our approach comprises proper entity detection and natural language processing, which we will compare to pure term based techniques. Even if this approach is certainly more laborious than 'out-of-the-box' information retrieval, it is well worth the effort, providing us with understandable and succinct summaries of what's really going on.\n", "title": "\"What's in the news?\u201c - or: why Angela Merkel is not significant", "url": "https://2015.berlinbuzzwords.de/session/whats-news-or-why-angela-merkel-not-significant.html", "speaker": "Andrej Rosenheinrich"}, {"level": "Intermediate", "track": "Store", "abstract": "In this talk we'll focus on the use of Crate alongside Weave in Docker containers, the technical challenges, best practices learned, and getting a simple web application running alongside it. You'll learn about the reasons why Crate.IO is building \"yet another NoSQL database\" and why it's unique and important when running containerized applications. We'll show why the shared-nothing architecture is so important when deploying large clusters and talk about the ways we've leveraged Lucene, Elasticsearch, and built an optimized distributed SQL planner.\n\nYou will learn how to deploy a Crate cluster within minutes in the cloud using Docker, some of the challenges you'll encounter, and how to overcome them. Crate focuses on super simple integrations with any cloud provider, striving to be as turnkey as possible with minimal up-front configuration required to establish a cluster. Once established, we'll show how to scale the cluster horizontally by simply adding more nodes.\n\nThe session will also give you examples when you should use Crate compared to other similar technologies such as MongoDB, Hadoop, Cassandra or FoundationDB. We'll talk about Crate's strengths and what types of applications are well-suited for this type of data store, as well what is not. Finally we'll outline how to architect an application that is easy to scale using Crate, Docker, Weave, and a simple web application.", "title": "Understanding databases for distributed Docker applications.", "url": "https://2015.berlinbuzzwords.de/session/understanding-databases-distributed-docker-applications.html", "speaker": "Chris Ward"}, {"level": "Intermediate", "track": "Search", "abstract": "This talk will present the improvements and new features, but also some incompatible changes in the Lucene 5 release:\nLucene 5 will focus on data safety: The move to Java 7 was completed. Lucene now uses all the brand new features (NIO.2) of Java 7 to make the indexing process more stable and resulting indexes durable. Checksums are used during merging to prevent bugs in the underlying JVM or data corruption due to networking errors (e.g., while distributing indexes during recovery in Elasticsearch) to persist in newly created index segments.\nThe previous major version, Lucene 4 was a major release that introduced index codecs. In Lucene 5, the API around codecs will be cleaned up and will likely get more stable. There are also new features, like a common FilterCache that can be reused by Solr and Elasticsearch.\nIn parallel Apache Solr 5 was released, the first version that will now work as a server out of the box, so Solr is no longer exposed as a webapp. Init.d scripts are included and configuration is managed easier through Zookeeper.\nThis talk will give an overview over the background of these changes and how to make the best out of it.\n", "title": "Apache Lucene 5 - New Features and Improvements for Apache Solr and Elasticsearch", "url": "https://2015.berlinbuzzwords.de/session/apache-lucene-5-new-features-and-improvements-apache-solr-and-elasticsearch.html", "speaker": "Uwe Schindler"}, {"level": "Intermediate", "track": "Scale", "abstract": "Elasticsearch is a\u00a0real-time search and analytics engine that\u00a0promises\u00a0horizontal\u00a0scalability and high\u00a0performance almost\u00a0out of the box. Still, even though it's incredibly easy to get started with Elasticsearch,\u00a0there is ample room for decisions that look good at first but\u00a0might bite you later in production. Luckily, with Elasticsearch\u00a0we are usually able\u00a0to solve all the challenges that arise, but it's just so much nicer if we already know what to look out for right from the start than having to fix it in hindsight.\nThis talk builds on the combined experience that codecentric has\u00a0gathered from\u00a0developing and operating the Elasticsearch-based cloud service CenterDevice as well as various customer projects done\u00a0over the last two years. It formulates important lessons learned regarding Elasticseahttp://berlinbuzzwords.de/session/analytics-age-internet-thingsrch scalability and performance as easy-to-remember Do's and Don'ts, backed up with anecdotes from actual events. The topics covered range from mapping and query definition over data modeling\u00a0to\u00a0cluster configuration and zero downtime\u00a0re-indexing. Nothing is held back - we share\u00a0all the things we wished we had known two years ago.\n", "title": "The Do's and Don'ts of Elasticsearch Scalability and Performance", "url": "https://2015.berlinbuzzwords.de/session/dos-and-donts-elasticsearch-scalability-and-performance.html", "speaker": "Patrick Peschlow"}, {"level": "Intermediate", "track": "Scale", "abstract": "Recommender Systems are a very successful application of large scale data processing. They are used to recommend new items of interest to users of a service, such as new movies on Netflix, or shopping articles on Amazon. Recommender systems have become an essential part of most web-based services to enhance the user experience.\u00a0A powerful approach for implementing recommenders are the so called \"latent factor models\", a special case of the collaborative filtering techniques, which exploit the similarity between user tastes and item characteristics, which are automatically extracted.:\nThis talk details our experience with implementing three variants of the ALS (Alternating Least Squares) algorithm to train a latent factor model using the Apache Flink system and scaling them to large clusters and data sets. In order to scale these algorithms to extremely large data sets, Flink\u2019s functionality was significantly enhanced to be able to distribute and process very large records efficiently. A preliminary presentation of the results, scaling ALS to 28 billion user ratings can be found here: http://data-artisans.com/computing-recommendations-with-flink.html\n", "title": "Computing recommendations at extreme scale with Apache Flink", "url": "https://2015.berlinbuzzwords.de/session/computing-recommendations-extreme-scale-apache-flink.html", "speaker": "Till Rohrmann"}, {"level": "Beginner", "track": "Scale", "abstract": "The essence of near-real-time stream processing is to compute huge volumes of data as it is received. This talk will focus on creating a pipeline for collecting huge volumes of data anfd processing near-real time using Storm.\u00a0\nStorm is a high-volume, continuous, reliable stream processing system developed at BackType and open-sourced by Twitter. Storm is being widely used in lot of organizations and has variety of uses-cases like: \u00a0 \u00a0\n* Realtime analytics \u00a0\u00a0 \u00a0\n* Distributed RPC \u00a0\u00a0 \u00a0\n* ETL etc. \u00a0\u00a0\n\u00a0\n\u00a0 During the course of 40\u00a0minutes using an example of Real-time Wikipedia edit we will try and understand: \u00a0\u00a0\n\u00a0* Basic concepts of stream-processing. \u00a0\n\u00a0 \u00a0* High level understanding of components involved in\u00a0Storm. \u00a0\u00a0 \u00a0\n* Writing producer in Python which will will push in Queue\u00a0the real-time edit feed from Wikipedia. \u00a0 \u00a0\u00a0\n\u00a0* Write storm topologies in python to consume feed and process real-time metrics like: \u00a0 \u00a0 \u00a0\u00a0 \u00a0\u00a0\u00a0\n\u00a0 \u00a0 \u00a0 * Number of articles edited. \u00a0\u00a0 \u00a0\u00a0\u00a0\n\u00a0 \u00a0 \u00a0 * Category wise count of articles being edited. \u00a0\u00a0 \u00a0\u00a0\n\u00a0 \u00a0 \u00a0 * Distinct people editing the articles \u00a0\u00a0 \u00a0\u00a0\u00a0\n\u00a0 \u00a0 \u00a0 * GeoLocation counters etc. \u00a0\u00a0\n\u00a0* Technological challenges revolving around near-real time stream processing systems: \u00a0 \u00a0\n\u00a0* Achieve low latency for processing as compared to batch processing. \u00a0 \u00a0 \u00a0\n\u00a0* State-management in workers to maintain aggregated counts like counting edits for same category of articles. \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0\n\u00a0* Handling failures and crashes\n\u00a0* Deployment Startergies. \u00a0 \u00a0\n\u00a0\n", "title": "Designing NRT(NearRealTime) stream processing systems: Using Storm", "url": "https://2015.berlinbuzzwords.de/session/designing-nrtnearrealtime-stream-processing-systems-using-storm.html", "speaker": "Konark Modi"}, {"level": "Intermediate", "track": "Scale", "abstract": "Live demo of building an in-memory data pipeline and data warehouse from a web console with architectural guidelines and lessons learned. The tools and APIs behind are built on top of Spark, Tachyon, Mesos/YARN, SparkSQL and are using our own open-sourced Spark Job Server and Http Spark SQL Rest Service.\nLast year Spark emerged as a strong technology candidate for distributed computing at scale, as a Hadoop MapReduce and Hive successor. Tachyon is a very promising young project that provides an in-memory distributed file system that serves a caching layer for sharing datasets across multiple Spark/Hadoop applications. Mesos and YARN are resource managers that ensure a more efficient utilization of the\u00a0resources in a distributed cluster. Parquet is a columnar storage format that enables storing data and schema in the same file, without the need for an external\u00a0metastore.\nIn this talk we will showcase the strengths of all these open source technologies and will share the lessons learned while using them for building an in-memory data pipeline (data ingestion and transformation) and a data warehouse for interactive querying of the in-memory datasets (Spark JVM and Tachyon) using a\u00a0familiar SQL interface.\u00a0\nWe will provide architectural guidelines\u00a0and design patterns meant to help us achieve optimal CPU/Memory for the utmost performance during large scale processing and interactive querying.\u00a0We will touch on RDDs, shuffle, file consolidation, RDD persistence models (memory, disk, off-heap) serialization, Tachyon (native and hdfs apis) and will provide tips and tricks for maximizing performance and working around the\u00a0weaknesses of these technologies.\nWe will also provide a quick description of the Spark Job Rest (https://github.com/Atigeo/spark-job-rest) and Http Spark SQL REST (https://github.com/Atigeo/jaws-spark-sql-rest) projects that we decided to share back with the open source community.\n", "title": "In-memory data pipeline and warehouse at scale using Spark, Spark SQL, Tachyon and Parquet", "url": "https://2015.berlinbuzzwords.de/session/memory-data-pipeline-and-warehouse-scale-using-spark-spark-sql-tachyon-and-parquet.html", "speaker": "Ema IancutaRadu Chilom"}, {"level": "Intermediate", "track": "Search", "abstract": "In Comperio we work on projects that aim to learn from the documents and social activity published on the web. The challenge is to\u00a0summarize recent activity,\u00a0drawing\u00a0together current events with past activity and finding new sources to learn from. We base our on Elasticsearch and its significant terms technology. In this presentation we show how we expand on the base functionality provided in Elasticsearch to focus on areas such as immediate trends, entity identification and topic building using additional techniques from Information Retrieval (IR)\u00a0and Natural Language Processing (NLP).\n", "title": "Beyond significant terms", "url": "https://2015.berlinbuzzwords.de/session/beyond-significant-terms.html", "speaker": "Andr\u00e9 Lynum"}, {"level": "Intermediate", "track": "Search", "abstract": "Apache Kylin is an open source distributed analytics engine that originated at ebay, Inc. and provides a SQL-interface and multi-dimensional analysis for online analytical processing. Kylin was recently accepted by the Apache Software Foundation as an incubator project and already has a number of integrations with HDFS, MapReduce, Hive, HBase and Apache Drill.\nThis session will provide an overview about Apache Kylin, how it works, what it does and will give an introduction\u00a0to Online Analytical Processing (OLAP), how Business Intelligence works and how Kylin + Hadoop can help in analyzing extremely large datasets.\u00a0\n", "title": "An introduction to Apache Kylin - Business Intelligence meets Big Data", "url": "https://2015.berlinbuzzwords.de/session/introduction-apache-kylin-business-intelligence-meets-big-data.html", "speaker": "Fabian Wilckens"}, {"level": "Intermediate", "track": "Scale", "abstract": "Instrumentation has seen explosive adoption on the cloud in recent years. With the rise of micro-services we are now in an era where we measure the most trivial events in our systems. At Trademob, a mobile DSP with upwards of 125k requests per second across +700 instances we generate and collect millions of time-series data points. Gaining key insights from this data has proven to be a huge challenge.\nOutlier and Anomaly detection are two techniques that help us comprehend the behavior of our systems and allow us to take actionable decisions with little or no human intervention. Outlier Detection is the identification of misbehavior across multiple subsystems and/or aggregation layers on a machine level, whereas Anomaly Detection lets us identify issues by detecting deviations against normal behavior on a temporal level.\nAt Trademob, we developed a real-time monitoring system to conquer those challenges in order to reduce false positive alerts and increase overall business performance. By correlating a multitude of metrics we can determine system interdependencies, preemptively detect issues and also gain key insights to causality. This session will provide insights into both the system\u2019s architecture and the algorithms used to detect unwanted behaviors.\n", "title": "Real-Time Monitoring of Distributed Systems", "url": "https://2015.berlinbuzzwords.de/session/real-time-monitoring-distributed-systems.html", "speaker": "Tobias KuhnNakul Selvaraj"}, {"level": "Intermediate", "track": "Search", "abstract": "A brand new take on our Solr&Elasticsearch talk from last year! As the title suggests, this time we\u2019ll dive deeper into how these two search engines scale and perform. And of course, we\u2019ll take into account all the goodies that came with Elasticsearch and Solr since.\nBoth search engines are based on Lucene, so you\u2019d expect similar numbers, but:\nat scale, small differences can give different numbers\nas with most functionality, Elasticsearch and Solr take different paths to achieve similar results, so you\u2019d tune them differently\ntheir distributed models are quite different\n\nWe\u2019ll show how you\u2019d tune Elasticsearch and Solr for 2 common use-cases - logging and product search - and what numbers we got after tuning. Also, we\u2019ll share some best practices for scaling out massive Elasticsearch and Solr clusters. For example, how to divide data into shards and indices/collections that account for growth, when to use routing and how to make sure that coordinating nodes don\u2019t become unresponsive.\nBy the end you\u2019ll see how Elasticsearch and Solr compare when you dive deeper into their functionality. You\u2019ll know which important Lucene knobs to turn and how to do that in each search engine. Also, you\u2019ll know how to use specific scaling features such as automatic rebalancing for Elasticsearch and shard splitting for Solr.\n", "title": "Side by Side with Elasticsearch & Solr part 2: Performance & Scalability", "url": "https://2015.berlinbuzzwords.de/session/side-side-elasticsearch-solr-part-2-performance-scalability.html", "speaker": "Radu GheorgheRafa\u0142 Ku\u0107"}, {"level": "Intermediate", "track": "Search", "abstract": "Lucene works great with independent text documents, but real life problems often require to handle relations between documents. Aside of several workarounds, like term encodings, field collapsing or term positions, we have two mainstream approaches to handle document relations: join and block-join. Both have their downsides. Join lacks performance, while block-join makes is really expensive to handle index updates, since it requires to wipe a whole block of related documents.\nThis session presents an attempt to apply join index, borrowed from RDBMS world, for addressing drawbacks of the both join approaches currently present in Lucene. We will look into the idea per se, possible implementation approaches, and review the benchmarking results. \u00a0\u00a0\u00a0\u00a0\u00a0\nJoin us! Get to know about forthcoming cool feature!\nDuring the session attendees will learn\nHow modern join algorithms work, their strengths and weaknesses.\nHow RDBMS\u2019s join indices can be applied for Lucene\nWhat use cases can benefit from join indices, supported by benchmarks.\n", "title": "Approaching Join Index for Lucene", "url": "https://2015.berlinbuzzwords.de/session/approaching-join-index-lucene.html", "speaker": "Mikhail Khludnev"}, {"level": "Intermediate", "track": "Search", "abstract": "Ever wonder how Watson beat all comers in Jeopardy or how Siri or Google Now work? \u00a0Thinking about deploying Question Answering (QA) technology in your application? \u00a0QA and NLP technology have finally hit the mainstream and are making information access easier and more personalized every day. \u00a0The best part? \u00a0Open source technologies make it easier than ever to build and deploy question answering technology. \u00a0In this talk, we'll lay the foundation of building a next generation QA system using open source like Apache Solr and various open source NLP libraries as well as demonstrate a working system able to answer real natural language questions.\u00a0\n", "title": "You've got questions.  We've got answers!", "url": "https://2015.berlinbuzzwords.de/session/youve-got-questions-weve-got-answers.html", "speaker": "Grant Ingersoll"}, {"level": "Intermediate", "track": "Store", "abstract": "The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance.\nThat may very well be true, but it took us a year to get there and that road was dotted with site\u2019s lowered availability and compromised performance.\nIn this talk I am going to share the insights we gained when we migrated parts of the Yammer\u2019s messaging pipeline from a custom storage solution backed by Java BDB to Cassandra. Covering topics like:\nWhy we decided to move to Cassandra from our proprietary Berkley DB backed database\u00a0\nModeling data and capacity supported by metrics\nZero downtime production rollout\nHow things started falling apart after three months of seamless operation\u00a0\nHow to diagnose and fix things later on\n\n\u00a0\nAbout Yammer:\nYammer is a private social network that helps employees collaborate across departments, locations and business apps. We currently have 10 million users and are planning to grow at least 10 times. To achieve that, our systems needs to be cheap and easy to scale and resilient to failure.\n", "title": "Cassandra at Yammer", "url": "https://2015.berlinbuzzwords.de/session/cassandra-yammer.html", "speaker": "Michal Rutkowski"}, {"level": "Intermediate", "track": "Scale", "abstract": "Brandwatch is a world-leading social media monitoring tool. We find, process and store nearly 70M mentions from the Web every day from our crawlers and firehoses. As the amount of data we find continually increases, the harder it becomes to separate the signals from the noise for our customers.\nOver the last year, we have been building, experimenting and scaling a distributed cluster of JVMs that process the data from our client\u2019s queries and detect influential mentions of their brands online and alert on unusual and important trends. We used Apache Kafka, ZooKeeper, and Spring to achieve this.\nThis talk explains the architecture that we built, how it performs, scales and some of the difficulties we\u2019ve faced along the way. We\u2019ve learned a lot in the process. We hope you'll learn from us too!\n", "title": "Detecting Events on the Web with Java, Kafka and ZooKeeper", "url": "https://2015.berlinbuzzwords.de/session/detecting-events-web-java-kafka-and-zookeeper.html", "speaker": "James Stanier"}, {"level": "Intermediate", "track": "Scale", "abstract": "Relevance and Personalization is crucial to building personalized local commerce experience at Groupon. We have built infrastructure that processes real time user interaction stream and produces personalized real time analytics that are further enhanced to present relevant personalized experience to hundreds of millions of users of Groupon across the world. This talk covers the use case and use of our Kafka-Storm-HBase-Redis pipeline to ingest over 3 million data points per second in real time which in turn brings in millions of dollars in additional revenue. Specially we will discuss how we scaled this system for hundreds of millions of users including solution choices, different techniques and strategies, traditional and innovative approaches. Solution includes some interesting algorithmic choices to reduce data size such as bloom filters and HyperLogLog, as well as use of big data technologies such as HBase, Kafka & Storm. Attendees can take away learnings from our real-life experience that can help them understand various tuning methods, their tradeoffs and apply them in their solutions \u00a0\n", "title": "Real Time Big Data Analytics with Kafka, Storm & HBase", "url": "https://2015.berlinbuzzwords.de/session/real-time-big-data-analytics-kafka-storm-hbase.html", "speaker": "Ameya Kanitkar"}, {"level": "Intermediate", "track": "Search", "abstract": "Etsy loves metrics. Everything that happens in our data centres gets recorded, graphed and stored. But with over a million metrics flowing in constantly, it\u2019s hard for any team to keep on top of all that information. Graphing everything doesn\u2019t scale, and traditional alerting methods based on thresholds become very prone to false positives.\nThat\u2019s why we started Kale, an open-source software suite for pattern mining and anomaly detection in operational data streams. These are big topics with decades of research, but many of the methods in the literature are ineffective on terabytes of noisy data with unusual statistical characteristics, and techniques that require extensive manual analysis are unsuitable when your ops teams have service levels to maintain.\nIn this talk I\u2019ll briefly cover the main challenges that traditional statistical methods face in this environment, and introduce some pragmatic alternatives that scale well and are easy to implement (and automate) on Elasticsearch and similar platforms. I\u2019ll talk about the stumbling blocks we encountered with the first release of Kale, and the resulting architectural changes coming in version 2.0. And I\u2019ll go into a little technical detail on the algorithms we use for fingerprinting and searching metrics, and detecting different kinds of unusual activity. These techniques have potential applications in clustering, outlier detection, similarity search and supervised learning, and they are not limited to the data centre but can be applied to any high-volume timeseries data.\nKale version 1 is described here:\u00a0https://codeascraft.com/2013/06/11/introducing-kale/\nVersion 2\u00a0has the same goals but a very different architecture and suite of tools. Come along if you'd like to learn more.\n", "title": "Signatures, patterns and trends: Timeseries data mining at Etsy", "url": "https://2015.berlinbuzzwords.de/session/signatures-patterns-and-trends-timeseries-data-mining-etsy.html", "speaker": "Andrew Clegg"}, {"level": "Intermediate", "track": "Scale", "abstract": "Apache Flink is a distributed engine for batch and streaming data analysis. Flink offers familiar programming APIs based on parallel collections that represent data streams and transformations and window definitions on these collections.\nFlink supports these APIs with a robust execution backend. Both batch and streaming APIs are backed by the same execution engine that has true streaming capabilities, resulting in true real-time stream processing and latency reduction in many batch programs. Flink implements its own memory manager and custom data processing algorithms inside the JVM, which makes the system behave very robustly both in-memory and under memory pressure. Flink has iterative processing built-in, implementing native iteration operators that create dataflows with feedback. Finally, Flink contains its own cost-based optimizer, type extraction, and data serialization stack.\nThe end result is a platform that is fast, easy to program against, unifies batch and stream processing without compromising on latency or throughput, requires very little tuning to sustain data-intensive workloads, and solves many of the problems of heavy data processing inside the JVM.\nThis talk gives an overview of Flink\u2019s APIs, the most important features of Flink\u2019s runtime and the benefits for users, as well as a roadmap of the project.\n", "title": "Apache Flink deep-dive", "url": "https://2015.berlinbuzzwords.de/session/apache-flink-deep-dive.html", "speaker": "Stephan Ewen"}, {"level": "Intermediate", "track": "Store", "abstract": "Time series data is everywhere: IoT, sensor data, financial transactions. The industry has moved to databases like Cassandra to handle the high velocity and high volume of data that is now common place. However data is pointless without being able to process it in near real time. That's where Spark combined with Cassandra comes in, what was one just your storage system can be transformed into your analytics system, and you'll be surprised how easy it is!\nSo, join me for a whirl wind tour of how to use these two awesome open source projects for time series\u00a0data.\u00a0We'll\u00a0cover:\nAn overview of Cassandra - Why is it so good for time series?\nAn introduction to Spark\u00a0\nWhat can\u2019t be done in Cassandra and how Spark can fill in the gaps\nHow to build analytics on top of your operational data without the typical Extract-Transform-Load\nSpecific use cases: sensor data, customer event\u00a0data, financial transactions\n\nMy goal is that by the end of this talk you\u2019ll know whether Cassandra/Spark\u00a0are right for your next project.\n", "title": "Real time analytics with Apache Cassandra and Apache Spark", "url": "https://2015.berlinbuzzwords.de/session/real-time-analytics-apache-cassandra-and-apache-spark.html", "speaker": "Christopher Batey"}, {"level": "Advanced", "track": "Search", "abstract": "Sorted lists of integers are commonly used in Lucene's implementation of inverted index. Those lists are often compressed in-memory as a trade-off between memory footprint and access speed and CPU utilization. Thus, encoding and, more important, decoding of these lists consumes significant CPU time. We can use a SIMD instructions available in modern commodity processors to boost integer decompression performance. Key obstacle here is that Java/HotSpot doesn't provide access to SIMD instructions. Calling a JNI method from Java is rather expensive comparing to a simple C function call. Fortunately, for cases when native method just perform some encoding/decoding on a byte array, HotSpot has a private interface (it will likely to become a more standard extension) which adds a minimum overhead to the native routine.\nIn this talk we will show our prototype of Lucene Codec which uses a simple C library for compressing lists of integers using binary packing and SIMD instructions(https://github.com/lemire/simdcomp), which significantly improves decoding throughput.\n", "title": "Fast Decompression Lucene Codec", "url": "https://2015.berlinbuzzwords.de/session/fast-decompression-lucene-codec.html", "speaker": "Ivan MamontovMikhail Khludnev"}, {"level": "Intermediate", "track": "Search", "abstract": "Elasticsearch users will be familiar with Zen discovery. The\u00a0Elasticsearch 'discovery' module\u00a0actually encompasses a number of things - node discovery, master election, cluster state publishing, failure detection and handling. I will discuss implications of this model\u00a0for Elasticsearch as a distributed system, drawing upon my experience building eskka, an Akka-cluster based discovery plugin.\n", "title": "Diving into Elasticsearch Discovery", "url": "https://2015.berlinbuzzwords.de/session/diving-elasticsearch-discovery.html", "speaker": "Shikhar Bhushan"}, {"level": "Intermediate", "track": "Scale", "abstract": "The talk proposes and demonstrates a methodology for monitoring \u00a0and troubleshooting the performance of a typical web application by using Elasticsearch as an event analytics engine.\u00a0It discusses ways of collecting performance metrics for the transaction data (response time, error codes, URL path, data base tables, etc.), the appropriate level of details captured, sampling techniques and tips for efficiently storing the data.\nIt then shows how to use Elasticsearch aggregations to provide the application performance specific metrics (response time percentiles, error rates) at a global level and also segmented over multiple dimensions (service, server, URL path, etc.). It shows how to use these techniques and visualisations to perform a top-down analysis through the data in order to identify a performance issue.\nDuring the talk, the audience learns how Packetbeat, Yahoo Boomerang, Logstash, Elasticsearch, Kibana and Bonito work together for a complete application performance management\u00a0solution that is more applicable and more flexible than the commercial offerings.\n", "title": "Application performance management with open source tools", "url": "https://2015.berlinbuzzwords.de/session/application-performance-management-open-source-tools.html", "speaker": "Tudor GolubencoMonica Sarbu"}, {"level": "Intermediate", "track": "Search", "abstract": "It is hard to understand what is hidden in big high dimensional data. However, a moderate number of simple one dimensional projections is enough to answer hard questions about the data via techniques such as visualization, classification and clustering. Random projections have emerged as an extremely effective component of many algorithms for high dimensional data. For example, they are used in the context of nearest neighbor search (via locality sensitive hashing), dimensionality reduction and clustering. The goal of the talk is to give a pleasant journey into the rich area of random projections via many graphical illustrations and intuitive examples. We\u00a0present how and why random projections work and where they break. We\u00a0discuss several interesting properties of high dimensional data. For example, why data in high dimensions is likely to look Gaussian when projected in low dimensions; how to spot interesting patterns in high dimensional data by projecting into a lower dimension; and how to choose meaningful low dimensional projections. The method of random projections has a number of good properties: 1) scalability; 2)\u00a0\u00a0it\u00a0reduces the machine learning problem to search and can take advantage of existing infrastructure; 3)\u00a0it is relatively simple to implement and 4) it is robust to noisy data.\u00a0\nLink to the talk website:\u00a0http://stefansavev.com/randomtrees/\n", "title": "Using Random Projections to Make Sense of High-Dimensional Big Data", "url": "https://2015.berlinbuzzwords.de/session/using-random-projections-make-sense-high-dimensional-big-data.html", "speaker": "Stefan SavevMichael Kleen"}, {"level": "Intermediate", "track": "Scale", "abstract": "A simple application may start out with one database, but as you scale and add features, it usually turns into a tangled mess of datastores, replicas, caches, search indexes, analytics systems and message queues. When new data is written, how do you make sure it ends up in all the right places? If something goes wrong, how do you recover?\nChange Data Capture (CDC) is an old idea: let the application subscribe to a stream of everything that is written to a database \u2014 a feed of data changes. You can use that feed to update search indexes, invalidate caches, create snapshots, generate recommendations, copy data into another database, and so on. For example, LinkedIn's Databus and Facebook's Wormhole do this. But the idea is not as widely known as it should be.\nIn this talk, I will explain why change data capture is so useful, and how it prevents race conditions and other ugly problems. Then I'll go into the practical details of implementing CDC with PostgreSQL and Apache Kafka, and discuss the approaches you can use to do the same with various other databases.\nA new era of sanity in data systems awaits!\n", "title": "Change Data Capture: The Magic Wand We Forgot", "url": "https://2015.berlinbuzzwords.de/session/change-data-capture-magic-wand-we-forgot.html", "speaker": "Martin Kleppmann"}, {"level": "Intermediate", "track": "Scale", "abstract": "NSQ is an open source realtime distributed messaging platform designed to operate at scale and handle billions of messages per day. It is built entirely in Go and promotes distributed and decentralized topologies without single points of failure. This enables fault tolerance and high availability coupled with a reliable message delivery guarantee.\nNSQ was built by the engineering team at Bitly to support its high volume even stream processing architecture. It powers our\u00a0near real-time analytics systems and is now used by a range of companies including Docker, Stripe and BuzzFeed.\nDuring this talk we will answer questions such as why was NSQ built and how it fits into the larger picture. We will also take a look at some code examples.\u00a0By using real-life scenarios that we faced at Bitly, we\u00a0will discusss how NSQ deals with concepts of availability, performance, latency, fault tolerance and how it's enabled us to scale effectively. We will also discuss why Go was an excellent language to use for building NSQ.\u00a0\n", "title": "Scale with NSQ: a realtime distributed messaging platform", "url": "https://2015.berlinbuzzwords.de/session/scale-nsq-realtime-distributed-messaging-platform.html", "speaker": "Georgi Knox"}, {"level": "Intermediate", "track": "Scale", "abstract": "Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop. \u00a0Until recently, MapReduce was the only Hadoop\u00a0execution engine for\u00a0Hive\u00a0queries. But today, alternative execution engines are available \u2014 such as\u00a0Apache Spark\u00a0and\u00a0Apache Tez. \u00a0The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.eins zwo\nIn this\u00a0talk\u00a0we'll discuss the Hive on Spark project. \u00a0Topics include\u00a0the motivations, such as improving Hive user experience and streamlining\u00a0operational management for Spark shops, some background and comparisons of MapRededuce and\u00a0Spark, and\u00a0the technical process\u00a0of\u00a0porting a complex real-world application from MapReduce\u00a0to Spark. \u00a0A demo will also be presented.\n", "title": "Hive on Spark", "url": "https://2015.berlinbuzzwords.de/session/hive-spark.html", "speaker": "Szehon Ho"}, {"level": "Beginner", "track": "Search", "abstract": "In this talk I will introduce Storm-Crawler\u00a0https://github.com/DigitalPebble/storm-crawler,\u00a0a collection of resources for building low-latency, large scale web crawlers on Apache Storm. We will compare with similar projects like Apache Nutch and present several use cases where the storm-crawler is being used. \u00a0In particular we will see how the Storm-crawler can be used with ElasticSearch and Kibana for crawling and indexing web pages.\n", "title": "Low latency scalable web crawling on Apache Storm", "url": "https://2015.berlinbuzzwords.de/session/low-latency-scalable-web-crawling-apache-storm.html", "speaker": "Julien Nioche"}]