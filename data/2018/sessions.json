[{"level": "Intermediate", "track": "Search", "abstract": "Data scientists now have access to vast amounts of data; therefore, it is important to ensure the data they analyze is meaningful and relevant. Search engines are used to find relevance in vast volumes of data; however, until recently, they could only do basic data analysis. We will investigate new features recently added to Apache Solr that make search an attractive option for data scientists. The talk will focus on the Solr Analytics Component and other complementary features offered by Solr.\nThis talk is presented by Bloomberg.", "title": "Relevant Data Analysis: Apache Solr Analytics", "url": "https://2018.berlinbuzzwords.de/18/session/relevant-data-analysis-apache-solr-analytics.html", "speaker": "Houston Putman"}, {"level": "Intermediate", "track": "Search", "abstract": "Search engines have come a long way since the early days of reliance on information retrieval as user intent drives modern search applications.\u00a0 Businesses driven by search like eCommerce, content-based companies lose a small but decent amount of traffic to queries getting a lower number of impressions/clicks/conversions as few or irrelevant results are computed and shown. This session focus on identifying poor/low performing queries and terms using Head and Tail Analysis, classify with potential reasons and suggest corrections or improved query models for them. The session concludes with emphasizing on experiments being vital to the entire process of improving search relevance. The session is based on Head-n-Tail Analysis in Fusion 4 by Chao Han, VP Head of Research, Lucidworks.\nThis talk is presented by Lucidworks.", "title": "Head-N-Tail Analysis to Increase Engagement", "url": "https://2018.berlinbuzzwords.de/18/session/head-n-tail-analysis-increase-engagement.html", "speaker": "Amrit Sarkar"}, {"level": "Beginner", "track": "Scale", "abstract": "At eBay Kleinanzeigen, we aim to inspire our users with a feed of the best items tailored to them. This becomes an interesting problem with more than 20 million monthly users and over 28 million live ads, with thousands of interactions taking place on our platform every second.\nSome of the challenges that pop up are how to deal with new visitors, or ones that only visit occasionally. The posted items are often also very short-lived, as many get sold quickly. This requires us to be responsive (near real-time) with respect to our inventory and the users\u2019 behaviour to help them find a match and be successful.\nTechnologies such as Kafka Streams and Elasticsearch allow us to approach the problem in a modern, elegant and scalable way, without the need for specialised clusters and long-running overnight batch jobs.\nThis talk is presented by eBay Tech.", "title": "Building a personalised home feed using Kafka Streams and Elasticsearch", "url": "https://2018.berlinbuzzwords.de/18/session/building-personalised-home-feed-using-kafka-streams-and-elasticsearch.html", "speaker": "Christiane LemkeDomenic Vossen"}, {"level": "Intermediate", "track": "Scale", "abstract": "The vast majority of the recommender systems nowadays are based on algorithms which are good at representing linear relations between users and items, or items and items. With the growing popularity of deep learning applications, it was quite natural for us to experiment with a more advanced recommendation engine which could represent more complex, non-linear relations. With this we were able to generate much more relevant recommendations to the users.\nThe new recommendation engine was developed by a student as part of his master thesis, lead by a PHD colleague from the mobile.de data team. The engine is based on deep learning and combines 3 sub neural networks. The engine showed really good results compared to our current recommendation engine, what brought us to the decision of trying to deploy it into our production system. Once we started to review the new engine, the following main challenges were raised:\n\u00a0- We are using Java/Scala in our production system: Is there a possibility to deploy a \u00a0Tensorflow model, which was trained in python, in Java/Scala ? \n- How would we be able to deploy new models into the production system, without any\u00a0\u00a0downtimes ?\n- The model contains 3 sub neural networks, each sub network is responsible for different \u00a0functionality: Would it be possible to isolate each of those sub networks and deploy it \u00a0separately in order to scale out the entire engine ? \n- How could we generate recommendations in real-time without pre-calculating the recommendations for each user/item over night ?\n- Does such complex system can scale at all?\nThe main focus of this talk would be on our journey towards the design and implementation of a scalable architecture, giving all the mentioned above requirements, which could support the deployment of the new Deep Learning Recommender in production. During the talk I would try to present the different architecture components and how each component helps in solving the mentioned above challenges. In addition, I would try to describe how the different components of the Deep Learning Recommender could be reused and help us to improve also our search functionalities.\n\u00a0\nKeywords: Tensorflow, Tensorflow Serving, Scala, Docker, Kafka,\u00a0nearest neighbor search, Elasticsearch, Deep Learninig.\n", "title": "Deep Learning Recommender Architecture \u2013 From a Master Thesis to Production", "url": "https://2018.berlinbuzzwords.de/18/session/deep-learning-recommender-architecture-master-thesis-production.html", "speaker": "Igor Mazor"}, {"level": "Intermediate", "track": "Store", "abstract": "Artificial Intelligence is transforming healthcare. One of our goal at Babylon is to make healthcare more affordable by providing, amongst other AI powered tools, a medical chatbot that produces an accurate diagnosis of symptoms reported through an app.\u00a0\nIn this talk we\u2019ll explore the challenges in building and maintaining a medical knowledge base. We will share some valuable lessons learnt in using streams and graph databases to store and process medical data and show how such tools are integrated in our inference tasks and machine learning applications. \u00a0\n", "title": "Streams and graphs: behind the scenes of building a doctor in your pocket.", "url": "https://2018.berlinbuzzwords.de/18/session/streams-and-graphs-behind-scenes-building-doctor-your-pocket.html", "speaker": "Domenico Corapi"}, {"level": "Intermediate", "track": "Stream", "abstract": "Kafka security has come a long way since its early days when none was available whatsoever.\u00a0In this talk I will give a brief overview of how security evolved in Kafka and explain what currently works, as well as giving a brief outlook into what is currently being developed by the community.\nWe will discuss authentication via SSL, Kerberos und Delegation tokens and touch the Kafka versions that introduced these features and related major changes. Following that I will explain how to use ACLs in Kafka and how they are implemented internally, which will then serve as the basis for diving down into development of custom authorizers and principal builders to extend the basic Kafka security - for this we will use the example of authorizing based on the groups a user is assigned in an Active Directory structure.\nThe talk will be fairly technical, we will look at class structures of Kafka and look at how they interact with each other as well as look at code for an example of extending Kafka security features. However non-technical listeners will also gain i solid understanding of what is possible out of the box and what isn't.\n\u00a0\n", "title": "Kafka Security - A brief overview of its history, current state and how it can be customized", "url": "https://2018.berlinbuzzwords.de/18/session/kafka-security-brief-overview-its-history-current-state-and-how-it-can-be-customized.html", "speaker": "S\u00f6nke Liebau"}, {"level": "Beginner", "track": "Stream", "abstract": "There\u2019s a lot of excitement and growing interest in machine learning, even in mainstream business practice. The Data Scientist is a Star whose skills are much in demand. But for machine learning to be a practical success in real world, production settings, there\u2019s a lot beyond the algorithm and model that must be done correctly. Is the question or issue being addressed by ML appropriate for the specific goal? Can models be evaluated in a meaningful way and deployed and maintained in production with the expertise available? \u00a0Is there sufficient domain knowledge to make certain the project is addressing the right issues? Is there a way to take practical action on the insights gained through machine-driven decisions?\nThis presentation will explore real world examples to discover some of the pitfalls to avoid and to show a variety of tips and best practices - from planning to model management and deployment in production -- that can help make your machine learning systems successful.\n", "title": "Beyond the Algorithm: What Makes Machine Learning Work?", "url": "https://2018.berlinbuzzwords.de/18/session/beyond-algorithm-what-makes-machine-learning-work.html", "speaker": "Ellen Friedman"}, {"level": "Intermediate", "track": "Store", "abstract": "The folk wisdom has always been that when running stateful applications inside containers that the only viable choice was to externalize the state so that the containers themselves are stateless or nearly so. Keeping large amounts of state inside containers is possible, but it is considered a problem because stateful containers generally can\u2019t preserve that state across restarts.\nIn practice, this complicates the management of large-scale Kubernetes-based infrastructure because these high-performance storage systems require separate management. In terms of overall system management, it would be ideal if we could run a software-defined storage system directly in containers managed by Kubernetes, but that has been hampered by lack of direct device access and difficult questions about what happens to the state on container restarts.\nI will describe recent developments that make it possible for Kubernetes to manage both compute and storage tiers in the same cluster. Container restarts can be handled gracefully without loss of data or a requirement to rebuild storage structures and access to storage from compute containers is extremely fast. In some environments it is even possible to implement elastic storage frameworks that can fold data onto just a few containers during quiescent periods or explode it in just a few seconds across a large number of machines when higher speed access is required.\nThe benefits of systems like this extends beyond management simplicity because applications can be more agile precisely because the storage layer is more stable and can be uniformly accessed from any container host. Even better, it makes it a snap to configure and deploy a full-scale compute and storage infrastructure.\n", "title": "Big News for Big Data in Kubernetes", "url": "https://2018.berlinbuzzwords.de/18/session/big-news-big-data-kubernetes.html", "speaker": "Ted Dunning"}, {"level": "Intermediate", "track": "Search", "abstract": "Within many search solutions, the data quality of the index is not only determined by the indexing procedure (e. g. tokenization, lowercasing), but also by data load and data preparation processes running before the data actually is pushed into the search application. However, developing these processes can be a great challenge. In many cases, data has to be extracted from one or more sources, transformed and enriched in different ways as well as loaded into the target search application, all of which can imply a lot of complexity. Various time consuming issues and challenges potentially have to be overcome, such as dealing with performance bottlenecks, transforming different data formats, synchronizing source systems, enriching data via lookups in source systems or considering security mechanisms within clients.\nMeeting these challenges is particularly difficult in the context of search applications as the most widespread tools that are specialized on handling such problems are only able to deal with structured data. However, search solutions mostly require dealing with semistructured (json, xml) or unstructured data (logs, full texts). For this reason, the open source Apache Top-Level project Apache NiFi is a very powerful option for search solutions. It provides various methods to extract data from various different source systems, to process data of any format as well as to load data into search applications. Users can develop dataflows via a user interface in order to realize a resource efficient, fault tolerant and highly scalable processing of data.\u00a0\nThe presentation shows how developing search solutions can be facilitated using Apache NiFi. Useful features to process and to enrich semistructured and unstructured data are demonstrated. Finally, recend advances in the interaction with Apache Solr and Elasticsearch are discussed. \u00a0\n", "title": "Meeting complex data load and data preparation challenges for search applications with Apache NiFi", "url": "https://2018.berlinbuzzwords.de/18/session/meeting-complex-data-load-and-data-preparation-challenges-search-applications-apache-nifi.html", "speaker": "Johannes Peter"}, {"level": "Beginner", "track": "Stream", "abstract": "This workshop will give you hands-on experience using Kafka Streams to solve a variety of event stream processing problems. The examples and exercises are based on real-world usage from our stream processing platform in Schibsted, used to process over 800 million incoming events daily from users across the globe. We will cover topics ranging from the basics like filtering and transforming events, to how you can use Kafka Streams for data routing, aggregating events, and enriching and joining event streams. To solve the exercises we'll be using Java 8 and/or Scala.\n", "title": "Event stream processing using Kafka Streams", "url": "https://2018.berlinbuzzwords.de/18/session/event-stream-processing-using-kafka-streams.html", "speaker": "Fredrik Vraalsen"}, {"level": "Intermediate", "track": "Scale", "abstract": "How does one build a pyspark model and deploy it in a scala pipeline with no code rewrite - Solving the greatest fights between datascientist who want to code in python and data engineers who like the tried and tested type safety of the JVM.\n\u00a0How does one beat the spark context latency to serve spark models in milliseconds to handle near realtime business needs\n\u00a0How does one build a ML model, zip it up and deploy it across platforms in a completely vendor neutral way i.e. build your model on AWS and deploy it on GCP or vice-versa.\n\u00a0How does one leverage the years of efforts spent in software engineering and use it directly in building datascience pipelines without reinventing the wheel and pain.\n\u00a0How does on build a completely GDPR compliant machine learning model with 0.88 on the ROC curve.\n", "title": "Deploying Large Spark Models to production and model scoring in near real time", "url": "https://2018.berlinbuzzwords.de/18/session/deploying-large-spark-models-production-and-model-scoring-near-real-time.html", "speaker": "Subhojit Banerjee"}, {"level": "Intermediate", "track": "Scale", "abstract": "Before releasing a public dataset, practitioners need to thread the balance between utility and protection of individuals. In this talk we'll move from theory to real-life while handling massive public datasets. We'll showcase newly available tools that help with PII detection, and bring concepts like k-anonymity and l-diversity to a practical realm.\nRelated research: \"Considerations for Sensitive Data within Machine Learning Datasets\" - https://cloud.google.com/solutions/sensitive-data-and-ml-datasets\n", "title": "Protecting sensitive data in huge datasets: Cloud tools you can use", "url": "https://2018.berlinbuzzwords.de/18/session/protecting-sensitive-data-huge-datasets-cloud-tools-you-can-use.html", "speaker": "Felipe Hoffa"}, {"level": "Intermediate", "track": "Scale", "abstract": "Programming is a human communication activity. We want to minimize misunderstandings in our code to be able to work effectively as teams. This means we need to learn how to look at our code to spot areas where we could improve our communication skills. We want to get our ideas across. We want that our abstractions, our models, make sense to others.\nLiterature is a discipline with a long track record of authors and researchers trying to find out how to make writing communication effective. What could we learn from them?\nIn this talk I want to explore the relation between the process of writing computer programs with that of writing literary works of fiction. In particular I want to show some ideas presented by Umberto Eco in his book Lector in Fabula, seeing how we can improve knowledge sharing via our code, tests, documentation, and other artifacts.\nThe goal is to learn the skills required to help others understand how we made decisions about the tradeoffs in our code, like choosing abstractions, deciding on the level of performance required, or the amount of documentation needed for a project.\n\u00a0\n", "title": "Lector in Codigo", "url": "https://2018.berlinbuzzwords.de/18/session/lector-codigo.html", "speaker": "Alvaro Videla"}, {"level": "Intermediate", "track": "Store", "abstract": "Indexes are what make efficient access for our data storage systems possible. Though traditionally implemented with highly-optimized tree-based data structures, this past December\u00a0a group from Google proposed a novel idea: replace certain types of index structures with trained machine learning algorithms. After all, an index is nothing other than a model that maps a key to the position of a record; in this light, exchanging, say, your B-tree search with\u00a0a deep neural network prediction seems at least possible, if not practical. Surprisingly, doing so can often lead to significant performance improvements, in terms of both time and memory consumption.\nIn this talk we discuss how learned indexes\u00a0accomplish this. We focus on neural networks, and in particular how recent trends in processor architecture design make them computationally competitive against tree search. We then have a look at how machine learning algorithms can be applied to the task of range indexation, how they can deliver error bound guarantees, and how their accuracy can be honed by layering them recursively. We finish with a review of the Google group's results on three realistic datasets and a brief mention of how machine learning can be applied to other indexation tasks.\n", "title": "Learned Indexes: a New Idea for Efficient Data Access", "url": "https://2018.berlinbuzzwords.de/18/session/learned-indexes-new-idea-efficient-data-access.html", "speaker": "Robert Rodger"}, {"level": "Beginner", "track": "Scale", "abstract": "Large amounts of unknown data seeks helpful tools to identify itself and generate content! Ideally without crashing too often...\nWith one or two files, you can take time to manually identify them, and pull out useful content. With thousands of files, or the internet's worth, no amount of mechnical turks will scale this for you! Rolling your own will be slow, and probably crash your JVM... Luckily, there are open source tools and programs out there to help.\nWe'll start by figuring out why identifying what a given blob of 1s and 0s represents is tricky. Then, we'll see how tools like Apache Tika can help identify, and extract common metadata, text and embedded resources. As we scale out, we'll see how things can go wrong. Finally, we'll see how best to handle Big Data quantities, without crashing your cluster! (Too often...)\n", "title": "Scalably crashing JVMs, or why binary data to content is hard", "url": "https://2018.berlinbuzzwords.de/18/session/scalably-crashing-jvms-or-why-binary-data-content-hard.html", "speaker": "Nick Burch"}, {"level": "Intermediate", "track": "Search", "abstract": "Modern search systems at scale are often architected as a real-time processing pipeline where the query and its results flow through multiple stages before returning them to the user. Concretely, a \u00a0search query flowing through this pipeline might be stemmed by a stemmer, tokenized by a \"parts of speech\" natural language parser, transformed to a query plan conforming to a boolean retrieval model, executed against an inverted index, \u00a0and finally the top-k results could be reordered based on an online machine-learned ranker. Many of these transformations of queries and results require performing a network IO to external specialized services. Our proposal is to model the search pipeline as a monad transformer composed of Reader monad and a Future.\n\u00a0\nAbout 1500 people search for handmade and vintage items on Etsy every second. Several different backends power Etsy's search, among them Solr, Elasticsearch, our own key-value-store Arizona, and services for machine learning and inference. How do all these systems work together, present a common interface to Etsy's developers and a coherent search experience to our users?\n\u00a0\nThis problem requires a distributed system that scales very well, and has a state space that is still easy to reason about. We have built a smart proxy in scala with minimal state to solve this problem. It expands on the ideas of the \"Your Server as a Function\" paper. The idea is that basically all program state comes in via the request encoded in a Reader monad, the proxy calls out to the appropriate backend services and combines their responses, behaving like a pure function.\n\u00a0\nThe proxy retrieves search results from a Solr backend, ranks them based on a machine learning model that incorporates the user's context, and returns the ranked results. If a search is unsuccessful, it can decide to kick off alternative search requests and return their results instead, without needing any frontend interaction. Via an intermediate tree representation that encodes a boolean retrieval model which is independent of the backends, we can combine key-value-store, machine learning and search indexes to improve the search result for the end user. The interplay of the different backends to create new services is achieved via a configuration system. It makes heavy use of Scala's type system to lift the types from the data fields in the storage backends into the scala code and ensure consistent types from the incoming thrift request throughout the entire system.\n\u00a0\nTwitter\u2019s Finagle library provides us with RPC clients that abstract over different protocols like thrift, mux or http and lets us interact with them in an asynchronous manner via Futures. Using Futures, we can build custom machine learning pipelines by composing sequentially & concurrently over calls to backend services. It becomes easy to add custom query pipelines for machine learning, such as a recommendation query pipeline that queries a user database, gets the user\u2019s purchase history and favorites, and constructs a recommendation set using a model that takes the user context as input. With experimentation as a first class citizen in our design, A/B testing allows us to test different feature sets and modelling techniques in our machine learning services.\n\u00a0\nIn order to maintain independent failure domains for suggestions and search we run the proxy on Kubernetes as separate services. The Kubernetes autoscaler helps us to react to changing traffic patterns in a flexible way. We will talk about our learnings along the way of building this proxy, and trying to find the right abstraction for the search problem.\n\u00a0\n", "title": "Your Search Service as a Composable Function", "url": "https://2018.berlinbuzzwords.de/18/session/your-search-service-composable-function-0.html", "speaker": "Stefanie SchirmerAakash Sabharwal"}, {"level": "Intermediate", "track": "Scale", "abstract": "DataFrame is an awesome interface for data manipulation in\u00a0Spark\u00a0but when the complexity grows outside of the capabilities of\u00a0Spark\u00a0itself, you need to resort to \"violence\". In this talk I will explain one of the projects which became too complex to be executed using the DataFrame API and had to be rewritten into a custom code applied using mapPartitions function. We will cover some of the tips and tricks for reducing lineage complexity, share our process of analyzing pain points and get into details of mapPartitions functionality to leverage\u00a0Spark's distributed processing capabilities and reliability while executing custom code.\n", "title": "When DataFrames fail, resort to mapPartitions", "url": "https://2018.berlinbuzzwords.de/18/session/when-dataframes-fail-resort-mappartitions.html", "speaker": "Matija Gobec"}, {"level": "Intermediate", "track": "Search", "abstract": "The goal of Solr's AutoScaling framework is for search clusters to be able to grow to a trillion documents without much human intervention.\nThe first part of the talk covers AutoScaling framework concepts. We'll talk about AutoScaling Policies and Preferences, the AutoScaling API and event triggers.\nFurther, we\u2019ll discuss practical use-cases to keep the cluster healthy and performing optimally, complete with fault tolerance.\nFor example, we'll cover how to achieve these scenarios by utilizing the framework.\nEffectively managing disk space by setting triggers and sending out alerts.\nMaintaining a minimum replication factor when nodes go down. We'll also use rules to make sure the replicas are spread out, thus maximizing fault tolerance.\nScaling out replicas to serve more traffic by setting thresholds. The thresholds could be latency or QPS based. We could also run it as schedulers to better serve peak load.\nMove replicas around to balance load across the cluster.\nIndexing triggers: Are shards getting too large? Support for auto shard splits etc.\n", "title": "Practical Use-Cases of Solr's AutoScaling Framework", "url": "https://2018.berlinbuzzwords.de/18/session/practical-use-cases-solrs-autoscaling-framework.html", "speaker": "Varun Thacker"}, {"level": "Intermediate", "track": "Search", "abstract": "The default boolean retrieval model in generic full text search engines is usually not the best choice for domains like e-commerce because of the lack of decidability about optimal combination of conjunctions and disjunctions of terms that would yield a significant number of relevant results. To improve this, Apache Lucene/Solr introduced the minimum match criteria, where a specified minimum number of query terms must match. But the problem is to decide and tell Solr which are the most important terms (the most salient query theme) that \u201cmust\u201d match.\nWe present a framework to identify (a) important weighted terms in queries (called Must-have Tokens or MTs) and (b) augment them using synonyms. A dependency parser learns weights of tokens to build the MTs list and a neural net embedding and word sense disambiguation is deployed to learn the synonyms specific to MTs in the query context. The models to determine them are built at scale on Apache Spark by analysing clickstream and catalog data across various domains. A custom query parser for Solr is used to augment the queries with these MTs and synonyms.\n", "title": "From boolean towards semantic retrieval models", "url": "https://2018.berlinbuzzwords.de/18/session/boolean-towards-semantic-retrieval-models.html", "speaker": "Arpan GuptaSeinjuti Chatterjee"}, {"level": "Intermediate", "track": "Search", "abstract": "Apache Solr, Elasticsearch and Vespa are three of the most popular general purpose search engines that are used in search, recommendations and analytics based applications. Machine learning is a critical aid to improving relevance beyond the native ranking algorithms (e.g. TF-IDF, BM25 etc.) by leveraging editorial judgements and user behaviour and factoring them into ranking of results. This is known as \"learning to rank\" (LTR) or \"machine learned ranking\" (MLR) etc.\nIn this talk, the speaker presents a comparison of machine learning frameworks across all of these search engines [1]. The comparison is followed by a quick demonstration on how to use all of these frameworks. This talk is aimed at those who are looking to decide upon which search engine to use in their applications based on the machine learned ranking capabilities available for it, and the ease of using such features.\n[1] 1. LTR based re-ranking module in Solr, https://lucene.apache.org/solr/guide/7_2/learning-to-rank.html \u00a0\u00a0\u00a0 \u00a0 2. Elasticsearch's LTR plugin, https://github.com/o19s/elasticsearch-learning-to-rank \u00a0\u00a0 \u00a0\u00a0 3. Vespa's MLR functionality, http://docs.vespa.ai/documentation/ranking.html\n\u00a0\n", "title": "Machine Learning for Apache Solr, Elasticsearch & Vespa", "url": "https://2018.berlinbuzzwords.de/18/session/machine-learning-apache-solr-elasticsearch-vespa.html", "speaker": "Mitali Jha"}, {"level": "Intermediate", "track": "Stream", "abstract": "Collaborative filtering is a well known method to implement recommendation engines. Although modern techniques, such as Alternating Least Squares (ALS), allow us to perform product rating predictions with large amounts of observations, typically ALS is implemented as a (distributed) batch algorithm where retraining must be performed with the entirety of the data. However, in scenarios where we have large amounts of data arriving as stream, batch retraining might be problematic. In this talk Rui will guide us in building a distributed streaming ALS implementation based on Stochastic Gradient Descent, where model training can be performed using the observations as they arrive. The advantages of real-time streaming collaborative filtering will be discussed as well as the scenarios where batch ALS might be preferable.\n", "title": "Building your own Distributed Streaming Recommendation Engine", "url": "https://2018.berlinbuzzwords.de/18/session/building-your-own-distributed-streaming-recommendation-engine.html", "speaker": "Rui Vieira"}, {"level": "Beginner", "track": "Scale", "abstract": "A large fraction of big data projects fail to deliver return of investment, or take years before they do so. The reasons are typically a combination of project management, leadership, organisation, available competence, and technical failures. In this presentation, I will focus on the technical aspects, and present the most common or costly data engineering mistakes that I have experienced when building scalable data processing technology over the last five years, as well as advice for how to avoid them. The presentation includes war stories from large scale production environments, some that lead to reprocessing of petabytes of data, or DDoSing critical services with a Hadoop cluster, and what we learnt from the incidents.\n", "title": "Top 10 data engineering mistakes", "url": "https://2018.berlinbuzzwords.de/18/session/top-10-data-engineering-mistakes.html", "speaker": "Lars Albertsson"}, {"level": "Beginner", "track": "Stream", "abstract": "Taking my lead from the (alleged) Russian participation in the 2016 US election, my contribution to the 2017 UK general election was @dissidentbot. This is 500 lines of ruby code running on a raspberry pi and tasked with heckling UK politicians via Twitter. As such it provided a valuable contribution to the democratic debate. While twitter has not yet identified\u00a0@Dissidentbot as actively interfering with the UK election, I am confident its few hundreds of tweets helped shape the outcome.\nThis talk looks at the engineering aspects of the project, the\u00a0production experience\u00a0such as Twitter's spam blocking algorithms, runaway-reply-loops, and the challenge of generating new content.\nMore\u00a0important\u00a0are the social and ethical aspects. Twitter has changed the nature of communication between polticians and the electorate, sometimes for the better, but sadly, sometimes not. Does @Dissidentbot improve the communications by giving us an immediate right to respond with our own message?\u00a0Or does it worsen the on-line discourse which it's clear is shaping referendums and elections.\nMost critically: the fact that a ruby script randomly picking phrases can become an active participant in the online debate of an election is not a good sign for a functional democracy.\u00a0\nExpect live demos with audience participation\n", "title": "@Dissidentbot: dissent will be automated!", "url": "https://2018.berlinbuzzwords.de/18/session/dissidentbot-dissent-will-be-automated.html", "speaker": "Steve Loughran"}, {"level": "Beginner", "track": "Search", "abstract": "Apache Solr has come a long way from being used for simple full-text search to modern day analytics, geospatial, media and multi-tenant search applications. However, it suffers from the inductive problem of \u201cschema-resolution\u201d.\nWhile there exists a \u201cschema-less\u201d mode in Apache Solr, it doesn't really solve the above problem, as it generates a very generic schema under the hood. At Unbxd, a multi-tenant e-commerce search platform, arriving at the most optimal schema is critical for performance and functionality.\nThis talk presents our contribution to Solr (SOLR-11741), a \u201cschema-learning mode\u201d that leverages \u201cfield type hierarchy\u201d to solve the schema inference problem by learning from source documents and run-time query patterns. The talk additionally focuses on how searching, sorting & faceting can become more efficient with this feature and provide insights into data anomalies.\n", "title": "Schema Learning in Apache Solr", "url": "https://2018.berlinbuzzwords.de/18/session/schema-learning-apache-solr.html", "speaker": "Abhishek Kumar Singh"}, {"level": "Beginner", "track": "Scale", "abstract": "Academic researchers find novel solutions to thorny problems in idealized environments. \u00a0A research background is excellent preparation for advancing the state of the art, but newly-minted professional data scientists can find themselves in industry with an arsenal of problem-solving techniques that are not as potent as they seemed in graduate school: \u00a0data sets are larger and messier, solutions are judged by their outcomes rather than by their novelty, and products, unlike publications, require ongoing maintenance and support.\n\u00a0\nThis talk will draw on the speaker\u2019s experience bringing a mathematics research background to a team in industry. \u00a0We will show both the challenges that data scientists face when entering industry from academia and the unique skills that they bring from their research background. \u00a0We shall frame the discussion with a running example of cutting-edge statistical research embodied in an imperfect implementation. We\u2019ll demonstrate iterative refinements to our implementation, showing how to take a research prototype to production code, with particular attention to real-world pitfalls that might not appear in a researcher\u2019s daily work. \u00a0Finally, we\u2019ll show how trained researchers can turn their background into a superpower for applied teams in industry.\n\u00a0\nEarly-career attendees who are considering joining industry from academia will learn how to navigate the challenges they\u2019ll face on a mixed team and how to best use their gifts and skills in a new environment. Established practitioners will learn how to support, engage, and nurture their colleagues who are transitioning from academia. \u00a0Everyone will learn how to adapt implementations and ideas from the research world for production applications.\n\u00a0\n", "title": "From Research to Production: What they didn\u2019t teach you in Grad School", "url": "https://2018.berlinbuzzwords.de/18/session/research-production-what-they-didnt-teach-you-grad-school.html", "speaker": "Sophie Watson"}, {"level": "Intermediate", "track": "Scale", "abstract": "The usual steps of developing a machine learning model are: training on a training set, tuning on a validation set and evaluating the performance on the test set. Often this is the end of the story. However, if the model is particularly good, it will be deployed to serve predictions in a production system.\nIn this talk we present what happens to a machine learning model after it is deployed in production at Zalando Payments. We focus on the precautions we need to take to ensure that a model\u2019s predictions always stay at the high quality we expect. The stakes are high, particularly for models that directly touch the revenue stream. Since we cannot afford to let a drop in prediction quality pass unnoticed, we need to continuously monitor our deployed machine learning models.\nAs we operate in the fraud detection domain, one additional challenge we face is that we only know several weeks later if a customer paid his order at Zalando and if our predictions were accurate in that case. This makes the simple solution of monitoring the prediction accuracy impractical, because by the time we notice the problem, it is already too late.\nIn this talk, we present our solution, which consists of monitoring the similarity between the distributions of features in the live traffic and the distributions of features in the test set on which the model was evaluated. This allows us to immediately detect if the conditions under which the model was evaluated have substantially changed, which would invalidate the conclusions we drew in the initial testing. We describe how the mentioned changes in feature distributions are automatically detected using the TDigest algorithm, and how alerts are raised.\nFurther, we delve into the technical implementation decisions: First, we describe how we collect the live traffic of a mission-critical service in a non-intrusive way, in order to avoid interfering with the normal operation of the service. Secondly, we present how the collected data is processed in a scalable way using Apache Spark. Finally, we show how we automate everything with AWS Data Pipelines.", "title": "Continuous Live Monitoring of Machine Learning Models with Delayed Label Feedback", "url": "https://2018.berlinbuzzwords.de/18/session/continuous-live-monitoring-machine-learning-models-delayed-label-feedback.html", "speaker": "Patrick BaierLorand Dali"}, {"level": "Intermediate", "track": "Scale", "abstract": "The landscape for storing your big data is quite complex, with several competing formats and different implementations of each format. Understanding your use of the data is critical for picking the format. Depending on your use case, the different formats perform very differently. Although you can use a hammer to drive a screw, it isn\u2019t fast or easy to do so.\nThe use cases that we\u2019ve examined are:\nreading all of the columns\nreading a few of the columns\nfiltering using a filter predicate\nwriting the data\n\nWhile previous work has compared the size and speed from Hive, this presentation will present benchmarks from Spark including the new work that radically improves the performance of Spark on ORC. This presentation will also include tips and suggestions to optimize the performance of your application while reading and writing the data.\nFinally, the value of having open source benchmarks that are available to all interested parties is hugely important and all of the code is available from Apache. \u00a0\n", "title": "Fast Access To Your Complex Data - Avro, JSON, ORC, and Parquet", "url": "https://2018.berlinbuzzwords.de/18/session/fast-access-your-complex-data-avro-json-orc-and-parquet.html", "speaker": "Owen O'Malley"}, {"level": "Intermediate", "track": "Scale", "abstract": "A classifier labeling Van Gogh drawings as invoices and a chatbot insulting users on Twitter are only two examples of Machine Learning (ML) models, which went wild as soon as they hit production. Although evaluated on a test set, in the face of unseen data ML models oftentimes\u00a0behave in an unpredictable way. Depending on the application, such a model may lead to decreasing revenue, bad reputation or even a threat to the health of people.\nTo ensure a stable rollout of new models into production, we have to promote ML models to first class citiziens in the Continuous Delivery pipeline. Kubernetes and Kafka are two great tools to support the rollout of new machine learning models in a (semi-) automated way. I will show a pipeline built with these tools, which will lead to more confidence in your deployments and happier\u00a0users.\n", "title": "Ship your Machine Learning Application", "url": "https://2018.berlinbuzzwords.de/18/session/ship-your-machine-learning-application.html", "speaker": "Michael Stockerl"}, {"level": "Beginner", "track": "Search", "abstract": "Nearly everybody\u00a0once faced\u00a0this situation:\u00a0you\u00a0visit\u00a0an\u00a0online shopping site,\u00a0head over to the search\u00a0box\u00a0and\u00a0start typing. Next,\u00a0you'll get a dropdown\u00a0list of search\u00a0suggestions\u00a0\u2013 but\u00a0many\u00a0times\u00a0these\u00a0suggestions are\u00a0of\u00a0poor quality.\u00a0On the other\u00a0hand,\u00a0search suggestions\u00a0are\u00a0a helpful feature\u00a0and\u00a0add\u00a0value to\u00a0the search\u00a0user experience. In addition, they\u00a0can\u00a0also\u00a0increase\u00a0your\u00a0shop performance w.r.t.\u00a0indicators\u00a0such\u00a0as\u00a0'add to basket' and 'order value'.\u00a0But\u00a0what most people\u00a0don\u2019t expect:\u00a0bad suggestions\u00a0can\u00a0potentially\u00a0harm\u00a0your shop performance.\u00a0\nWe've\u00a0observed\u00a0during A/B tests\u00a0on\u00a0our\u00a0top tier\u00a0e-commerce\u00a0site\u00a0that\u00a0poor search\u00a0suggestions\u00a0may\u00a0have\u00a0a negative\u00a0impact on\u00a0our\u00a0business\u00a0performance indicators.\u00a0Based on this\u00a0observation we\u00a0decided to start a project to\u00a0iteratively improve the quality of our search suggestion engine.\u00a0\nIn this\u00a0talk, we\u00a0will\u00a0share our iterative methodology and\u00a0our\u00a0approach to\u00a0solving the suggestion problem using Elasticsearch, Lucene and machine learning techniques.\u00a0\n", "title": "Search Suggestions \u2013 the underestimated killer feature of your online shop", "url": "https://2018.berlinbuzzwords.de/18/session/search-suggestions-underestimated-killer-feature-your-online-shop.html", "speaker": "Joshua BacherChristine Bellstedt"}, {"level": "Intermediate", "track": "Search", "abstract": "Recommendation engines are one of the most well-known, widely-used and highest value use cases for applied machine learning. Search and recommender systems are closely linked, often co-existing and intermingling. Indeed, modern search applications at scale typically involve significant elements of machine learning, while personalization systems rely heavily on and are deeply integrated with search engines. In this session, I will explore this link between search and recommendations.\nIn particular, I will cover three of the most common approaches for\u00a0using search engines to serve\u00a0personalized\u00a0recommendation models. I call these the score then search, native search\u00a0and custom ranking\u00a0approaches. I will detail each approach, comparing\u00a0it with the others\u00a0in terms of various considerations important for production systems at scale, including the architecture, schemas, performance, quality and flexibility\u00a0aspects. Finally, I will also contrast these model-based approaches with what is achievable using pure search.\n", "title": "Search and Recommendations: 3 Sides of the Same Coin", "url": "https://2018.berlinbuzzwords.de/18/session/search-and-recommendations-3-sides-same-coin.html", "speaker": "Nick Pentreath"}, {"level": "Intermediate", "track": "Scale", "abstract": "New data-processing frameworks such as Spark and Flink have made writing Hadoop jobs very easy. However, as the data grows, developers face new challenges: from stability issues to allocating the right amount of resources, large jobs are often hard to tune and debug as their distributed nature and scale make them hard to observe.\nBabar, an open source profiler developed at Criteo, was introduced to make it easier for users to profile their Hadoop jobs and their resource usage with little effort and instrumentation. It helps understand CPU usage, memory consumption and GC load over the entire application, as well as where the CPU time is spent using flame graph\u00a0visualizations.\nIn this session, we will see how to instrument a Spark job and go through its optimization in order to improve latency and stability, while reducing its footprint on the cluster.\n", "title": "Profiling and optimizing a Spark job with Babar", "url": "https://2018.berlinbuzzwords.de/18/session/profiling-and-optimizing-spark-job-babar.html", "speaker": "Benoit Hanotte"}, {"level": "Intermediate", "track": "Stream", "abstract": "The Schibsted Data Platform is the global processing hub for data in Schibsted, and we receive roughly 800 million user behaviour events from more than 40 sites worldwide each day. The Data Platform\u2019s responsibility is not only to collect, structure, and index the incoming data, but also add extra value by adding additional information to the events, known as enrichments.\nTo offer targeted advertising based on location, the Data Platform enriches all incoming events using an API that will translate IP addresses into coordinates. To do this in real-time and at scale, with sub-second latencies, we utilize Kafka and Kafka Streams.\nIn this presentation, I will introduce Kafka, Kafka Streams, the Kafka Streams DSL and the Processor API to explain how it can be used for branching, caching, bulking, asynchronous HTTP lookups, and joining. I will also talk about experiences related to operations, performance, and scaling.\n", "title": "Translating 800 million IP addresses to coordinates each day using Kafka Streams", "url": "https://2018.berlinbuzzwords.de/18/session/translating-800-million-ip-addresses-coordinates-each-day-using-kafka-streams.html", "speaker": "H\u00e5kon \u00c5mdal"}, {"level": "Intermediate", "track": "Scale", "abstract": "Tensorflow is all kind of fancy, from helping startups raising their Series A in Silicon Valley to detecting if something is a cat. However, when things start to get \u201creal\u201d you may find yourself no longer dealing with mnist.csv, and instead needing do large scale data prep as well as training. This talk will explore how Tensorflow can be used in conjunction with Apache Spark, Flink, and BEAM to create a full machine learning pipeline including that annoying \u201cfeature engineering\u201d and \u201cdata prep\u201d components that we like to pretend don\u2019t exist. We\u2019ll also talk about how these feature prep stages need to be integrated into the serving layer.\nThis talk will also explore how Apache Arrow impacts cross-language development for big-data including things like deep learning. Even if you\u2019re not trying to raise a round of funding in Silicon Valley, this talk will give you tools to do interesting machine learning problems at scale (or find more cats).\n", "title": "Working with Tensorflow from the JVM: How Big Data and Deep Learning can be BFFs", "url": "https://2018.berlinbuzzwords.de/18/session/working-tensorflow-jvm-how-big-data-and-deep-learning-can-be-bffs.html", "speaker": "Holden Karau"}, {"level": "Beginner", "track": "Stream", "abstract": "The digitalization of everyday life, where the average bus passenger would rather ignore the surroundings\u00a0and stare blankly into his phone for snapchat updates, creates great opportunities for digitalization of public transport, as well as some\u00a0challenges. Ruter, Norway\u2019s largest public transport authority, is putting significant effort into improving digital services for customers. \u00a0However, the current technology stack behind Ruter's operations, is not able to provide the necessary real-time information.\nIn this talk I will explain how Ruter is participating in the development of\u00a0new European standards for information technology for public transport (ITxPT), and how Apache\u00a0Kafka and Kafka streams serve as the core components in building \"Transport-as-a-Service\".\n", "title": "Transport-as-a-Service (TaaS) - How we build next generation plug-and-play IT-systems for public transport (ITxPT) at Ruter", "url": "https://2018.berlinbuzzwords.de/18/session/transport-service-taas-how-we-build-next-generation-plug-and-play-it-systems-public.html", "speaker": "Christoffer Vig"}, {"level": "Beginner", "track": "Search", "abstract": "Noise (on Github) is a new open source search library written in Mozilla's Rust programming language. It is centered around JSON documents. Whenever you insert data, it gets automatically indexed. There's no need to take care of your indices manually. Strings will be indexed for full-text searches, numbers for range queries and geospatial data (GeoJSON) for bounding box queries.\nAlthough there's already too many query languages out there, Noise is introducing another one. But compared to others, Noise's query language is is based on a different concept called \"Query by Example\", which got invented in the 70s but has forgotten since. This way of doing queries was adapted for the use with JSON. It leads to an easy to understand syntax that you can also understand months after you've written the query.\nA Node.js binding makes it easy to get started with Noise, it's a simple \"npm install noise-search\".\nAfter the talk you should have an understanding on how Noise is different from other similar systems and how it works internally.\nNoise is licensed under Apache License 2.0 and MIT License.\n", "title": "Noise: a search library in Rust", "url": "https://2018.berlinbuzzwords.de/18/session/noise-search-library-rust.html", "speaker": "Volker Mische"}, {"level": "Intermediate", "track": "Stream", "abstract": "\u00a0\nThroughout the last months, our AntiSpam platform underwent many significant changes. The biggest of them, the migration to the cloud, and the removal of our Hadoop cluster made us change the way we manage and work with our data.\nThis talk will be about this transition. During the first part of the presentation, I will focus on the reasons why we performed the migration and the architecture of our system. Particularly, I want to share which components of Google Cloud we are using, and most importantly how we use them.\nThe second part will be dedicated to the small improvements and advantages we got from the cloud. For example, how BigQuery made it easier for us to train our machine learning models, and how we use Data Studio to make sure our predictions are on point.\nLastly, I will conclude by introducing the new techniques and machine learning models we have developed to detect and punish spammers since our last time at Buzzwords '17.\n\u00a0\n", "title": "Lifting AntiSpam to the cloud and beyond", "url": "https://2018.berlinbuzzwords.de/18/session/lifting-antispam-cloud-and-beyond.html", "speaker": "Juan De Dios Santos Rivera"}, {"level": "Beginner", "track": "Scale", "abstract": "Are you a data analyst who works with Spark and often gets confused by failures you don\u2019t understand? Have you seen a bunch of presentations or blog posts about Spark performance but you are still not certain how to apply the hints you have been given in practice?\nSpark is commonly used by people who are not experts in programming but they know SQL and sometimes basic Python. They treat Spark as a tool for getting business value from the the data. And that is how it should be! Although it\u2019s common that queries they run do not work for any obvious reason. This talk is designed for such Spark users and will be focused on common problems with Spark (especially DataFrames and SQL) which can be solved by anyone familiar with SQL. You don\u2019t need to read bytecode to understand the techniques presented and apply them in practice!\nThis talk will be a case study of multiple DataFrame queries in Spark which initially do not work. I will not only explain how to fix them, but we will go through the solution step-by-step so you will learn what to pay attention to and how to apply similar techniques to your codebase!\n", "title": "DataFrames in Spark - the analysts perspective.", "url": "https://2018.berlinbuzzwords.de/18/session/dataframes-spark-analysts-perspective.html", "speaker": "Marcin Szymaniuk"}, {"level": "Intermediate", "track": "Search", "abstract": "In this session, we will introduce large-scale log management system called NELO used in Naver corp and mainly discuss how to maintain elasticsearch indices for paas style logging system. Naver Corporation is an Internet content service company which operates Korea's top search engine Naver and manages global mobile services such as the mobile messenger LINE, video messenger Snow, and group communication service BAND. NELO is handling various different kinds of logs and more than 3 billions of logs are incoming every day. As backend storage and search engine, we are heavily depending on elasticsearch. Because the number of logs and variety of logs is increasing, managing indices in elasticsearch clusters are more and more complicated. In the beginning, we only created one index every day, but as scales are growing, we suffered mapping explosion issues and performance issues. By introducing index management service inside NELO, now we have resolved mapping explosion issues and supported custom type and custom retention time, etc. In this session we will explain our first and recent index model and how to resolve mapping explosion and how to support custom type. From this information, users will be able to understand difficulties of maintaining large scale elasticsearch cluster and index model for multi-tenant log management system which can cover many different kinds of logs with different mappings.\n", "title": "Elasticsearch index management for paas style logging system", "url": "https://2018.berlinbuzzwords.de/18/session/elasticsearch-index-management-paas-style-logging-system.html", "speaker": "Jaeik LeeQin Tang"}, {"level": "Beginner", "track": "Scale", "abstract": "Intelligent applications employ machine learning and large-scale data processing to improve with longevity and popularity. Most of the applications you can\u2019t live without today are intelligent, and it\u2019s an easy bet that the applications you\u2019ll be most excited about developing tomorrow will be intelligent as well. It\u2019s an even easier bet that you\u2019ll want to be deploying tomorrow\u2019s applications on a contemporary container platform with a great developer workflow like Kubernetes.\nIntelligent applications pose some new challenges for developers, but this hands-on workshop will show you how to navigate them confidently. You'll learn how to develop an intelligent application on Kubernetes from the ground up, using an open-source stack including Jupyter, Numpy, Apache Spark, and other community projects. We\u2019ll cover:\nWhy contemporary analytics frameworks are a good fit for microservice architectures;\nA crash course in some essential data processing and machine learning techniques;\nDevelopment workflows for cross-functional teams;\nHow to deploy scale-out compute clusters as part of contemporary applications; and\nBuilding a data-driven application in the cloud, from the ground up.\n\nThis workshop is largely self-contained: \u00a0bring some familiarity with Python and leave empowered and inspired to develop a great intelligent application. While some parts of our presentation explain concepts in the context of Apache Spark, the techniques and concepts you\u2019ll learn are applicable to developing applications using any parallel compute framework supporting elastic scale-out.\n\u00a0\n", "title": "Intelligent applications in containers:  from prototype to production", "url": "https://2018.berlinbuzzwords.de/18/session/intelligent-applications-containers-prototype-production.html", "speaker": "William BentonMichael McCune"}, {"level": "Intermediate", "track": "Scale", "abstract": "With the abundance of Remote Sensing satellite imagery, the possibilities are endless as to the kind of insights that can be derived from them. One such use is to determine land use for agriculture and non-agricultural purposes.\nIn this talk, we\u2019ll be looking at leveraging Sentinel-2 satellite imagery data along with OpenStreetMap labels to be able to classify land use as agricultural or non-agricultural. Sentinel-2 data has a 10-meter resolution in RGB bands and is well-suited for land use classification. \u00a0Using these two datasets, many different ML tasks can be performed like - image segmentation into two classes (farm land and non-farm land) or more challenging task of identification of crop type being cultivated on fields. \u00a0\nFor this talk, we\u2019ll be looking at leveraging Convolutional Neural Networks (CNNs) \u00a0built with Apache MXNet to train Deep Learning models for land use classification. We\u2019ll be covering the different Deep Learning Architectures considered for this particular use case along with the performance metrics for each of the different architectures.\nWe\u2019ll be leveraging streaming pipelines built on Apache Flink for model training and inference. Developers will come away with a better understanding of how to analyze satellite imagery and the different Deep Learning architectures along with their pros/cons when analyzing satellite imagery for land use.\n", "title": "Large Scale Landuse Classification of Satellite Imagery", "url": "https://2018.berlinbuzzwords.de/18/session/large-scale-landuse-classification-satellite-imagery.html", "speaker": "Suneel Marthi"}, {"level": "Intermediate", "track": "Search", "abstract": "A user types \u201cblack clutch\u201d into your search engine. Do they mean the handbag, the automobile part, or something else entirely?\nSearch is about matching the intent of the user with the information they need. For decades, \u201crelevance\u201d in information retrieval systems has meant things like BM25, TFIDF, field boosting, document boosting, etc. These simple heuristics and strategies have served us well, but ultimately fall short because they fail to semantically model intent. Our systems don\u2019t actually understand what users want, they just hope a few magic numbers will get us close enough.\nQuery Understanding is about using real intelligence to put users first. In this session, we\u2019ll talk about what Query Understanding is, why it\u2019s important, and some practical strategies for making your search experience smarter.\n", "title": "Getting Started with Query Understanding", "url": "https://2018.berlinbuzzwords.de/18/session/getting-started-query-understanding.html", "speaker": "Giovanni Fernandez-Kincade"}, {"level": "Beginner", "track": "Scale", "abstract": "Have you seen the Silicon Valley episode where they trained a neural network on a mobile phone to recognize whether an object is a Hotdog or not a Hotdog? We're going to build that app, and more, in just a few hours - we will do everything from training a neural network to having an app that can take photos, send it to a deployed server to test it against your trained model (model inferencing). All this will be done with end-to-end TLS security. This workshop is aimed at data scientists and data engineers who are eager to learn about deep learning with Big Data. We will write, train, and deploy a convolutional neural network in Tensorflow/Keras on the Hops platform as a Jupyter Notebook. \u00a0We will show you how you can scale out training to reduce training time. We will also show you how to embed Deep Neural Networks (DNNs) in production pipelines for both training and inference using Apache Spark. We will base our tutorial on exercises given to students in Sweden\u2019s first graduate course on Deep Learning \u2013 ID2223 at KTH. We will provide both a virtual machine instance and access to the same cluster used in our course, at www.hops.site.\nAll code and datasets are 100% open source. Code is available from Github at https://github.com/hopshadoop, while datasets are available from Hops Hadoop at www.hops.site.\u00a0All you will need is your laptop and an Internet connection.\n", "title": "TensorFlow: Hotdog or Not with Toppings (Training/Inferencing/Security/Distribution)", "url": "https://2018.berlinbuzzwords.de/18/session/tensorflow-hotdog-or-not-toppings-traininginferencingsecuritydistribution.html", "speaker": "Jim Dowling"}, {"level": "Intermediate", "track": "Scale", "abstract": "Machine Learning is at the heart of the Criteo platform. We use ML to determine which ads to display to the right user at the right time. Models on user browsing history and advertiser attributes are trained offline and used online to determine which ad to display to a given user.\nIn this talk, I will present Criteo's ML pipeline allowing us to read 5 PB of logs to train and deploy several thousands models a day, continuously improve the features of our models and use these models to predict which ads we need to display online. Displaying 3.5 billions daily ads online requires 500 000 predictions per second.\nI will expose the problems we had and how we resolved them. This includes open source technologies such as HDFS, Yarn, Spark and home built technologies for our ml algorithms, scheduling, testing and monitoring.\n\u00a0\n", "title": "From 5 PB of daily logs to thousands of ML models - Story of a ML pipeline", "url": "https://2018.berlinbuzzwords.de/18/session/5-pb-daily-logs-thousands-ml-models-story-ml-pipeline.html", "speaker": "Fabian H\u00f6ring"}, {"level": "Intermediate", "track": "Search", "abstract": "Learning to rank (LTR) has been considered the next generation tool to improve relevance of product search solutions. Inspired by its popularity (Buzzword 2017) and by the challenges we have at GetYourGuide, a global marketplace for tours and activities, we started a project to introduce LTR in our Search Engine. In this talk, we would like to share our Logbook from day 1 to the current project status. It covers challenges, lessons learned, good practices and pain points you need to know when navigating on \u00a0LTR tides, such as:\nPick the right tools to collect data, train and run ML models on your Search Engine\nDistribute and scale training and test into Spark to move faster\nKnow your domain (what is relevance for the business) and the diversity of real user queries\nChallenge the quality of your training set and how you collect judgments after each iteration\nDesign A/B experiments and define metrics to evaluate the quality of your models\n\nImproving ranking is always an ongoing project. Each of previous topics brought, and they are still bringing, us many good learnings. I would like to share them on this talk and bring good practical contributions to new adopters of LTR for ranking on product (enterprise) search.\n", "title": "Learning to Rank journey at GetYourGuide: Our Logbook", "url": "https://2018.berlinbuzzwords.de/18/session/learning-rank-journey-getyourguide-our-logbook.html", "speaker": "Felipe Besson"}, {"level": "Intermediate", "track": "Store", "abstract": "Creating containers for an application is easy (even if it\u2019s a goold old distributed application like Apache Hadoop), just a few steps of packaging.\nThe hard part isn't packaging: it's deploying\nHow can we run the containers together? How to configure them? How do the services in the containers find and talk to each other? How do you deploy and manage clusters with hundred of nodes?\nModern cloud native tools like Kubernetes or Consul/Nomad could help a lot but they could be used in different way.\nIt this presentation I will demonstrate multiple solutions to manage containerized clusters with different cloud-native tools including kubernetes, and docker-swarm/compose.\nNo matter which tools you use, the same questions of service discovery and configuration management arise. This talk will show the key elements needed to make that containerized cluster work.\nTools:\nkubernetes, docker-swam, docker-compose, consul, consul-template, nomad\ntogether with: Hadoop, Yarn, Spark, Kafka, ...\n", "title": "From docker to kubernetes: running Apache Hadoop in a cloud native way", "url": "https://2018.berlinbuzzwords.de/18/session/docker-kubernetes-running-apache-hadoop-cloud-native-way.html", "speaker": "Marton Elek"}, {"level": "Intermediate", "track": "Search", "abstract": "For the majority of cases, the current SolrCloud distributed indexing works great. There is a subset of use cases for which Solr's legacy Master/Slave replication may be a better fit, like cases where NRT is not required, and where read availability is more important than consistency. Solr 7 now ships with three different types of replicas to choose depending on different consistency and availability needs. With a combination of replica types, one can create a SolrCloud cluster that behaves like the Master/Slave architecture from Solr < 4.0 and provides separation of responsibilities (search vs index) while still getting most of the SolrCloud benefits, like high availability of writes, replica discovery, collections API, etc. This talk will be a deep dive into the new Replica Type feature: reasons for implementing it, differences between the types and how/why one would choose to use them, and implementation details of the feature.\n", "title": "New Replica Types in Solr 7", "url": "https://2018.berlinbuzzwords.de/18/session/new-replica-types-solr-7.html", "speaker": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe"}, {"level": "Beginner", "track": "Scale", "abstract": "Banks are data companies. All the services they provide and processes they execute eventually boil down to processing data and making decisions based on the information derived from it. Obviously, technology is what allows them to do that at scale. In contrast to other technology companies though, data at banks tends to be the Achilles heel rather than the biggest asset. To comply with regulation and compliance requirements, they are mandated to handle information properly and provide it to regulators upon request. To stay competitive, they have to innovate to find new sources of revenue or to streamline business processes. These often contradictory forces, combined with legacy IT landscapes, frequently turn out to produce chaotic environments.\nAt SolarisBank, we are fortunate to not have the legacy technology other non-startups struggle with. Hence we have the opportunity to build data infrastructure with a modern tech stack and according to contemporary architectural principles. Nevertheless, regulation and legislation requirements often run counter to design patterns you would normally take for granted. For instance, it might feel like common sense to you to keep large sets of raw, immutable data around indefinitely for future analytics purposes. Your Data Protection Officer though will have a very different opinion on that. This talk is about the engineering aspects of building a data lake in a highly regulated environment. Along the lines of the SolarisBank case, I will discuss the impact of regulatory requirements on system architecture and present design patterns to comply without wreaking havoc on usability.\n", "title": "Building a data lake at a bank", "url": "https://2018.berlinbuzzwords.de/18/session/building-data-lake-bank.html", "speaker": "Dominic Breuker"}, {"level": "Intermediate", "track": "Stream", "abstract": "When working with Apache Flink users, we see many different types of stream processing applications being implemented on top of Apache Flink. Over time, we noticed common patterns and saw how most streaming applications can be reduced to a few core archetypes or \u201capplication blueprints\u201d. In this talk we will outline a few of these \u201cstream processing application blueprints\u201d in a practical, hands-on format, and also take a look at operational considerations.\u00a0\nIn the first half of the talk, we will quickly recap the basics of the modern stream processing space and also take a look at the challenges posed by stateful and event-time aware stream processing.\nIn the second half, we will look at common application blueprints and see how they can be applied in practice. Among the archetypes we will cover:\nAggregating IoT event data, in which event-time aware processing, handling of late data and state are important\nData enrichment, in which a stream of real-time events is \u201cenriched\u201d with data from slowly-changing database of supplemental data points\nDynamic stream processing, in which a stream of control messages and dynamically-updated user logic is used to process a stream of events for use cases such as alerting and fraud detection\n", "title": "Stream Processing for the Practitioner: Blueprints for common stream processing use cases with Apache Flink", "url": "https://2018.berlinbuzzwords.de/18/session/stream-processing-practitioner-blueprints-common-stream-processing-use-cases-apache-flink.html", "speaker": "Aljoscha Krettek"}, {"level": "Intermediate", "track": "Stream", "abstract": "Come learn about the latest changes in Apache Flink 1.5 and how we made stateful stream processing more powerful, more expressive, and more flexible to support applications that were previously difficult to realize.\nApache Flink 1.5 is the biggest Flink release the community has ever published. Not only does it contain many enhancements for its APIs and libraries but it also comes with major improvements to three of its core components. We reworked the network stack and added a credit-based flow control mechanism which results in even better throughput/latency trade-offs and more efficient checkpoints. Fast local recovery helps to speed up the recovery of applications with large state. Finally, the redesign of Flink's distributed architecture makes Flink fit naturally onto Kubernetes, Yarn, Mesos, and standalone setups with support for resource elasticity.\n", "title": "What's new in Stateful Stream Processing with Apache Flink 1.5 and beyond", "url": "https://2018.berlinbuzzwords.de/18/session/whats-new-stateful-stream-processing-apache-flink-15-and-beyond.html", "speaker": "Nico Kruber"}, {"level": "Advanced", "track": "Scale", "abstract": "The challenges to run spark and flink at scale in a kubernetes cluster. The needed multi tenant environment at a larger scale provide additional challenges on top. The jobs profiles have big range in terms of of size and runtime.\nThe talk will show same faced problems on flink, spark and kubernetes side, discussed alternatives and used solutions to that.\nTopics are around\u00a0\ndeployment, tuning\u00a0\nruntime variance, congestion\nshuffle\ncluster resilience / update behaviour\nmonitoring, logs \u00a0\n", "title": "Spark and Flink running scalable in Kubernetes", "url": "https://2018.berlinbuzzwords.de/18/session/spark-and-flink-running-scalable-kubernetes.html", "speaker": "Frank Conrad"}, {"level": "Beginner", "track": "Search", "abstract": "When 1000+ different products and services, with millions of customers and more than 25% of US depend and use your platforms, an intelligent efficient data platform behind is what makes the difference.\nIn this session, I share how we are solving for large scale graph platform with different flavor of use cases and the lessons learnt. Some of the main data use-cases at Microsoft are for advertising, real-time streaming, fraud prediction, analytics, graph and AI. If there is a thing more valuable than data itself, it is the connection between data. Most of what we call \"intelligence\" is based on connections, inference and graph. I will expand on the challenges and learnings with the graph ontology use-case. Come along me for this journey where we built different prototypes with DSE Titan, Spark GraphX, OWL reasoners with Python, Apache Jena. Working through these prototypes required conquering multiple engineering, data science challenges, analytics infrastructure issues, cloud migration, technology whitelisting process/issues on cloud, real-time streaming, measurements and testing. Hopefully this session helps your journey on intelligent large-scale graph processing!\n", "title": "Large Scale Graph Solutions: Use-cases and Lessons Learnt", "url": "https://2018.berlinbuzzwords.de/18/session/large-scale-graph-solutions-use-cases-and-lessons-learnt.html", "speaker": "Rekha Joshi"}, {"level": "Intermediate", "track": "Search", "abstract": "Is search the next industry to be revolutionized by deep learning? Lately researchers have been applying neural networks to search applications, with impressive gains. Search users use different language than what's contained in the corpus. For example, doctors create articles discussing jargon like 'myocardial infarction' but patients search use lay-terms like 'heart attack'. \u00a0Mapping vocabularies using expert created taxonomies or word embeddings (word2vec, LDA, etc) can help. Manual approaches can take a great amount of work or don't map between searcher and document vocabulary. When clear associations between relevant documents and queries can be made, neural search can learn the patterns between query and document language embeddings, with tremendous gains on text search. Such embeddings can also be used to provide alternative representations of the user queries in order to better capture the user intents.\nJoin Doug Turnbull, author of 'Relevant Search', and Tommaso Teofili, author of 'Deep Learning for Search' as we explore this promising frontier. Is it a silver bullet? What are the pros and cons? And how can it fit into your search infrastructure using Solr, Elasticsearch or Lucene?\n", "title": "The Neural Search Frontier", "url": "https://2018.berlinbuzzwords.de/18/session/neural-search-frontier.html", "speaker": "Doug TurnbullTommaso Teofili"}, {"level": "Advanced", "track": "Search", "abstract": "Java 9, that came out on 21 September 2017, brings a lot of new features and speed improvements, but also many challenging problems for deploying Java applications. Apache Lucene/Solr 7, released two days before Java 9, was thoroughly tested with preview builds of the new Java release and was one of the first applications that were ready to be used with Java 9 from the beginning! Why was the adoption of the new Java 9 release so important for Lucene/Solr and Elasticsearch?\nIn this talk, Uwe will present some of the speed improvements that Java 9 (and later) brings to Apache Lucene users, so one should upgrade as soon as possible. He will also present what changed behind the scenes to make Apache Lucene compatible to the Java 9 module system (Jigsaw) and how the testing was done. Finally he will present the plans of using Multi-Release JAR files in current Lucene release, so users can still stay with Java 8, but get significantly improved performance when used with later newer Java versions: Java 10 is already there when berlinbuzzwords start!\n", "title": "Apache Lucene and Java 9+ \u2014 Opportunities and Challenges for Apache Solr and Elasticsearch", "url": "https://2018.berlinbuzzwords.de/18/session/apache-lucene-and-java-9-opportunities-and-challenges-apache-solr-and-elasticsearch.html", "speaker": "Uwe Schindler"}, {"level": "Intermediate", "track": "Stream", "abstract": "SQL is the lingua franca of data processing and everybody working with data knows SQL. Apache Flink provides SQL support for querying and processing batch and streaming data. Flink\u2019s SQL support powers large-scale production systems at Alibaba, Huawei, and Uber. Based on Flink SQL, these companies have built systems for their internal users as well as publicly offered services for paying customers. In my talk, I will discuss why you should and how you can (not being Alibaba or Uber) leverage the simplicity and power of SQL on Flink.\nI will start exploring the use cases that Flink SQL was designed for and present real-world problems that it can solve. In particular, I'll explain why unified batch and stream processing is important and what it means to run SQL queries on streams of data. After discussing why and when you should use Flink SQL, I will show how to leverage its full potential. The Flink community is developing a service that integrates a query interface, (external) table catalogs, and result serving functionality for static, appending, and updating result sets. I will discuss the design and features of this query service and how it will enable exploratory batch and streaming queries, ETL pipelines, and live updating query results that serve applications, such as real-time dashboards. The talk concludes with a brief demo of a client running queries against the service.\n", "title": "Why and how to leverage the power and simplicity of SQL on Apache Flink", "url": "https://2018.berlinbuzzwords.de/18/session/why-and-how-leverage-power-and-simplicity-sql-apache-flink.html", "speaker": "Fabian Hueske"}, {"level": "Beginner", "track": "Stream", "abstract": "Modern businesses have data at their core, and this data is changing continuously. Stream processing is what allows you harness this torrent of information in real-time, and thousands of companies use Apache Kafka as the streaming\u00a0platform\u00a0to transform and reshape their industries. However, the world of stream processing still has a very high barrier to entry. Today\u2019s most popular stream processing technologies require the user to write code in programming languages such as Java or Scala. This hard requirement on coding skills is preventing many companies to unlock the benefits of stream processing to their full effect.\nHowever, imagine that instead of having to write a lot of code, all you\u2019d need to get started with stream processing is a simple\u00a0SQL\u00a0statement, such as:\u00a0SELECT*\u00a0FROM\u00a0payments-kafka-stream\u00a0WHERE\u00a0fraudProbability > 0.8, so that you can\u00a0detect\u00a0anomalies and fraudulent activities in data feeds, monitor application behavior and infrastructure, conduct session-based analysis of user activities, and perform real-time ETL.\nIn this talk, I introduce the audience to KSQL, the open source streaming SQL engine for Apache Kafka.\u00a0 KSQL provides an easy and completely interactive SQL interface for data processing on Kafka -- no need to write any programming code. \u00a0KSQL brings together the worlds of streams and databases by allowing you to work with your data in a stream and in a table format. \u00a0Built on top of Kafka's Streams API, KSQL supports many powerful operations including filtering, transformations, aggregations, joins, windowing, sessionization, and much more. \u00a0It is open source, distributed, scalable, fault-tolerant, and real-time. \u00a0You will learn how KSQL makes it easy to get started with a wide range of stream processing use cases such as those described at the beginning. \u00a0I cover how to get up and running with KSQL and explore the under-the-hood details of how it all works.\n", "title": "Big Data, Fast Data, Easy Data: distributed stream processing for everyone with KSQL, the streaming SQL engine for Apache Kafka", "url": "https://2018.berlinbuzzwords.de/18/session/big-data-fast-data-easy-data-distributed-stream-processing-everyone-ksql-streaming-sql.html", "speaker": "Michael Noll"}, {"level": "Intermediate", "track": "Scale", "abstract": "In this talk\u00a0we make a trip through the world of text recognition with free software and go step by step through the individual sections of a flexible and scalable OCR application. In a live demo you will be shown how Tesseract is used for text recognition and how the quality can be significantly improved doing a little pre-processing with openCV.\u00a0Subsequently\u00a0the documents are\u00a0stored and indexed in Elasticsearch to allow full text search. All this with just a few lines of code and all in the sense of interactive programming with Jupyter.\nAgenda\nQuirks and pitfalls in text recognition of scanned documents\nPotential of pre-processing with openCV\nUse Tesseract at scale\nQuantify,\u00a0compare and revaluate results\nUse of Tensorflow in a production-ready\u00a0application \u00a0\n", "title": "Scalable OCR pipelines using Python, Tensorflow and Tesseract", "url": "https://2018.berlinbuzzwords.de/18/session/scalable-ocr-pipelines-using-python-tensorflow-and-tesseract.html", "speaker": "Mark Keinh\u00f6rster"}, {"level": "Beginner", "track": "Search", "abstract": "Search is a critical piece of infrastructure at just about any based company that deals with data. At large companies, it makes a lot of sense to centralize and build a platform for different teams and customers. Such a platform directly translates to saving development time, avoiding reinventing the wheel, and better maintainability, all of which lead to cost-savings.\nWhile\u00a0it is a complex problem, spanning a challenging landscape, building a search platform gets even more challenging when you try and cater to the diverse set of users that come with it. The varying requirements in terms of feature set, scale, security, etc. are a few complexities that require deep understanding and thought when designing a search platform that would support varying use-cases.\u00a0\u00a0Proactively thinking about the requirements, priorities, and overall design help ensure better availability and maintainability of a search platform.\nHaving built search platforms at both, large and small organizations in the past, I realized that there are more than a few key questions that should be addressed, if not answered when starting to build the platform. Addressing these questions helps starting off the project on the right foot and allow for a platform that is easier to manage and extend in the future. From security to scalability, and beyond, this talk would highlight the questions that I feel should be addressed when starting to build a search platform. At the end, the attendees of the talk would have a much better understanding of the important aspects about designing a search platform.\n", "title": "Building a search platform 101", "url": "https://2018.berlinbuzzwords.de/18/session/building-search-platform-101.html", "speaker": "Anshum Gupta"}, {"level": "Intermediate", "track": "Scale", "abstract": "In her keynote speech Aur\u00e9lie will focus on data subject's rights and the technical consequences of these obligations. As the General Data Protection Regulation (GDPR) will be enforced as of May 25th 2018 and replace the current Data Protection Directive, data subject\u2019s rights are being re-introduced and reinforced, building the ground work for a more balanced approach towards data uses within our increasingly digitized societies. Aur\u00e9lie will dive into the practical perspective of how to manage these new GDPR data subject rights, what it means for your processes and IT systems and when or to what type of data they apply.", "title": "Technical Consequences of the Data Subject's Rights at KINO", "url": "https://2018.berlinbuzzwords.de/18/session/technical-consequences-data-subjects-rights-kino.html", "speaker": "Aur\u00e9lie Pols"}, {"level": "Beginner", "track": "Search", "abstract": "Search engines are databases that specialize in retrieving information from a data corpus. Compared to traditional databases like PostgreSQL, search engines allow to work with text and other unstructured data more efficiently. Projects like Xapian and Lucene can perform efficient indexing and querying of large amounts of documents. Solr and Elasticsearch have added clustering and distributed query execution to scale out the search features.\nThe most obvious gap between traditional databases and search engines is the query language. Whereas relational databases can typically be queried with SQL, search engines usually implement a custom API.\nAt CrateDB, we don\u2019t think you should have to give up SQL just because you\u2019re using search engine features. That\u2019s why we created a fully-functional SQL interface on top of Elasticsearch and Lucene. You get all the benefits of traditional databases, as well as the features of a distributed search engine.\nDo you want to store huge amounts of data and search it in real time? Do you have unstructured and structured data? Do you want to run distributed joins? Do you want to add nodes and scale your cluster horizontally? Do you want to leverage the power of SQL? If so, CrateDB is a great match.\nIn this talk, I will give an introduction to CrateDB, its architecture, and show what people have built with it.\n", "title": "CrateDB: A Search Engine or a Database? Both!", "url": "https://2018.berlinbuzzwords.de/18/session/cratedb-search-engine-or-database-both.html", "speaker": "Maximilian Michels"}, {"level": "Intermediate", "track": "Scale", "abstract": "Blockchain has been a buzzword and there is a lot of hype around it, there is no doubt. But what are people doing to actually make it happen? We will explain what are the technical details that make blockchain technology unique. Let's get into enterprise grade blockchain applications and discuss various concepts in the space. A deep dive into Hyperledger frameworks and tools will be provided, followed by some interesting study of real world applications. We will finish with a demo of how blockchain can help improve the world we live in today.\nThe Keynote will be held at Kino.", "title": "Blockchain in Wildlife - Use Cases in the Field at KINO", "url": "https://2018.berlinbuzzwords.de/18/session/blockchain-wildlife-use-cases-field-kino.html", "speaker": "Marta Piekarska"}, {"level": "Beginner", "track": "Search", "abstract": "At Home24, Europe\u2019s largest online furniture retailer, we face the challenge of calculating good recommendations for products where we lack enough user behavior data. Although we get fairly good results with recommendations based on product metadata, for some products the metadata is just not granular enough to produce good results. In particular, \"material: wood, color: brown\" matches a wide variety of products that don\u2019t actually fit together from a human perspective.\nTo make up for this deficit, we found a way to augment our algorithms with product images - analysing the colors in each image to recommend products that match well visually.\n", "title": "Calculating recommendations based on product images", "url": "https://2018.berlinbuzzwords.de/18/session/calculating-recommendations-based-product-images.html", "speaker": "Vlad Dolezal"}, {"level": "Intermediate", "track": "Stream", "abstract": "The need to integrate a swarm of systems has always been present in the history of IT, however with the advent of big data and the internet of things this has simply exploded. Through the explanation of several real life use cases in companies of all sizes, this talk will introduce you to Apache NiFi, a powerful and scalable system to process, transform and distribute data.\nNiFi is an open source project from the Apache Foundation that works perfectly as mediation logic between systems and to perform most of your ETL requirements. This talk will show you how NiFi can be used by humans in BI, Data Science, Development and Operations teams to easily fulfill your data move requirements.\nAfter this talk you will know where you can leverage NiFi, but also where you should not use it, in a nutshell you will add another tool in your belt to work on data integration problems.\n", "title": "Connecting the data infrastructure with the DataFlow", "url": "https://2018.berlinbuzzwords.de/18/session/connecting-data-infrastructure-dataflow.html", "speaker": "Pere Urbon-Bayes"}, {"level": "Intermediate", "track": "Search", "abstract": "After the initial release in 2010 Elasticsearch\u00a0has become the most widely used full-text search engine, but it is not stopping there. The revolution happened and now it is time for evolution. We dive into the following questions:\nHow did leniency help the initial adoption, but why and how do we lean more on strictness today?\nHow can upgrades be improved to avoid any downtime even when changing major versions?\nHow can new resiliency features improve recovery scenarios and add totally new features?\nWhy are types finally disappearing and how are we are trying to avoid the upgrade pain as much as possible?\nWhat are examples for some clever performance improvements?\nHow can you shrink and (finally) split shards in a highly efficient way?\n\nAttendees learn both about new and upcoming features as well as the motivation and engineering challenges behind them.\n", "title": "Elasticsearch (R)Evolution \u2014 You Know, for Search...", "url": "https://2018.berlinbuzzwords.de/18/session/elasticsearch-revolution-you-know-search.html", "speaker": "Philipp Krenn"}]