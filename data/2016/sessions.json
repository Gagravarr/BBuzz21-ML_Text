[{"level": "Beginner", "track": "Scale", "abstract": "\nIn this session, you will get your hands on stream processing with Apache Flink, an open source platform for distributed stream and batch processing.\nWe will introduce Flink's DataStream API and help you to setup a local development environment to implement and test scalable stream processing programs.\nHands-on coding exercises will make this a fun session to attend.\nPlease note: This is a hands-on session. You should be familiar with Java or Scala and bring a laptop.\nWe recommend to prepare the Flink setup by following these instructions (http://dataartisans.github.io/flink-training/devSetup/handsOn.html) in advance to save some time.", "title": "Getting started with Stream Processing on Apache Flink", "url": "https://2016.berlinbuzzwords.de/session/getting-started-stream-processing-apache-flink.html", "speaker": "Maximilian MichelsVasia KalavriFabian Hueske"}, {"level": "Intermediate", "track": "Search", "abstract": "The modern day search engine has significantly evolved from its keyword matching days to its current form which leverages a wide variety of data inputs and user feedback loops to help users find out what\u2019s most important in their data.\u00a0 At Lucidworks, we leverage Apache Spark and Solr and a good dose of data engineering to build smarter, richer search and data applications.\u00a0 The combination of these two key technologies plus some supporting libraries create a powerful combination for data scientists and engineers to not only explore, but enhance data.\u00a0 In this hands-on session, we\u2019ll cover some common use cases, a bit of background on how they work (theoretical and practical) and mix in a good dose of examples for attendees to try out on their own.", "title": "Data Engineering with Apache Solr and Spark", "url": "https://2016.berlinbuzzwords.de/session/data-engineering-apache-solr-and-spark.html", "speaker": "Grant Ingersoll"}, {"level": "Intermediate", "track": "Scale", "abstract": "A/B tests, or randomized controlled experiments, have been widely applied in different industries to optimize the business process and the user experience. Here we'll introduce a Python library, ExpAn, intended for the statistical analysis of A/B tests.\nThe input data to ExpAn has a standard format, which is defined to interface with different data sources. The main statistical functions in ExpAn are all standalone and work with either the library-specific input data structure or some Python built-in data types. Among others, the functions can be used to assess whether the randomization is appropriate, and measure the expectation and error margin of the uplift due to the treatment. We also implemented a robust discretization algorithm to handle typical heavy-tailed distributions in the real world. Finally, a generic result structure is designed to incorporate results from different types of analyses.\nOne can easily feed data from other domain-specific data fetching modules into ExpAn. Other advanced algorithms for the analysis of A/B test data can be implemented and plugged into ExpAn, eg. a Bayesian hypothesis testing scheme instead of the frequentist approach. The generality of the result structure also makes it handy to apply different kinds of visualization on top of the data.\n\nThis is a talk sponsored by Zalando.", "title": "ExpAn - A Python library for advanced statistical analysis of A/B tests", "url": "https://2016.berlinbuzzwords.de/session/expan-python-library-advanced-statistical-analysis-ab-tests.html", "speaker": "Dominic Heger"}, {"level": "Intermediate", "track": "Search", "abstract": "We observed that marketing teams of major companies tend to build their marketing campaigns based on study of client segmentation and market analysis which can be an heavy workload that directly deteriorate its ROI\nUsing our client Big Data (6 000 000 client informations on a Hadoop / Spark Cluster) we tried an other approach to dramaticaly increase the agility for building and tayloring new marketing campaigns.\nWe used Test&Learn methodolgies to increase the \"uplift\" of the campaigns inspired by the multi-armed bandit problem (https://en.wikipedia.org/wiki/Multi-armed_bandit) and implemented the whole approach in Apache Spark.\nIn this talk we want to share our feedbacks on the approach the success we had and probably more importantly the pitfalls we encountered.", "title": "Project use case : Improve your marketing reach using large scale machine learning on Spark", "url": "https://2016.berlinbuzzwords.de/session/project-use-case-improve-your-marketing-reach-using-large-scale-machine-learning-spark.html", "speaker": "Nina BertrandMatthieu Vautrot"}, {"level": "Intermediate", "track": "Search", "abstract": "\nApache Solr is a powerful search and analytics engine with features such as full-text search, faceting, joins, sorting and capable of handling large amounts of data across a large number of servers. However, with all that power and scalability comes complexity. Solr 6 supports a Parallel SQL feature which provides a simplified, well-known interface to your data in Solr, performs key operations such as sorts and shuffling inside Solr for massive speedups, provides best-practices based query optimization and by leveraging the scalability of SolrCloud and a clever implementation, allows you to throw massive amounts of computation power behind analytical queries.\nIn this talk, we will explore the why, what and how of Parallel SQL and its building block Streaming Expressions in Solr 6 with a hint of the exciting new developments around this feature.\nThis is a talk sponsored by LucidWorks.\n\u00a0", "title": "Parallel SQL and Streaming Expressions in Apache Solr 6", "url": "https://2016.berlinbuzzwords.de/session/parallel-sql-and-streaming-expressions-apache-solr-6.html", "speaker": "Shalin Shekhar Mangar"}, {"level": "Beginner", "track": "Scale", "abstract": "Hubot, GitHub's open source chat bot, is completely revolutionizing how we build, ship and operate software at scale. As a widely distributed company, we rely on online chat as one of our primary communication tools. ChatOps helps us improve situational awareness during incidents, to share knowledge, to ship software, and much more. We'll look at some concrete examples and talk about the cultural implications embracing ChatOps has had at GitHub.", "title": "Shipping at Scale: ChatOps at GitHub", "url": "https://2016.berlinbuzzwords.de/session/shipping-scale-chatops-github.html", "speaker": "Georgi Knox"}, {"level": "Intermediate", "track": "Scale", "abstract": "Streaming is the latest hot topic in the big data world. We want to process data immediately and continuously. Modern stream processors have matured significantly and offer exceptional features, including sub-second latencies, high throughput, fault-tolerance, and seamless integration with various data sources and sinks.\nMany sources of streaming data consist of related or connected events: user interactions in a social network, web page clicks, movie ratings, product purchases. These connected events can be naturally represented as edges in an evolving graph.\n\nIn this talk I will explain how we can leverage a powerful stream processor, such as Apache Flink, and academic research of the past two decades, to build graph streaming applications. I will describe how we can model graphs as streams and how we can compute graph properties without storing and managing the graph state. I will introduce useful graph summary data structures and show how they allow us to build graph algorithms in the streaming model, such as connected components, bipartiteness detection, and distance estimation.", "title": "Graphs as Streams: Rethinking Graph Processing in the Streaming Era", "url": "https://2016.berlinbuzzwords.de/session/graphs-streams-rethinking-graph-processing-streaming-era.html", "speaker": "Vasia Kalavri"}, {"level": "Beginner", "track": "Keynote (at Kino)", "abstract": "For a long time, a substantial portion of data processing that companies did ran as big batch jobs -- CSV files dumped out of databases, log files collected at the end of the day etc. But businesses operate in real-time and the software they run is catching up. Rather than processing data only at the end of the day, why not react to it continuously as the data arrives? This is the emerging world of stream processing. But stream processing only becomes possible when the fundamental data capture is done in a streaming fashion; after all, you can\u2019t process a daily batch of CSV dumps as a stream. This shift towards stream processing has driven the popularity of Apache Kafka. Making all the organization's data is available centrally as free-flowing streams enables a company's business logic to be represented as stream processing operations. Essentially, applications are stream processors in this new world of stream processing.\nIn her keynote, she will explain how the fundamental nature of application development will change as stream processing goes mainstream.\nSlides: https://speakerdeck.com/nehanarkhede/applications-in-the-emerging-world-...", "title": "Application development and data in the emerging world of stream processing", "url": "https://2016.berlinbuzzwords.de/session/application-development-and-data-emerging-world-stream-processing.html", "speaker": "Neha Narkhede"}, {"level": "Beginner", "track": "Keynote (at Kino)", "abstract": "In her keynote \u201cInspiring the Next Generation to Run Away and Join Our Software Circus\u201d \u00a0she describes how we can all create magical experiences that inspire and connect under-represented communities to new technologies. Producing sufficient numbers of graduates who are prepared for science, technology, engineering, and mathematics (STEM) occupations has become a national priority around the globe. We all have a responsibility to onboard, attract and enroll new entrants to STEM if you care about the future of open innovation. In this keynote, she will discuss how to combine connected learning principles to create and deliver new technology workshops to inspire and encourage under-represented and under-served communities.\nhttp://de.slideshare.net/xbrlspy1/keynote-berlin-buzzwords-2016-inspirin...", "title": "Inspiring the Next Generation to Run Away and Join Our Software Circus", "url": "https://2016.berlinbuzzwords.de/session/inspiring-next-generation-run-away-and-join-our-software-circus.html", "speaker": "Diane Mueller-Klingspor"}, {"level": "Beginner", "track": "Search", "abstract": "Many companies tout themselves as \u201copen source,\u201d offering support for popular tools and technologies. But professional services alone don\u2019t typically produce the kind of revenue required for long term business success. So what does it take to build a profitable and healthy open source company? In this session, Will Hayes, CEO of Lucidworks \u2014 the company built on and around Apache Lucene and Solr \u2014 will share how business success need not come at the expense of the open source community\u2019s integrity. Using Lucidworks as a case study Hayes will explain the power of specificity in driving both revenue and committer happiness, exploring why open source companies should avoid commercializing product features with broad, cross-industry appeal, such as security, management and monitoring. These are fundamental features and monetizing them will harm the integrity of the open source community.\nHayes will also explore the strategy of going to market with product features that serve a specific industry or use case \u2014 for example, with an enterprise search product, ongoing fraud detection for financial services customers \u2014 as this will produce business value and prevent community tension.\n\u00a0\n", "title": "Community & Commercialization: How to Build an Open Source Company in 2016", "url": "https://2016.berlinbuzzwords.de/session/community-commercialization-how-build-open-source-company-2016.html", "speaker": "Will Hayes"}, {"level": "Intermediate", "track": "Scale", "abstract": "In 2004 Google published the MapReduce paper, a programming model that kick-started big data as we know it. Ten years later, Google introduced Dataflow - a new paradigm for big data, integrating batch and stream processing in one common abstraction. This time the offer was more than a paper, but also an open source Java SDK and a cloud managed service to run it. In 2016 big data players like Cask, Cloudera, Data Artisans, PayPal, Slack, Talend joined Google to propose Dataflow for incubation at the Apache Software Foundation - now accepted as Apache Beam. Dataflow is here, not only unifying batch and streaming, but also the big data world.\nIn this talk we are going to review Dataflow's differentiating elements and why they matter. \u00a0We\u2019ll demonstrate Dataflow\u2019s capabilities through a real-time demo with practical insights on how to manage and visualize streaming data flows.\n", "title": "Google Dataflow: The new open model for batch and stream processing", "url": "https://2016.berlinbuzzwords.de/session/google-dataflow-new-open-model-batch-and-stream-processing.html", "speaker": "Felipe Hoffa"}, {"level": "Advanced", "track": "Store", "abstract": "Over the years, we have built many data-analytics systems using components like Apache ZooKeeper, etcd, Consul, or homebrewed implementations of Raft. These components are used in a number of systems to perform what we call distributed coordination: master election, group membership, configuration metadata, locks, barriers, etc. Like many other systems we have seen abused, these components are often used in scenarios where they are convenient, but not strictly necessary. This observation begs the question of where it is necessary to use a distributed consensus primitive.\nTo understand when it is necessary to rely on a consensus primitive, we need to step back and understand precisely what such a consensus primitive provides and its association to all the problems that we have been using it for. There are many fundamental results in the academic literature that can be used here to explain the need to use a consensus primitive: the relationship between state-machine replication and atomic broadcast, the equivalence between atomic broadcast and consensus, and the equivalence between consensus and leader election. Even further, there is the famous Herlihy consensus hierarchy showing the strength of asynchronous shared memory primitives based on their equivalence to consensus. This hierarchy shows that some useful primitives (e.g., distributed registers) do not need consensus, showing that for many problems we come across when building distributed systems, it is possible to rely on weaker, possibly simpler solutions. Some other primitives, like compare-and-swap, are equivalent to consensus.\nThe goal of this presentation is to revisit the distributed consensus problem in the light of all these fundamental results, but keeping the discussion away from pure theory. We discuss such primitives and results in a very practical way, using our experience with the Apache ZooKeeper project, and argue that it might be possible to reduce the reliance on such consensus primitives, but consensus is certainly not going away because it is a fundamental problem in distributed computing.\n", "title": "Towards consensus on distributed consensus", "url": "https://2016.berlinbuzzwords.de/session/towards-consensus-distributed-consensus.html", "speaker": "Flavio Junqueira"}, {"level": "Intermediate", "track": "Search", "abstract": "At the beginning of the year 2016, the Apache Lucene team decided to focus on releasing Apache Lucene 6. Around Berlin Buzzwords, the new version will be available for testing.\nThis talk will present the new features that Lucene 6 will bring: A new data type called \"points\" (also known as dimensional values)\u00a0and corresponding queries as faster, multidimensional replacement for NumericRangeQuery. It will also\u00a0completely remove the concept of \"filters\" in favour of\u00a0non-scoring queries. Last but not least, Lucene and Solr 6 will require\u00a0Java 8 as minimum requirement.\nThis talk will also present the many new features in Apache Solr 6: It now understands an SQL dialect to do queries and aggregations. In combination with a brand new JDBC driver, one can query Solr using standard database tools.\n", "title": "Apache Lucene 6: What's coming next?", "url": "https://2016.berlinbuzzwords.de/session/apache-lucene-6-whats-coming-next.html", "speaker": "Uwe Schindler"}, {"level": "Intermediate", "track": "Scale", "abstract": "In the past few years Apache Kafka has established itself as the world's most popular real-time, large-scale messaging system. It is used across a wide range of industries by thousands of companies such as Netflix, Cisco, PayPal, Twitter, and many others.\nIn this session I am introducing the audience to Kafka Streams, which is the latest addition to the Apache Kafka project.\u00a0 Kafka Streams is a stream processing library natively integrated with Kafka. It has a very low barrier to entry, easy operationalization, and a high-level DSL for writing stream processing applications. As such it is the most convenient yet scalable option to process and analyze data that is backed by Kafka.\u00a0 We will provide the audience with an overview of Kafka Streams including its design and API, typical use cases, code examples, and an outlook of its upcoming roadmap.\u00a0 We will also compare Kafka Streams' light-weight library approach with heavier, framework-based tools such as Apache Storm and Spark Streaming, which require you to understand and operate a whole different infrastructure for processing real-time data in Kafka.\n", "title": "Introducing Kafka Streams, the new stream processing library of Apache Kafka", "url": "https://2016.berlinbuzzwords.de/session/introducing-kafka-streams-new-stream-processing-library-apache-kafka.html", "speaker": "Michael Noll"}, {"level": "Intermediate", "track": "Scale", "abstract": "This talk is the story of the transformation of a legacy architecture into a robust multi-region streaming pipeline which relies on Apache Kafka as a core component in order to transport and access terabytes of logs and billions of messages every day. I\u2019ll present the pitfalls and challenges we faced during our journey and how, with proper design, they were overcome in order to meet the expectations of our engineers, who are the final users of this system.\n", "title": "Scaling Yelp\u2019s Logging Pipeline with Apache Kafka", "url": "https://2016.berlinbuzzwords.de/session/scaling-yelps-logging-pipeline-apache-kafka.html", "speaker": "Enrico Canzonieri"}, {"level": "Intermediate", "track": "Store", "abstract": "The Wikimedia Foundation is a non-profit and charitable organization driven by a vision of a world where every human can freely share in the sum of all knowledge.\u00a0 Each month Wikimedia sites serve over 18 billion page views to 500 million unique visitors around the world.\nAmong the many resources offered by Wikimedia is a public-facing API that provides low-latency, programmatic access to full-history content and meta-data, in a variety of formats.\u00a0 Commonly, results from this system are the product of computationally intensive transformations, and must be pre-generated and persisted to meet latency expectations.\u00a0 Unsurprisingly, there are numerous challenges to providing low-latency storage of such a massive data-set, in a demanding, globally distributed environment.\nIn this talk, we will cover the Wikimedia content API, and it's use of Apache Cassandra, a massively-scalable distributed database, as storage for a diverse and growing set of use-cases.\u00a0 Trials, tribulations, and triumphs, of both a development and operational nature will be discussed.\n", "title": "Wikimedia Content API: A Cassandra Use-case", "url": "https://2016.berlinbuzzwords.de/session/wikimedia-content-api-cassandra-use-case.html", "speaker": "Eric Evans"}, {"level": "Beginner", "track": "Scale", "abstract": "Blockchains are usually assimilated to bitcoins, but in reality they are much more than being the Bitcoin Network backbone.\n\u00a0On one hand, the \"Blockchain\" can be seen as a distributed, peer to peer log like system, on top of which distributed applications can be built with security, encryption and accountability.\nAnd on the other hand, we are \u00a0transitioning to an age of smart devices, with a\u00a0need to communicate in a Machine to Machine fashion, with security, and scale that make a centralized architecture not viable in the long term.\nIn this session, we intend to introduce the blockchain as a scalable and secure solution, on top of which critical IoT applications can be built, by using examples from\u00a0the Ethereum Project to\u00a0implement\u00a0smart contracts, and embed\u00a0more intelligence into smart devices without relying on a centralized system.\u00a0\n", "title": "Leveraging blockchain technologies for the internet of things.", "url": "https://2016.berlinbuzzwords.de/session/leveraging-blockchain-technologies-internet-things.html", "speaker": "Sam BESSALAH"}, {"level": "Intermediate", "track": "Scale", "abstract": "Complex event processing (CEP) and stream analytics are commonly treated as distinct classes of stream processing applications. While CEP workloads identify patterns from event streams in near real-time, stream analytics queries ingest and aggregate high-volume streams. Both types of use cases have very different requirements which resulted in diverging system designs. CEP systems excel at low-latency processing whereas engines for stream analytics achieve high throughput usually due to distributed scale-out architectures.\nRecent advances in open source stream processing yielded systems that can process several millions of events per second at sub-second latency. Systems like Apache Flink enable applications that include typical CEP features as well as heavy aggregations. An example of these use cases is an application that ingests network monitoring events, identifies access patterns such as intrusion attempts using CEP technology, and analyzes and aggregates identified access patterns.\nIn this talk we will show how Apache Flink unifies CEP and stream analytics workloads. Guided by examples, we introduce Flink\u2019s CEP-enriched StreamSQL interface and discuss how queries are compiled, optimized, and executed on Flink.\n", "title": "Streaming analytics and CEP - Two sides of the same coin", "url": "https://2016.berlinbuzzwords.de/session/streaming-analytics-and-cep-two-sides-same-coin.html", "speaker": "Till RohrmannFabian Hueske"}, {"level": "Intermediate", "track": "Store", "abstract": "Modern cars produce data. Lots of data. And Formula 1 cars produce more than their share.\nI will present a working demonstration of how modern data streaming can be applied to the data acquisition and analysis problem posed by modern motorsports.\nInstead of bringing multiple Formula 1 cars to the talk, I will show how we instrumented a\u00a0high fidelity\u00a0physics-based automotive simulator to produce realistic data from simulated cars running on the Spa-Francorchamps track. We move\u00a0data from the cars, to the pits, to the engineers back at HQ.\nThe result is near real-time visualization and comparison of performance and a great exposition of how to move data using messaging systems like Kafka. The code from this talk will be made available as open source.\n\u00a0\n", "title": "Fast Cars, Big Data - How Streaming Can Help Formula 1", "url": "https://2016.berlinbuzzwords.de/session/fast-cars-big-data-how-streaming-can-help-formula-1.html", "speaker": "Ted Dunning"}, {"level": "Advanced", "track": "Scale", "abstract": "Today's industrial systems produce more and more data everyday. Companies are increasingly using Big Data technologies and data analysis approaches in order to monitor their systems.\nI will illustrate this trend with a project of predictive maintenance on trains. In cities where millions of people use public transportation everyday, avoiding faults on trains during circulations is critical. The goal of the project is to predict faults on trains in advance so that can be are dispatched to the workshops accordingly, thus avoiding train delays and reducing maintenance costs.\nI'll explain our approach which uses random forests and artificial neural networks with theano, what are the results and how we measure success, and finally how, from developing models on historical data with python, we progressively moved to production and transposed the models to a distributed environment using Spark.\n", "title": "Predictive maintenance: from POC to production with Spark", "url": "https://2016.berlinbuzzwords.de/session/predictive-maintenance-poc-production-spark.html", "speaker": "Heloise Nonne"}, {"level": "Beginner", "track": "Scale", "abstract": "Data science doesn\u2019t always work the way you think, but there\u2019s enormous power to be unleashed by analyzing data when you do it right. The important interplay between technologies for gathering /\u00a0analyzing data and the human element to decide what actions to take based on that data is a big factor in the outcome. Data driven decisions not only influence individual projects and business success, they also can have an enormous impact even at the level of changing society.\nGoing beyond the details of tools and techniques, this talk connects the dots to give you a big picture of what makes some data science projects successful while others founder.\u00a0 The talk includes stories from a variety of data projects. I\u2019ll also provide practical tips for going from idea to innovation and how to get beyond the barriers that exist in real world settings.\u00a0 We will take a look at specific approaches that lead to success, including:\nAre you asking the right questions?\nDo you recognize a good idea when you have it?\nCan you engage interest in your work in order to see it have an impact?\n\nThe talk should be useful for audiences at all levels of experience.\n\u00a0\n\u00a0\n", "title": "The Surprising Course of Knowledge and Innovation", "url": "https://2016.berlinbuzzwords.de/session/surprising-course-knowledge-and-innovation.html", "speaker": "Ellen Friedman"}, {"level": "Advanced", "track": "Search", "abstract": "Learning to Rank (LTR), once the domain of academic researchers in machine learning and information retrieval, has begun to make great headway in practical applications on the web. Its tools and techniques offer a new way to think about challenges in relevance ranking, personalization, localization, ad targeting and multimedia search, but it shares enough conceptual foundations with traditional search relevance that it\u2019s easy for hands-on engineers to get started with.\nIn this talk, Andrew will introduce the field and its key concepts, before diving into one of its best-known algorithms, Ranking SVM, which adapts Support Vector Machines to ranking tasks instead of classification problems.\nWith real examples taken from work at Etsy and elsewhere, he\u2019ll talk about how you can use this algorithm and others like it to incorporate a huge variety of features into your search ranking model: query-specific term weights, implicit user feedback, temporal and geographic data, and even image features. And he\u2019ll touch on applications of LTR beyond traditional search, including ad click prediction and content-based recommendations.\nNo past experience of machine learning will be required.\u00a0Attendees can expect to leave this talk with an understanding of LTR and its applications, and enough insight into Ranking SVM to enable them to experiment with ranking models on their own data.\n\u00a0\n", "title": "Learning to Rank: where search meets machine learning", "url": "https://2016.berlinbuzzwords.de/session/learning-rank-where-search-meets-machine-learning.html", "speaker": "Andrew Clegg"}, {"level": "Intermediate", "track": "Store", "abstract": "Real time insights into multi-dimensional data is a key asset for data-driven businesses. We present the architecture of our fast and reliable streaming-only data processing pipeline which harnesses the qualities of Kafka, Flink and Druid. This trio turns out to be a very good choice for building real-time online analytics systems.\nIn recent years Apache Kafka has become the de-facto standard for highly available and highly scalable messaging.\nApache Flink allows us to consume, process and produce data with minimum delay. When using a streaming-only approach the challenge is to guarantee the correctness of your data. Flink\u2019s capability of using different sources (in our case Kafka for real-time and HDFS for historical data) easily lets us reprocess data without any need of maintaining multiple code bases often needed in Lambda Architectures.\nDruid is a datastore designed for real-time multidimensional analytics and overcomes weaknesses of alternative approaches like RDBs and Key-Value stores. It\u2019s streaming ingestion plays extremely well with Flink which is able to process every event as it arrives. We have already contributed our Flink sink to the Druid project so that you can use it out-of-the-box.\n", "title": "Real-time analytics with Flink and Druid", "url": "https://2016.berlinbuzzwords.de/session/real-time-analytics-flink-and-druid.html", "speaker": "Jan Gra\u00dfegger"}, {"level": "Intermediate", "track": "Scale", "abstract": "So you\u2019ve created some machine learning algorithms and tested them out in the lab, and they seem to be working fine. But how can you monitor them in production, especially if they are\u00a0constantly learning and updating, and you have many of them? This was the challenge we faced at Anodot and I\u2019ll talk about the interesting\u00a0way we solved it.\u00a0\nAt Anodot, we have\u00a0approximately 30 different types of machine learning algorithms, each one\u00a0with its own parameters and tuning capabilities, designed to provide real time anomaly\u00a0detection. Many of these are online learning algorithms, which get updated with every new piece of data that arrives.\u00a0Adding to the complexity, the outputs of some of the algorithms act as the inputs\u00a0others.\u00a0These algorithms run constantly on the vast number of signals that are sent to our SaaS cloud\u00a0(currently more than 35 million signals are reported to Anodot every 1 to 5 minutes).\u00a0We knew from day one that it was crucial to track the performance of these algorithms, so we\u00a0would know if something happened that improved or degraded their performance, but we were\u00a0faced with a challenge \u2013 how to accomplish this? \u00a0\nFirst, we collect time series metrics that constantly measure various performance indicators for\u00a0each of the algorithms. We measure the number of anomalies we discover for each customer,\u00a0their score distribution, the number of seasonal patterns discovered, classification changes and\u00a0rates between of the various model selection\u00a0algorithms, number of clusters and their quality from our various\u00a0clustering algorithms, and much more.\nBut, manual tracking of changes in these algorithm performance metrics is not feasible - there are too many models/algorithms to track manually (even with dashboards/reports). So, we discovered that by\u00a0applying our anomaly detection algorithms to track these metrics, we quickly discover any change in their behaviour and see their\u00a0abnormal patterns. When we get alerts on these abnormal changes,\u00a0we \u00a0determine if it was because of some algorithm\u00a0tuning, or perhaps it is a valid shift.\u00a0\nIn the talk I'll show multiple examples of how this approach helped us detect, fix and eventually\u00a0design better learning algorithms. I'll also describe the general methodology\u00a0for \"learning the learner\".\n", "title": "Learning the learner: Using machine learning to\u00a0 track performance of machine learning algorithms", "url": "https://2016.berlinbuzzwords.de/session/learning-learner-using-machine-learning-track-performance-machine-learning-algorithms.html", "speaker": "Ira Cohen"}, {"level": "Intermediate", "track": "Search", "abstract": "We\u2019ve learned how to run Docker containers. And we\u2019ve learned how to run Elasticsearch. However, to run containerized Elasticsearch nodes and to do that effectively -- and at scale -- takes a little more knowledge and work. Sure, containers can be easily started and stopped; but how do you do that with Elasticsearch inside them?\nIn this talk we\u2019ll quickly run over the basic Docker+Elasticsearch setup and focus on harder problems:\nArchitecting for Elasticsearch fault tolerance and high availability in containerized setup - using sharding, replication, node and shard-awareness for keeping your cluster green\nRunning Elasticsearch in different modes with re-usability in mind\nOptimizing and tuning Elasticsearch for popular use cases like ELK\nOps/Devops - monitoring Elasticsearch & Docker together - which metrics to watch, what they mean, how to act on them and first of all, how to watch them\n\nTakeaways:\nRunning Elasticsearch on Docker\nPerformance implications of running Elasticsearch on Docker\nHow to create highly available and fault tolerant clusters\nProper Elasticsearch setup in a container\nTuning Elasticsearch\nWhich metrics should be looked at when dealing with Elasticsearch\nMonitoring containers and applications running inside of them\n", "title": "Running High Performance And Fault Tolerant Elasticsearch Clusters On Docker", "url": "https://2016.berlinbuzzwords.de/session/running-high-performance-and-fault-tolerant-elasticsearch-clusters-docker.html", "speaker": "Rafa\u0142 Ku\u0107"}, {"level": "Beginner", "track": "Store", "abstract": "There are lots of claims about the benefits of NoSQL databases, but few realistic demonstrations of the impact that such a database can have on anything more than toy-sized data.\u00a0\nIn this talk, I will deconstruct a real-world database schema into the corresponding NoSQL design. The database that I will use is the Musicbrainz database, which exhibits many important idioms found in real databases, such as factoring relations into multiple tables to implement column families, linkage tables, and many-to-one relationships. In spite of such radical structural changes, the resulting denormalized and nested data can still be queried with SQL using Apache Drill, and the queries are often noticeably simpler than the queries used against the original data structures. The methods are practical and easy to apply, and can sometimes be largely automated. For example, I'll show how a percolator pattern can be used to allow the resulting NoSQL database to be automatically maintained in multiple NoSQL technologies simultaneously, so that full text search, recommendations, and the HBase API can all be used to access the same data.\n", "title": "Real-World NoSQL Schema Design", "url": "https://2016.berlinbuzzwords.de/session/real-world-nosql-schema-design.html", "speaker": "Tugdual Grall"}, {"level": "Advanced", "track": "Scale", "abstract": "A case study on how we grew the AudienceScience Helios programmatic advertising management system to > 1 Million transactions per second through our entire data pipeline including Kafka, Storm and Hadoop.\nThe AudienceScience Helios system acts as:\nA real time bidding client to all major exchanges (> 70 integrations world wide) A service for direct integration with publisher\u2019s website / ad server A engine data collection and management of billions of users The system runs world wide across 6 global data centers.\nWe\u2019ll share what we learned along our journey and how we are moving to even greater scale in the Cloud\u00a0for 2016\nGrowing The infrastructure \u00a0\nWe will share lessons learned growing Kafka worldwide and moving 120TB per day across our global network. How we grew our hadoop cluster from 16 nodes then to a 400 node cluster and then 550 node cluster with zero downtime \u00a0\nWe will outline our lessons learned about scaling Hadoop focusing on data design and coding as as well as hardware, monitoring, job management. We will show how we process over 6 billion messages a day in storm with 120K TPS peak on a 60 node cluster.\nMoving to The Cloud for even greater scale\nNext we can speak to our movement to the AWS Cloud to gain the freedom of operational budgets and instant gratification vs. capital expenditure and physical buildouts. Why we moved from Storm to Spark streaming as a common platform to move from a batch driven model to true Lambda architecture.\u00a0How/why we are moving Hadoop batch processes on fixed hardware to Spark managed by Quobole on-demand workflows in AWS to gain performance efficiencies and directly drive $ savings.\u00a0 Lastly we can show how we are move from simple Graphite/Carbon tools to C* for collecting Time Series monitoring Data, and then leveraging this same architecture to process real time bidding feedback from pricing optimization and signal analysis in Spark as part of the overall Lambda architecture.\n", "title": "The world wide 60 billion transaction per day journey", "url": "https://2016.berlinbuzzwords.de/session/world-wide-60-billion-transaction-day-journey.html", "speaker": "Frank ConradRanbir Chawla"}, {"level": "Intermediate", "track": "Store", "abstract": "Hadoop makes it relatively easy to store petabytes of data. However, storing data is not enough; columnar layouts for storage and in-memory execution allow the analysis of large amounts of data very quickly and efficiently. It provides the ability for multiple applications to share a common data representation and perform operations at full CPU throughput using SIMD and Vectorization. For interoperability, row based encodings (CSV, Thrift, Avro) combined with general purpose compression algorithms (GZip, LZO, Snappy) are common but inefficient. As discussed extensively in the database literature, a columnar layout with statistics and sorting provides vertical and horizontal partitioning, thus keeping IO to a minimum. Additionally a number of key big data technologies have or will soon have in-memory columnar capabilities. This includes Kudu, Ibis and Drill. Sharing a common in-memory columnar representation allows interoperability without the usual cost of serialization.\nUnderstanding modern CPU architecture is critical to maximizing processing throughput. We\u2019ll discuss the advantages of columnar layouts in Parquet and Arrow for in-memory processing and data encodings used for storage (dictionary, bit-packing, prefix coding). We\u2019ll dissect and explain the design choices that enable us to achieve all three goals of interoperability, space and query efficiency. In addition, we\u2019ll provide an overview of what\u2019s coming in Parquet and Arrow in the next year.\n", "title": "Efficient Data formats for Analytics with Parquet and Arrow", "url": "https://2016.berlinbuzzwords.de/session/efficient-data-formats-analytics-parquet-and-arrow.html", "speaker": "Julien Le Dem"}, {"level": "Intermediate", "track": "Search", "abstract": "Lucene will change the default scoring from TF/IDF to BM25 in the next major release. So unless you really enjoy surprises you better learn about it now! TF/IDF was easy enough to understand intuitively but how is it with BM25? What do all these parameters do? And what do people mean when they say it is \"probabilistic\"? In this talk I will tell the story of how we came from the Probability Ranking Principle to BM25 with a minimum of math and a maximum of explaining. I will also show how BM25 differs from TF/IDF, what it means in practice and give and intuition on what the parameters of this method actually do. You will leave this talk feeling good about Lucene changing the default. And of course you will learn many fancy buzzwords to show off with during the breaks.\n", "title": "BM25 demystified", "url": "https://2016.berlinbuzzwords.de/session/bm25-demystified.html", "speaker": "Britta Weber"}, {"level": "Beginner", "track": "Store", "abstract": "The attack on Sony led to lots of talk about how they could have defended themselves.\n\nIs your household any better? Have you defence in depth? Is all that protects you a ISP's base station hasn't had an upgrade for four years? How often do you upgrade Adobe Flash on your TV?   And what is it you really need to worry about in a world of internet-enabled-everything?\n\nI examine the threat model and attack vectors, going beyond \"keep your PC up to date\" to the new problems: smart TVs, fitness trackers, smartphone apps \u2014even the INFOSEC issues of modern cars. In the process: whether you are publishing so much private data that worrying about laptop security is moot. Finally, it considers whether your github credentials are strategic data to nation states.\n\nAfter this talk you'll not only want an OSS router, you'll be packet sniffing your TV, encrypting your sensitive data off-site, and, the next time you buy a car, asking to download its ABS and near-miss event history.", "title": "Household INFOSEC in a Post-Sony era", "url": "https://2016.berlinbuzzwords.de/session/household-infosec-post-sony-era.html", "speaker": "Steve Loughran"}, {"level": "Intermediate", "track": "Scale", "abstract": "Data Streaming is emerging as a new and increasingly popular architectural pattern for the data infrastructure. Data streaming architectures embrace the fact that data in practice never has the form of static data sets, but is continuously produced as streams of events over time. Moving away from centralized \u201cstate of the world\u201d databases and warehouses, the applications work directly on the streams of events and on application-specific local state that is an aggregate of the history of events.\nAmong the many disruptive promises of streaming architectures are\ndecreased latency from signal to decision\na unified way of handling real-time and historic data processing\ntime travel queries\nsimple versioning of applications and their state (think git update/rollback)\nsimplification of data processing stack.\n\nThis talk introduces the data streaming architecture paradigm, and shows how to build an exemplary set of simple but representative applications using the open source systems Apache Flink and Apache Kafka. Delivered by the creators of the Apache Flink framework, the talk explains the building blocks of data streaming applications, including\nevent stream logs\ntransformations and windows\nworking with time\napplication state and consistency\n\n\u00a0\n", "title": "A Data Streaming Architecture with Apache Flink", "url": "https://2016.berlinbuzzwords.de/session/data-streaming-architecture-apache-flink.html", "speaker": "Robert Metzger"}, {"level": "Intermediate", "track": "Scale", "abstract": "We present a new design pattern for data streaming applications, using Apache Flink and Apache Kafka: Building applications directly on top of the stream processor, rather than on top of key/value databases populated by data streams.\nUnlike classical setups that use stream processors or libraries to pre-process/aggregate events and update a database with the results, this setup simply gives the role of the database to the stream processor (here Apache Flink), routing queries to its workers who directly answer them from their internal state computed over the log of events (Apache Kafka).\nThis architecture pattern has many interesting implications:\n\u00a0 - All state updates happen locally in the stream processor. This eliminates the need for (distributed) transactions with an external database, leading to very high performance (we saw cases with >100x speedup compared to pushing the state into a database).\n\u00a0 - Consistency (exactly-once) is maintained across the entire pipeline: log, stream processor, and application state. That stands in contrast to setups today, where duplicate changes may be pushed to the database, because the database is typically not integrated with the stream processor's consistency mechanism.\n\u00a0 - Consistency and persistence are realized through distributed snapshots. Changes between snapshots are replayed from the input streams or Kafka. When snapshots run asynchronously, the overhead of these snapshots is very low\n\u00a0 - Because application state is part of a streaming program, one can naturally upgrade/rollback the program and state together, replay the stream with a modified program (playing through what-if scenarios) or fork off a new variant of the state (A/B testing)\n\u00a0 - This architecture eases the handling of out of order streams and late-arriving events. The stream processor can expose early results in addition to the correct results.\n\u00a0 This talk will cover both the high-level introduction to the architecture, the techniques in Flink/Kafka that make this approach possible, as well as experiences from a large scale setup and technical details.\n", "title": "The Stream Processor as a Database: Building Online Applications directly on Streams with Apache Flink and Apache Kafka", "url": "https://2016.berlinbuzzwords.de/session/stream-processor-database-building-online-applications-directly-streams-apache-flink-and.html", "speaker": "Stephan Ewen"}, {"level": "Beginner", "track": "Scale", "abstract": "The combination of Apache Kafka\u00a0as a event bus, Apache Storm\u00a0for real- or neartime processing, Apache Cassandra\u00a0as an operational storage layer as well as Apache Spark\u00a0to perform analytical queries against this storage turned out to be a extremely well performing system.\nWith increasing marketing costs per registration, it is even more important to keep players within the game as well as provide them with attractive offers aiming to increase the customer lifetime value and also create a better game experience.\nTo that end, we introduced interstitials that offer premium features or discounts for the player at InnoGames. Even though this is already a useful instrument, we aimed to customize those interstitials according to the behavior of the player. Therefore, we created a system that works with generic messages that contain data about user interactions, in real- or neartime -- later referred to as events. The system builds up a player profile that contains all game-relevant information about the players in a central location.\nThe system is also able to react to events, fetch information about the corresponding player from the profile in a matter of milliseconds and send out an interstitial based on this information.\nThe system consists of an Apache Kafka\u00a0component used as an event bus, an Apache Storm\u00a0topology to update the profile and trigger marketing actions based on events as well as an Apache Cassandra\u00a0cluster which serves as a storage layer for the profile. In addition, we set up a Apache Spark\u00a0cluster along Cassandra to run analytical queries against the data in Cassandra based on this article from DataStax (https://academy.datastax.com/demos/getting-started-apache-spark-and-cassandra). This combination is quite efficient but additionally we added a Spark REST JobServer\u00a0(https://github.com/spark-jobserver/spark-jobserver) and extended it so that we can read the player profile from Cassandra, cache it within the Spark context and reuse this context for several jobs. This increases the performance for analytical queries significantly.\nThe whole stack combines the possibility of fast event-based operations along with powerful analytical queries using Spark DataFrames. This concept can therefore not only be applied to a marketing system like the one we built, but also to a variety of different use cases. Another key technology used within the system is the Nashorn JavaScript engine included with Java 8. It is used within Storm to work with the player profile and the incoming events. In this way we are able to define new marketing action at runtime and have a very flexible and generic data processing bolt within our Storm topology.\n", "title": "Real Time Marketing with Kafka, Storm, Cassandra and a pinch of Spark", "url": "https://2016.berlinbuzzwords.de/session/real-time-marketing-kafka-storm-cassandra-and-pinch-spark.html", "speaker": "Volker Janz"}, {"level": "Intermediate", "track": "Search", "abstract": "With 100M+ products in a single Solr index it is hard work to keep response times as low as possible. A key factor is decreasing the index size and the number of terms indexed. We try to store the significant terms only. For long product descriptions this is challenging. Applying NLP at index time is a costly operation. In this talk I\u2019ll present a smart algorithm that run fast enough to be applied at index time.\n", "title": "Gain speed and space with NLP in Solr", "url": "https://2016.berlinbuzzwords.de/session/gain-speed-and-space-nlp-solr.html", "speaker": "Tobias K\u00e4ssmann"}, {"level": "Intermediate", "track": "Store", "abstract": "Over the past several years, the Hadoop ecosystem has made great strides in its real-time access capabilities, narrowing the gap compared to traditional database technologies. With systems such as Impala and Spark, analysts can now run complex queries or jobs over large datasets within a matter of seconds. With systems such as Apache HBase and Apache Phoenix, applications can achieve millisecond-scale random access to arbitrarily-sized datasets.\nDespite these advances, some important gaps remain that prevent many applications from transitioning to Hadoop-based architectures. Users are often caught between a rock and a hard place: columnar formats such as Apache Parquet offer extremely fast scan rates for analytics, but little to no ability for real-time modification or row-by-row indexed access. Online systems such as HBase offer very fast random access, but scan rates that are too slow for large scale data warehousing workloads.\nThis talk will investigate the trade-offs between real-time transactional access and fast analytic performance from the perspective of storage engine internals. It will\u00a0also describe Apache Kudu (incubating), a new addition to the open source Hadoop ecosystem that fills the gap described above, complementing HDFS and HBase to provide a new option to achieve fast scans and fast random access from a single API. \u00a0\n", "title": "Apache Kudu (incubating): Fast Analytics on Fast Data", "url": "https://2016.berlinbuzzwords.de/session/apache-kudu-incubating-fast-analytics-fast-data.html", "speaker": "Todd Lipcon"}, {"level": "Advanced", "track": "Store", "abstract": "Yet another key-value store? No, it's an index not a store and it is based on 'Finite State technology', but what does that mean?\nKeyvi - the short form for \"Key value index\" - defines a special subtype of the popular key value store (KVS) technologies. As you can imagine from the name, keyvi is an immutable key value store, therefore an index not a store. Keyvi's strengths: high compression ratio and extreme scalability.\nKeyvi powers Cliqz Websearch engine, replacing former engines based on Redis and Elastic Search. Serving terrabytes of data at scale and low-latency, keyvi is already proven technology while still being a young OSS project.\nBut keyvi is also different to well-established NoSQL engines, it is not an efficient implementation of well-known, common used data structures like hash tables and B-Trees. It brings finite state not only to the same level, but efficiently allows approximate, completion and graph matching to boldy go where NoSql hasn't gone before.\nThis session will introduce keyvi, show it's strengths, capabilities and finally look deep into the heart of it's data structures.\n\u00a0\nLink to presentation: https://cdn.rawgit.com/cliqz-oss/keyvi/master/doc/presentations/bbuzz2016/keyvi-presentation.svg#1_0 \u00a0\n", "title": "keyvi the key value index optimized for size and speed", "url": "https://2016.berlinbuzzwords.de/session/keyvi-key-value-index-optimized-size-and-speed.html", "speaker": "Hendrik Muhs"}, {"level": "Intermediate", "track": "Scale", "abstract": "Migrating a full data stack completely (http://www.slideshare.net/soobrosa/6w-bp-datashow) relying on AWS services (and a lots of them) to Microsoft Azure does not seem to be an enlightening and funny task. How we managed to do it with open sourcing tools as roadkills and what we learnt about the barebone necessities ending up within the sole body of a Raspberry Pi may shock the absolute believers of distributed computing. An adventure in bash, make and SQL with a detour in Moore's law and falling memory prices.\n\u00a0\n", "title": "Migrating a data stack from AWS to Azure (via Raspberry Pi)", "url": "https://2016.berlinbuzzwords.de/session/migrating-data-stack-aws-azure-raspberry-pi.html", "speaker": "Daniel Molnar"}, {"level": "Intermediate", "track": "Scale", "abstract": "Event Sourcing brings the\u00a0promise of highly-scalable and loosely-coupled systems that are performant, reliable, and\u00a0maintainable. It looked like a perfect solution for Yammer's reliability and performance challenges,\u00a0but nothing comes for free!\nOnly slightly over a year ago,\u00a0Yammer's entire system was either based on synchronous calls or\u00a0Ruby\u00a0workers and\u00a0RabbitMQ. For a while, we have been moving performance-critical components out of the Ruby on Rails monolith toward\u00a0Dropwizard-based services. This has served us well, but with increased reliability\u00a0requirements,\u00a0the\u00a0pressure to\u00a0simplify and\u00a0decouple\u00a0our system's architecture\u00a0also\u00a0increased.\nOver the course of the last year, we first created a prototype implementation of\u00a0Event\u00a0Sourcing and, after validating the idea, have been moving\u00a0it\u00a0to a\u00a0managed\u00a0Azure Event Hubs based solution.\u00a0\nWe are going to cover\u00a0not only\u00a0how to\u00a0migrate to a new technology,\u00a0but\u00a0also look\u00a0at\u00a0how to change\u00a0organizational thinking from a synchronous world\u00a0to one\u00a0of event streams. We\u00a0will\u00a0tell the war stories\u00a0of our migration from a self-hosted Kafka cluster to a\u00a0solution\u00a0based\u00a0in the cloud.\u00a0\n", "title": "Event Sourcing in Yammer", "url": "https://2016.berlinbuzzwords.de/session/event-sourcing-yammer.html", "speaker": "Michal RutkowskiDmitry StratiychukPhilipp Fehre"}, {"level": "Advanced", "track": "Scale", "abstract": "With Docker Inc. buying Unikernel Systems and sparking renewed interest in unikernel technology, 2016 is shaping to be an undeniably year of unikernels. OSv is the revolutionary new open source technology that combines the power of virtualization and micro-services architecture. This combination allows unmodified applications deployed in a virtualized environment to outperform bare-metal deployments. Yes. You've heard it right: for the first time ever we can stop asking the question of how much performance would I lose if I virtualize. OSv lets you ask a different question: how much would my application gain in performance if I virtualize it. This talk will start by looking into the architecture of OSv and the kind of optimizations it makes possible for native, unmodified applications. We will then focus on JVM-specific optimizations and specifically on speedups available to big data and NoSQL projects when they are deployed on OSv. We will also cover a companion project Seastar that is now being used as a base for a hight performant replacement for Apache Cassandra, beating the Apache implementation on most of the benchmarks.", "title": "OSv: Probably the best OS for Cloud workloads you've never heard of", "url": "https://2016.berlinbuzzwords.de/session/osv-probably-best-os-cloud-workloads-youve-never-heard.html", "speaker": "Roman Shaposhnik"}, {"level": "Intermediate", "track": "Scale", "abstract": "Causality is an essential component of how we make sense of the physical world,\u00a0and of our relations to other humans. If I put a cup on the table, and look\u00a0back at it, I expect it to be there. I also expect to get a reply to my\u00a0postcards, after I send them, and not before. \u00a0\nThese days hardly any service can claim not to have some form of distributed\u00a0algorithm at its core. In a distributed scenario, if we are not careful, it is\u00a0very easy to break the causal sense of things. In a key-value store my writes\u00a0can be directed to a replica, and my subsequent reads served from an outdated\u00a0one --- my cup might not be there when I look back. Message dissemination\u00a0middleware might not always provide the ordering I expect --- I might receive\u00a0some replies, before their leading questions.\u00a0\nLuckily, most of these problems were already there 30 years ago, although in a\u00a0much smaller scale, and lots of techniques have been developed to keep track of\u00a0causality and make sense of the complex interactions in modern systems. However\u00a0developers often look at techniques such as as replica synchronization with\u00a0version vectors, or causal broadcasting algorithms, as black boxes; or as\u00a0complex sets of rules that have to be followed and not questioned.\u00a0\nThis talk will focus on bringing back the intuition on causality, and show that\u00a0keeping in mind some simple concepts, allows to understand how version vectors\u00a0and vector clocks work, and were they differ, and how to use more sophisticated\u00a0mechanisms to handle millions of concurrent clients in modern distributed data\u00a0stores.\u00a0\n", "title": "Causality is simple", "url": "https://2016.berlinbuzzwords.de/session/causality-simple.html", "speaker": "Carlos Baquero"}, {"level": "Beginner", "track": "Scale", "abstract": "\"Business intelligence is the part of the company which is affected the most when we a company under goes microservice transformation\".\u00a0\n\nBol.com is the largest online retailer in the Netherlands and Belgium and is still growing at a staggering rate.\u00a0\n\nBol.com is a fact based decision making company hence business intelligence plays a\u00a0key role in the organization.\u00a0For gathering business intelligence (BI) insights we have a team of twelve dedicated BI engineers. However,\u00a0it is not just the scale of data, but also the complexity of these services that can makes\u00a0it difficult to gather all the business insights.\u00a0The growth of data, users and complexity have forced our team of to rethink our traditional relational data warehouse structure. Currently we are moving towards a more hybrid BI solution based on Microservices at big data scale, using Hadoop, Hbase, relational databases, etc.\u00a0\n\nIn this presentation, we will discuss key concepts that govern our datawarehousing,\u00a0the unique challenges we have faced and are still facing with petabytes of data along with design decisions, tooling landscape and architectural choices. We have created and explored various toolings and concepts for solving our problems.\u00a0\n\nWe discuss concepts pertaining to Data quality at big data scale, large scale batch scheduling, job monitoring, ETL processing, reporting, continuous deployment, shift in mindset of traditional datawarehouse developers\u00a0and team autonomy for BI at scale. Essentially, all the hacks to get our systems towards awesomeness. The presentation will guide audience through transformation of legacy (vintage) BI towards more scalable BI setup as well as mental and techincal block that came along the way. We believe it is important to educate impact of microservices on\u00a0datawarehouse and we have learned how we could do that at petabyte scale.\u00a0", "title": "Business Intelligence (at scale) in Microservice architecture", "url": "https://2016.berlinbuzzwords.de/session/business-intelligence-scale-microservice-architecture.html", "speaker": "Debarshi Basak"}, {"level": "Intermediate", "track": "Scale", "abstract": "At bol.com (the biggest online retailer in the Benelux) we want to help the customer find what they wanted. To automate this process we\u00a0need to understand what products/promotions we showed them and which of those made them happy. With the fine grained personalization that has been introduced over the last few years we see that just measuring \u2018what page\u2019 we showed (like all the standard web analytics systems do) is no longer enough. So we need something different. In order to get a solution that will support our business for the coming years we raised the bar to the top: Measure everything and analyze in near-realtime.\n<!--break-->\nIn this talk Niels Basjes will explain the project \u201cMeasuring 2.0\u201d, our next generation web analytics measuring and processing stack, that is to go live in the spring of 2016. Niels will go into\nthe custom built measuring system that will produce over 50000 measurements per second\nthe processing system and the algorithms\u00a0implemented with Apache Flink\nwhy we did not choose\u00a0Storm or Spark for this task\nthe development and operational hurdles needed to make this type of solution run in production\nthe architectural concepts to make this data available in the personalization services we have\n\n\u00a0\n", "title": "Measuring 2.0 \u2013 Building the next generation webanalytics solution using Apache Flink.", "url": "https://2016.berlinbuzzwords.de/session/measuring-20-building-next-generation-webanalytics-solution-using-apache-flink.html", "speaker": "Niels Basjes"}, {"level": "Intermediate", "track": "Scale", "abstract": "Writing software for the Internet puts engineers face to face with traffic that makes doing even simple things difficult. Seemingly straightforward operations such as counting, set membership and set cardinality become either extremely slow or prohibitively expensive to do using traditional methods.\nHelp is at hand, however: probabilistic techniques are both fascinating and extremely useful. By sacrificing a predictable amount of accuracy, we can perform operations at scales we never thought possible, and fast! We'll introduce approaches such as Bloom filters for set membership, count-min sketch for frequency in streams, and HyperLogLog for cardinality. No maths PhD is required.\nWe'll look at the before and after effects of using these techniques in real-world scenarios, and present the libraries that you can go away and play with right now.\u00a0 \u00a0\n", "title": "Acceptably inaccurate: probabilistic data structures", "url": "https://2016.berlinbuzzwords.de/session/acceptably-inaccurate-probabilistic-data-structures.html", "speaker": "James Stanier"}, {"level": "Beginner", "track": "Search", "abstract": "Keyword search: check. \u00a0\nFaceting: check.\nHighlighting: check.\nIf you think that\u2019s all there is to search, think again. \u00a0Search is everywhere and it\u2019s way more powerful than it used to be. \u00a0Stop building search for 10 years ago and get focused on delivering next generation insight to your users by leveraging next generation tools (NLP, machine learning, smarter data ingestion) and ideas to make your search better. \u00a0This light hearted talk, engine agnostic talk will examine common mistakes made in building search systems as well as tips and techniques for best practices in search, all in the context of the video games we all know and love.\n", "title": "Level Up: Everything you need to know about search, you learned playing video games", "url": "https://2016.berlinbuzzwords.de/session/level-everything-you-need-know-about-search-you-learned-playing-video-games.html", "speaker": "Grant Ingersoll"}, {"level": "Intermediate", "track": "Scale", "abstract": "New execution platforms may be popping up all the time with the intention of being the \"hot new thing\" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. Apache Crunch is a simple framework on top of MapReduce (with support for running on Spark as well) which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.\n", "title": "Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark.", "url": "https://2016.berlinbuzzwords.de/session/big-data-small-code-using-java-8-and-apache-crunch-quickly-develop-concise-efficient.html", "speaker": "David Whiting"}, {"level": "Advanced", "track": "Scale", "abstract": "Today if a byte of data were a gallon of water, in only 10 seconds there would be enough data to fill an average home, in 2020 it will only take 2 seconds. With this explosive growth comes the demand from consumers and businesses to leverage and act on what is happening right now. Without stream processing these demands will never be met, and there will be no big data and no Internet of Things. It is only a matter of time before you will be faced with building a real-time streaming pipeline. As soon as you embark on this journey, you will be faced with a myriad of questions. A major key decision you will need to quickly answer is which stream-processing framework should you use? When you survey the landscape you will find many contenders. In this session we will focus on the most popular open source frameworks, in particular: Apache Spark Streaming, Apache Storm, Apache Flink, and Apache Samza. We will dive into each of these tools and tease out all of the essential pieces you need to consider, compare and contrast them and end up with an understanding of how to evaluate each as well as future products.\u00a0\n", "title": "Help I need a stream processor - learning how to chose between Spark, Flink, Samza, and Storm", "url": "https://2016.berlinbuzzwords.de/session/help-i-need-stream-processor-learning-how-chose-between-spark-flink-samza-and-storm.html", "speaker": "Andrew Psaltis"}, {"level": "Intermediate", "track": "Scale", "abstract": "We're coming together for\u00a0Berlin Buzzwords' 7th edition and over the course of the years\u00a0a lot has changed in the Big Data Technology\u00a0ecosystem. Once-hot buzzwords have vanished and new buzzwords arose.\nWhile you would probably have written a MapReduce job in Java to crawl the web and analyze it on a massive scale this has now become much simpler with tools like Spark and Flink at hand.\nI want to do a live coding session where I show that today it\u00a0is possible to write a scalable web crawler and analytics tool which scrapes the past 6 years of Berlin\u00a0Buzzwords (websites) and shows some interesting insights in the Big Data trends of the past 6 years. While I will run the tool on the very limited data set of the historical Berlin Buzzwords websites I want to highlight that it would in principle scale to crawl\u00a0millions of websites and analyze petabytes of data.\n", "title": "Live-Hack: Analyzing 7 years of Buzzwords at Scale", "url": "https://2016.berlinbuzzwords.de/session/live-hack-analyzing-7-years-buzzwords-scale.html", "speaker": "Christoph Tavan"}, {"level": "Intermediate", "track": "Scale", "abstract": "Distributed Systems are a complex topic. There's abundant research about it but sometimes it is hard for a beginner to know where to start. I would like to outline the main concepts of distributed systems, so the interested person can have a clear path on how to start their own research as well.\nIn this talk I will review the different models: asynchronous vs. synchronous distributed systems; message passing vs shared memory communication; failure detectors and leader election problems; consensus and different kinds of replication.\nI will also review a series of books on distributed systems in order to recommend the best one according to the topics we would like to learn about, or the problems we would like to solve.\nThe goal of the talk is to set a good foundation for people interested in learning more about distributed systems.\n", "title": "What We Talk About When We Talk About Distributed Systems", "url": "https://2016.berlinbuzzwords.de/session/what-we-talk-about-when-we-talk-about-distributed-systems.html", "speaker": "Alvaro Videla"}, {"level": "Intermediate", "track": "Scale", "abstract": "Following the news from the big data world, you might get the impression that Java MapReduce is way past its prime and newer frameworks such as Spark or Flink are the way to go, no questions asked.\nHaving run big data workloads in a production environment on some of the largest web portals in Germany since 2007, I will argue that in many use cases, from an implementation or performance standpoint the actual choice of parallelization framework does not matter as much as you might think. Understanding your application domain, implementing a sound domain model, and optimizing your data flows based on that domain knowledge will be much more effective when trying to improve performance or write more elegant code than switching to the latest distributed computing framework.\nIn this session, I will discuss use-cases from our production systems and show how business logic is implemented that is efficient, yet agnostic of the actual computing framework. I will demonstrate how it interacts with MapReduce, how other frameworks such as Spark would work in its stead, and how domain-specific optimizations can work hand in hand with the computing framework of choice.\n", "title": "MapReduce is not dead, it just smells funny!", "url": "https://2016.berlinbuzzwords.de/session/mapreduce-not-dead-it-just-smells-funny.html", "speaker": "Christoph Schmitz"}, {"level": "Intermediate", "track": "Scale", "abstract": "The buzz around containers has not yet reached it climax. Rolling out big scale heterogeneous applications has just started. This talk is about real customer projects, which use docker to deploy applications in a rapidly environment changing by kubernetes and CoreOS.\n\nWe followed a very strict approach, separating persistent and stateless applications and run everything in small units orchestrated by Kubernetes. With our own tools we could create descriptions of environments very rapidly, creating full complex environments with a single command. Examples in Java, Python and Ruby are shown. Security has been addressed to pass an extensive security audit.\n\nUsing the latest feature of systemd on Linux on CoreOS, we could move the configuration management into the kubernetes domain.", "title": "Shaping Applications for Docker, CoreOS, Kubernetes and Co", "url": "https://2016.berlinbuzzwords.de/session/shaping-applications-docker-coreos-kubernetes-and-co.html", "speaker": "Thomas Fricke"}, {"level": "Advanced", "track": "Search", "abstract": "What challenges could a search engine have? Large number of documents? Large query load? Very complex queries? A challenging privileging model? Expected low query latency? High volume of document updates? Updates to documents reflected in milliseconds? Realtime alerting for any search? Absolutely no downtime any time of the day, week or year? What if a search engine had all these challenges? Meet the backend which drives News Search at Bloomberg LP. In this talk, Ramkumar Aiyengar talks about how he and his colleagues successfully pushed Solr over the last three years to unchartered territories, to deliver a real-time search engine critical to the workflow of hundreds of thousands of customers worldwide.\n", "title": "Building a real-time news search engine", "url": "https://2016.berlinbuzzwords.de/session/building-real-time-news-search-engine.html", "speaker": "Ramkumar Aiyengar"}, {"level": "Intermediate", "track": "Scale", "abstract": "A talk covering the best-of-breed platform consisting of\u00a0Spark, Mesos, Akka, Cassandra and Kafka. SMACK is more of a toolbox of technologies to allow the building of resilient ingestion pipelines, offering a high degree of freedom in the selection of analysis and query possibilities and baked-in support for flow-control. More and more customers are using this stack, which is rapidly becoming the new industry standard for Big Data solutions. German Version can be found here:\u00a0https://speakerdeck.com/stefan79/fast-data-smack-down \u00a0\n", "title": "SMACK Stack - Data done Right", "url": "https://2016.berlinbuzzwords.de/session/smack-stack-data-done-right.html", "speaker": "Stefan Siprell"}]