[{"level": "Beginner", "track": "Search", "abstract": "So your boss wants you to setup a multisources/multiformats search engine, and he thinks it requires 5 min to do so ? Well, the best we can do, is to show you what Datafari - an open source enterprise search solution - can (and cannot do) in 5 min, and then you will have more food to build a realistic scenario for your boss.", "title": "5 min to discover an open source enterprise search solution", "url": "https://2021.berlinbuzzwords.de/session/5-min-discover-open-source-enterprise-search-solution", "speaker": "C\u00e9dric Ulmer"}, {"level": "Beginner", "track": "Scale", "abstract": "Covid-19 put fuel to rising trends and accelerated changes that would have otherwise taken longer to take place. From remote work, artificial intelligence, and faster shifts to the cloud - businesses were forced to adapt in a tumultuous year. These are having lasting implications for the future of work.\nWhat is this new thing being referred to as \u201cCloud Engineering\u201d and why is that relevant to the future of work? Jobs requesting AI/ML skills are expected to grow 71% over the next 5 years and is considered one of the most disruptive skill areas of tech (ITCareerfinder/Burning Glass). \"since the pandemic struck, investment in AI platforms has skyrocketed, shifting from a 'nice-to-have' initiative to a full-blown business imperative.\u201c - CIO magazine.\nHow is AI both good and bad for society? What are the conversations happening on the Future of Work and what will this mean?", "title": "The Acceleration of Cloud Engineering and AI - The Future of Work is Now", "url": "https://2021.berlinbuzzwords.de/session/acceleration-cloud-engineering-and-ai-future-work-now", "speaker": "Grace Francisco"}, {"level": "Intermediate", "track": "Scale", "abstract": "As machine learning becomes more pervasive across industries the need to automate the deployment of the required infrastructure becomes even more important. With data velocity increasing every day it becomes more and more important to keep models fresh.\u00a0Combined with the ever growing popularity of Kubernetes, a full-cycle, containerized method for maintaining model freshness is needed.\nIn this talk we will present a containerized architecture to handle the\u00a0lifecycle of an ML model. We will describe our technologies and tools used along with our lessons learned along the way. We will show how fresh training data can be ingested, models can be trained, evaluated, and served in an automated and extensible fashion.\nAttendees of this talk will come away with a working knowledge of how a machine learning pipeline can be constructed and managed inside Kubernetes.\u00a0All code presented will be available on GitHub.", "title": "Applied MLOps to Maintain Model Freshness on Kubernetes", "url": "https://2021.berlinbuzzwords.de/session/applied-mlops-maintain-model-freshness-kubernetes", "speaker": "Jeff Zemerick"}, {"level": "Advanced", "track": "Search", "abstract": "How do shoppers pick a single product out of the vast number presented to them? One part of their decision making is to compare between the different products by weighing the pros and cons of their features for a given price: for shops with a huge inventory, this can no doubt be a challenging task.\nExplicit comparison features (i.e. \u201cclick on product X and Y to see them side-by-side\u201d) are a classical way of easing the shopping cognitive load, and recently eCommerce giants started incorporating this concept into a new type of recommendation. However, scaling this approach to huge inventories and a variety of verticals is a daunting task for traditional retailers: explicit comparisons are limited to manual 1:1 interfaces, and detailed comparison tables require a lot of manual work and often presuppose a well structured product catalogue.\nIn this talk, we present our pipeline to generate comparisons-as-recs at scale in a multi-tenant setting, with minimal assumptions about catalog size and web traffic. Our approach leverages both product meta-data (image, text) and behavioural data, and a combination of neural inference and decision-making principles. In particular, we show how to break down the problem into two main steps. First, for a given product we use dense representations to perform substitute identification, which determines a group of alternative products of the same category. Then, based on how their features and price vary, we select the final set of products and determine which features to display for comparison. Compared to existing, single-tenant literature, our experiments highlight the need for further improvements in dealing with noisy data and the adoption of data augmentation techniques: we conclude by sharing some practical tips for practitioners, and highlighting our testing and product roadmap.", "title": "\"Are You Sure?\": blending product comparisons and recommendations with A.I.", "url": "https://2021.berlinbuzzwords.de/session/are-you-sure-blending-product-comparisons-and-recommendations-ai", "speaker": "Patrick John Chia"}, {"level": "All", "track": "Search", "abstract": "Get to know about vector search and ask Dmitry Kan anything you need to know!\nThis session is presented by Haystack The search relevance conference.", "title": "Ask Me Anything: Vector Search!", "url": "https://2021.berlinbuzzwords.de/session/ask-me-anything-vector-search", "speaker": "Dmitry Kan"}, {"level": "Intermediate", "track": "Stream", "abstract": "Streaming applications often face changing resource needs over their lifetime: there might be workload differences during day- and nighttime, or business-related events that cause load spikes. Being able to automatically adapt to these changes is a common requirement for production deployments. Apache Flink has supported stateful job rescaling since the early days, but so far this had to be done by stopping and restarting jobs manually. In the latest release (1.13), the Flink community introduced a much anticipated feature: autoscaling. Now, you can add machines to your cluster for triggering an automatic scale up, or remove machines for letting it scale down again!\nIn this talk, we'll explore different deployment scenarios for streaming applications and explain how Flink users can benefit from autoscaling to enable new use cases, streamline day-to-day operations and avoid unnecessary costs. In addition, we\u2019ll briefly describe how this feature was implemented and how it enables further resource elasticity improvements in the future, such as scaling controlled by Flink.", "title": "Autoscaling Apache Flink Applications", "url": "https://2021.berlinbuzzwords.de/session/autoscaling-apache-flink-applications", "speaker": "Robert Metzger"}, {"level": "All", "track": "Scale", "abstract": "Join our awesome barcamp online and discuss hot topics and trends with peers. This is a great opportunity to acquire some new skills and get a fresh perspective! Everybody can take part, everybody can speak: bring a problem or a talk, and meet attendees and speakers. Hosted by the amazing Nick Burch!", "title": "Barcamp", "url": "https://2021.berlinbuzzwords.de/session/barcamp-0", "speaker": "Nick Burch"}, {"level": "Beginner", "track": "Scale", "abstract": "Open source crossed the chasm into mainstream with users in all industries. Maintaining the users\u2019 trust and sustaining innovation is key to open source\u2019s success. However, in a world where communities are passionate, multicultural, and primarily use online communication, it is challenging to move communities towards a shared vision in a frictionless, sustainable way.\nCommunity challenges can impact innovation, putting user adoption at risk and even more importantly, hurting community members. Stronger open source leadership can address these challenges and there is a call for more leaders in every project.\nGood news! Every contributor is a leader either through self leadership, leading others, or leading the community, yet most people have never been trained on how to lead. This talk provides the leadership training you need and covers: - What is leadership and why strengthen open source leadership - Key leadership and emotional intelligence principles - Practical ways to lead in order to create a diverse, thriving community especially in today\u2019s extreme ambiguity and change.", "title": "Be The Leader We All Need", "url": "https://2021.berlinbuzzwords.de/session/be-leader-we-all-need", "speaker": "Megan Byrd-Sanicki"}, {"level": "Beginner", "track": "Stream", "abstract": "For the most flexible, powerful stream processing engines, it seems like the barrier to entry has never been higher than it is now. If you\u2019ve tried, or have been interested in leveraging the strengths of real-time data processing - maybe for machine learning, IoT, anomaly detection or data analysis - but you\u2019ve been held back: I\u2019ve been there, and it\u2019s frustrating. And that\u2019s why this talk is for you.\u00a0\nThat being said, this talk is also for you if you ARE experienced with stream processing but you want an easy (and if I say so myself, pretty fun) way to add some of the newest, bleeding edge features to your toolbelt.\nThis session will be about getting started with Flink SQL. Apache Flink\u2019s high level SQL language has the familiarity of the SQL you know and love (or at least, know\u2026), but with some powerful new functionality, and of course, the benefit of being able to be used with Flink and PyFlink.\u00a0\nMore specifically, this will be a pragmatic entry into creating data pipelines with Flink SQL, as well as a sneak peek into some of its newest and most interesting features.", "title": "Better, Faster, Stronger Streaming: Your First Dive into Flink SQL", "url": "https://2021.berlinbuzzwords.de/session/better-faster-stronger-streaming-your-first-dive-flink-sql", "speaker": "Caito Scherr"}, {"level": "Intermediate", "track": "Search", "abstract": "It is proven that for relatively well-structured data, like in e-commerce for example, a hand tailored search configuration can easily outperform machine learning approaches for relevance. The search configuration considers the different searchable fields, a business taxonomy and ontology, some domain related synonyms, a few specific landing pages, boosts and some business numerical criteria.\nIn the same way, we describe an approach for relevance in the case of large-scale search engines which is not based on classical \"PageRank\" and machine learning approaches. We propose a model based on social interactions between communities and individuals that are using or configuring the search engine. We then compare this model with machine learning powered approaches.", "title": "Beyond Artificial Intelligence for Search", "url": "https://2021.berlinbuzzwords.de/session/beyond-artificial-intelligence-search", "speaker": "Lucian Precup"}, {"level": "Beginner", "track": "Scale", "abstract": "In this workshop, you will go on a journey from raw data, historical financial transactions, that we engineer into features using Apache Spark, to model training with TensorFlow, to model serving on Kubeflow's model serving framework, KFServing. You will learn how to leverage a feature store to build a single feature pipeline\u00a0that is used to supply features for both training and serving, and you will learn how to productionize the feature engineering and training pipelines with Airflow.\u00a0You will learn how you can scale this entire framework from running on your laptop to a cluster of hundreds of servers.\nWe will work with Hopsworks, an open-source data science platform that\u00a0includes the industry's first open-source feature store for machine learning. Hopsworks can run on anything from your laptop to a cluster of hundreds of servers. To enable participants to learn by doing, we will provide web-based access to a managed version of Hopsworks, running at www.hopsworks.ai. All you will need is a laptop and web access to join in, along with knowledge of Python.", "title": "Bringing models to Production with an open-source Feature Store", "url": "https://2021.berlinbuzzwords.de/session/bringing-models-production-open-source-feature-store", "speaker": "Jim Dowling"}, {"level": "Intermediate", "track": "Scale", "abstract": "Nowadays, users expect your app to be not only fast and reliable but also smart. As a consequence, more and more teams are becoming data-intensive \u2014 relying on data to build their solutions. And it\u2019s a common belief that putting models into production is one of the biggest bottlenecks in a journey of becoming more data-driven. While true, this step is only the beginning of the journey.\nI believe that a much broader transformation is required in how we think about product development lifecycle as well as communication flows between business, engineering, and data.\u00a0\nIn this talk, I\u2019ll show how we are building data-grounded solutions in the domain of search and recommendations and instilling an experimental culture in one of the biggest online marketplaces.", "title": "Building Data-Intensive Teams", "url": "https://2021.berlinbuzzwords.de/session/building-data-intensive-teams", "speaker": "Elias Nema"}, {"level": "Intermediate", "track": "Store", "abstract": "Metadata has been a key data infrastructure need since the beginning of our team's history at Stitch Fix.\nWe began this journey in 2015 with the setup of the Hive Metastore to work with Spark, Presto, and the rest of the platform infrastructure. But as our business needs grew, we felt the need to enhance and extend our metadata ecosystem.\nIn this talk, we want to share our journey of building additional capabilities with metadata to solve data and business challenges. Starting with our base infrastructure - the Hive Metastore, we will highlight each capability that led us to build the extensions into our present day metadata infrastructure. This includes improvements made to the Hive Metastore itself, extending the use of metadata beyond table schemas, and additional microservices we added to make access and use of metadata easier.\nBuilding these capabilities has helped our team use metadata to power internal use cases. We want to share how we went about building this ecosystem and the lessons we learned along the way.", "title": "Building a metadata ecosystem using the Hive Metastore", "url": "https://2021.berlinbuzzwords.de/session/building-metadata-ecosystem-using-hive-metastore", "speaker": "Neelesh Salian"}, {"level": "Intermediate", "track": "Scale", "abstract": "The need for companies to deploy and operate Big Data infrastructures hasn't gone away but their options to do so have dwindled in the past few years. That's why we decided to build a new Open Source Big Data distribution.\nIt includes the usual suspects like Apache Kafka, Apache Spark, Apache NiFi, etc.\nWe asked around and were told it's a crazy idea but we did it anyway: We implemented a Kubelet in Rust that uses systemd as its backend instead of a container runtime. We also started writing Operators that target these special kubelets.\nThis means we can deploy hybrid infrastructure (partly running in containers and partly on \"bare metal\") using the same stack, the same tools, the same description languages, the same knowledge, etc. getting the best of both worlds.\nIn this talk we'll share what we learned about writing Kubernetes Operators (in Rust) as well as gain insights into our new distribution.", "title": "Building a new Big Data distribution based on Kubernetes - with a twist!", "url": "https://2021.berlinbuzzwords.de/session/building-new-big-data-distribution-based-kubernetes-twist", "speaker": "Lars Francke"}, {"level": "Intermediate", "track": "Stream", "abstract": "Building streaming systems is a popular way for developers to implement applications that react to data changes and process events as they happen. It is an exciting new world that technologies like Apache Pulsar\u00a0made available for anyone to use. But all this goodness doesn\u2019t come for free. One of the challenges of this type of architecture is that its distributed nature makes it hard and sometimes even impossible to identify the root cause of problems quickly. \u00a0\nThat is why distributed tracing technologies are so important. By gluing together disparate services into a single and cohesive transaction, developers can provide to the operations team a way to pragmatically observe the system and to quickly identify the root cause of problems such as slowness and unavailability. This talk will explain how to implement distributed tracing in Pulsar applications using OpenTelemetry\u2014an observability framework for cloud-native software. A demo will be used to clarify the concepts.", "title": "Building Observable Streaming Systems with OpenTelemetry", "url": "https://2021.berlinbuzzwords.de/session/building-observable-streaming-systems-opentelemetry", "speaker": "Ricardo Ferreira"}, {"level": "Advanced", "track": "Store", "abstract": "At our company, we leverage Apache Druid to provide our customers with real-time analytics tools for various use-cases, including in-flight analytics, reporting and building target audiences.\nThe common challenge of these use-cases is counting distinct elements in real-time at scale.\nWe've been using Druid to solve these problems for the past 5 years, and gained a lot of experience with it.\nIn this talk, we will share some of the best practices and tips we've gathered over the years. \u00a0\nWe will cover the following topics:\u00a0\n\n\tData modeling\u00a0\n\t\n\n\tIngestion\u00a0\n\t\n\n\tRetention and deletion\u00a0\n\t\n\n\tQuery optimization", "title": "Casting the Spell: Druid in Practice", "url": "https://2021.berlinbuzzwords.de/session/casting-spell-druid-practice", "speaker": "Itai Yaffe"}, {"level": "Intermediate", "track": "Stream", "abstract": "Microservices are one of the big trends in software engineering of the last few years; organising business functionality in several self-contained, loosely coupled services helps teams to work efficiently, make the most suitable technical decisions, and react quickly to new business requirements.\nIn this session we'll discuss and showcase how open-source change data capture (CDC) with Debezium can help developers with typical challenges they often face when working on microservices. Come and join us to learn how to:\n* Employ the outbox pattern for reliable, eventually consistent data exchange between microservices, without incurring unsafe dual writes or tight coupling\n* Gradually extract microservices from existing monolithic applications, using CDC and the strangler fig pattern\n* Coordinate long-running business transactions across multiple services using CDC-based saga orchestration, ensuring such activity gets consistently applied or aborted by all participating services", "title": "Change Data Streaming Patterns in Distributed Systems", "url": "https://2021.berlinbuzzwords.de/session/change-data-streaming-patterns-distributed-systems", "speaker": "Gunnar Morling"}, {"level": "Beginner", "track": "Scale", "abstract": "An overview of the commercialisation of open source and a review of the potential revenue models and their interface with open source, from the first study into this in 2008 to the recent shift by Elastic away from open source to the SSPL licence. Amanda is the editor of Open Source, Law, Policy and Practise with over 20 authors, to be published in September and as the author of the chapter on commercial models and open has spent some time researching this area. She has spent over 13 years working in and around open source software and its commercialisation.", "title": "Commercial Models and Open Source - How Revenue Generation and Open Source Fit Together", "url": "https://2021.berlinbuzzwords.de/session/commercial-models-and-open-source-how-revenue-generation-and-open-source-fit-together", "speaker": "Amanda Brock"}, {"level": "Advanced", "track": "Scale", "abstract": "Image & text similarity problems have plenty of textbook ML solutions. However in the wild, these solutions often fail. In this talk I'll present a use-case about inferring product similarity from multiple sources of data (images, text, etc) and discuss how we developed a practical & scalable approach using our understanding of domain knowledge.", "title": "Comparing apples to apple trees: Injecting domain knowledge to similarity searches", "url": "https://2021.berlinbuzzwords.de/session/comparing-apples-apple-trees-injecting-domain-knowledge-similarity-searches", "speaker": "Yizhar Toren"}, {"level": "Intermediate", "track": "Stream", "abstract": "Open session where Lars Albertsson, data engineering entrepreneur and recurring Berlin Buzzwords speaker, answers questions regarding data engineering and DataOps.\nThis session will be hosted by Ellen Friedman.", "title": "Data engineering & DataOps - Ask Me Anything", "url": "https://2021.berlinbuzzwords.de/session/data-engineering-dataops-ask-me-anything", "speaker": "Lars Albertsson"}, {"level": "Intermediate", "track": "Stream", "abstract": "Letgo is a second-hand marketplace app reshaping secondhand trade in Turkey so re-use is the default trusted choice.\nWe designed the data platform to be built on top of the principles of self-servicing, privacy laws compliance, data governance at business unit level, minimal maintenance and cost containment by design.\nWe will describe how we defined our company-wise data model, leveraging Avro schemas and enabling at the same time most impactful features like:\n\n\ttagging private fields for sensitive data for data privacy laws compliance\n\t\n\n\tensuring quality and structure of the data landing in the company data lake\n\t\n\n\tefficient and reliable transportation and consumption of data at platform level\n\t\n\n\tdata catalog: discovery of available data by teams\n\t\nOur design is built around the Apache Kafka ecosystem\u2014with special mention to Kafka Connect\u2014for data ingestion and AWS services plus Spark framework for data transformations and data lake ingestion.\nThanks to these principles we are able to ensure data governance over batch and real-time data, while keeping at the same time a multi-tiered data lake: the inner tier keeps the most sensitive data and the outer tiers keep only the data accessible for each single company business unit.", "title": "Data governance in streaming at scale", "url": "https://2021.berlinbuzzwords.de/session/data-governance-streaming-scale", "speaker": "Sebasti\u00e1n Ortega"}, {"level": "Advanced", "track": "Store", "abstract": "Web technologies have come leaps and bounds. But are you still using the tired old database from last generation? Let's look at the methodology of microservices, compare it to bounded contexts, and look at ops tasks for micro-databases. Let's tour all the flavors of databases, understand their pros and cons, and when you would choose it. You'll leave with a roadmap for moving from data-monolith to micro-databases.", "title": "Databases in the Microservices World", "url": "https://2021.berlinbuzzwords.de/session/databases-microservices-world", "speaker": "Rob Richardson"}, {"level": "Beginner", "track": "Search", "abstract": "Elasticsearch, Solr, Solr or the comparably new contender Vespa from OATH/Yahoo. Which is a suitable search engine for me?\u00a0 We gathered a group of industry experts to help you answer this question:\nAnshum Gupta, Software Engineer - Apache Lucene/Solr committer and VP of Apache Lucene\nJosh Devins, Senior Principal Engineer, Machine Learning at Elastic\nJo Kristian Bergum, Senior Principal Software Engineer at Verizon Media working on Vespa.\nModerated by Charlie Hull, Managing Consultant at OpenSource Connections.\nThis debate is presented by Haystack, the search relevance conference. Find out more at https://haystackconf.com/", "title": "Debate: Which Search Engine?", "url": "https://2021.berlinbuzzwords.de/session/debate-which-search-engine", "speaker": "Jo Kristian Bergum"}, {"level": "Beginner", "track": "Scale", "abstract": "Rapid growth in machine learning and AI has made huge leaps in expanding the capabilities of machines. But this has come at the large energy cost of compute clusters which are making a bad impact on the environment. At the same time advances in computational neuroscience and neuromorphic engineering are converging and offering a viable bio-inspired alternatives to AI which perform with orders of magnitude less energy requirements, much faster latency, and many unique advantages.\nMy talk would inform the audience about this new trend and the many exciting developments happening in academia and the industry like bio-inspired neural networks and neuromorphic hardware platforms (Intel, Qualcomm, IBM, etc.).", "title": "Deep Learning, Neuroscience and the future of AI", "url": "https://2021.berlinbuzzwords.de/session/deep-learning-neuroscience-and-future-ai", "speaker": "Darjan Salaj"}, {"level": "Beginner", "track": "Scale", "abstract": "While everyone was sheltering in place in 2020, a group of citizen scientists decided to tackle the problem of auto-detecting interesting weather patterns in earth\u2019s imagery collected by NASA\u00a0satellites. The problem - we were dealing with a scale we had never seen before - 20 years worth of earth\u2019s imagery collected continuously\u00a0by not just NASA but also other private and public space agencies across the world which was only growing exponentially by the day. We wanted to build a reverse image search engine on this massive unlabelled dataset and automatically detect interesting phenomena such as hurricanes, polar vortexes, melting ice caps etc. NASA\u2019s scientists had performed extensive research to solve this problem in theory - but no one had attempted to build a production quality system to put it in practice before.\u00a0\nSpaceML was started in collaboration with NASA\u2019s Frontier Development Lab and Google Cloud and is built entirely by industry professionals and student mentees around the world with their donated time.\nIn this presentation we will talk about how we solved the problem of applying deep learning to continuously search for interesting\u00a0weather patterns in petabytes of earth\u2019s imagery. We will cover the challenges involved in continuous data processing, indexing and running\u00a0 distributed search while providing a low latency, highly available search API. And how we used Google Cloud offerings such as Dataflow, Functions, App Engine along with Pytorch and nearest neighbor search libraries such as SCANN, FAISS and Annoy to make it happen. We will detail the end to end self supervised learning system that we built with an eye on cost constrained usage of cloud resources while maintaining extensibility for other space science endeavors. We will also touch upon the organizational challenges in building this system with a highly distributed team including how we employed fast prototyping to build confidence in the system while gradually increasing the scale to petabytes of data.\nWe built this system with the goal of open sourcing the set of components to expand the project\u2019s applicability beyond space science. In this talk we will describe the architecture of the individual components so that you can leverage them to enable deep learning on any type of dataset in your field.", "title": "Democratizing climate science: Searching through NASA\u2019s earth data with AI at scale", "url": "https://2021.berlinbuzzwords.de/session/democratizing-climate-science-searching-through-nasas-earth-data-ai-scale", "speaker": "Sherin Thomas"}, {"level": "Intermediate", "track": "Search", "abstract": "Are you or your company migrating services to Kubernetes? Do you want to run everything there, even Solr? If so you might have a lot of questions on how to proceed. Houston and Tim intend to help clear things up a bit.\u00a0\nThis is a conversation about managing Solr on Kubernetes. Houston and Tim will be discussing the newly donated Apache Solr Operator, and how it fits into your Solr Management workflow on Kube. Specifically what aspects it has control over, and which aspects it does not. Covering all possible migration scenarios would be impossible, so the focus will be on explaining the architecture principles of the Solr Operator and defining the roles of each piece of the system. \u00a0\nHopefully, everyone will leave the discussion confident that they can answer the following questions:\nWho is in charge of what?\nKubernetes?\nSolr Operator?\nSolr?\nMe???\n\nWhat happens when one part of the system goes down?\nWhy should I, or should I not, use the Solr Operator?", "title": "Demystifying the Solr Operator", "url": "https://2021.berlinbuzzwords.de/session/demystifying-solr-operator", "speaker": "Houston Putman"}, {"level": "Intermediate", "track": "Stream", "abstract": "As modern software architecture evolves and we adopt event-driven systems into our practice, let's take time to get into the nitty-gritty of designing the payloads that actually carry those events around. With strategies for which fields to include and how to handle changes to the data structure as the requirements evolve, this session has real-world advice to keep you on track.\nWhen it comes to data formats, choosing between self-contained formats such as JSON or XML, or a serialization format like Avro, this session covers how to design an approach that fits your application and platform. The session includes examples of using event-streaming tools such as Kafka with your application and offers to gotchas to avoid when teaching your applications to play nicely together.", "title": "Designing Payloads for Event-Driven Systems", "url": "https://2021.berlinbuzzwords.de/session/designing-payloads-event-driven-systems", "speaker": "Lorna Mitchell"}, {"level": "Intermediate", "track": "Search", "abstract": "How to create digital products in a transparent and ethical way, guiding brands to create trustworthy experiences. Lara and Angel will redefine concepts such as User, Analytics and Purpose, by elevating emotions as the core principal focus.\nWORKSHOP TAKEAWAYS:\nA model for conceiving and designing product foundation from an ethical approach.\nA glossary of concepts to refer to when protecting an individual\u2019s product perceptions.\nThis workshop is presented by Empathy.co", "title": "Digital & Ethics", "url": "https://2021.berlinbuzzwords.de/session/digital-ethics", "speaker": "Angel Maldonado"}, {"level": "Intermediate", "track": "Scale", "abstract": "Have you recently moved to microservices? Your team is deploying the code much faster, but data transfer costs are going up as well, aren\u2019t they? That\u2019s because the size of containers matters the most, and keeping them lightweight means saving on bandwidth usage.\nBellSoft\u2019s engineers have come up with a solution, which is full-fledged Alpine Linux support in OpenJDK. By that, we\u2019ve also invented a real-life TARDIS: Containers that take only a few MB of storage but carry enormous potential. When JDK 16 is released, the Portola Project will integrate into the OpenJDK mainline within our JEP 386. Duct-taping with a glibc layer will become a thing of the past, as all the processes will connect flawlessly. Your company will get to use tiny container images independently of the distribution kit vendor. They have been available for a long time, but the official HotSpot port status for the musl library will expand the scope and simplify related development.\nMy talk is going to touch upon the benefits that Alpine Linux is bringing to the OpenJDK community. It will also explain how to optimize Docker images for free by changing just one or two lines of code. Lastly, I\u2019ll offer a tool for choosing an optimal container that will suit your project perfectly.", "title": "Docker Who: Small Containers Through Time and Space", "url": "https://2021.berlinbuzzwords.de/session/docker-who-small-containers-through-time-and-space", "speaker": "Dmitry Chuyko"}, {"level": "Intermediate", "track": "Search", "abstract": "Payloads are a powerful though seldom utilized feature in the Lucene-Solr ecosystem.\u00a0 This talk reviews the existing payload support in Lucene and introduces the new features in Lucene and Solr 9\u00a0 (LUCENE-9659 / SOLR-14787).\u00a0 The main focus of the talk will be to explore real world search & ml use cases that traditionally utilize a query time join and the application of Lucene payloads to solve them. This talk is for search practitioners interested in utilizing machine learned data in search based analytics dashboards.\u00a0\nMany Solr based applications attempting to deal with machine learned classifications are forced to implement a parent-child join relationship between a document and its classifications.\u00a0 This model introduces many additional system constraints and costs at both query and index time to maintain the ability to filter results as desired.\u00a0\nNew features in the payload span query in Lucene provide applications a way to maintain query flexibility without incurring the cost of performing a query-time join.\u00a0 This greatly simplifies system design and architecture and can provide dramatic improvements to query performance.\nA reference implementation will be presented that compares the join and payload approaches.\u00a0 The demonstration will show how to search for documents that have classifications above a particular confidence threshold at scale.", "title": "Document classification search; joins vs payloads", "url": "https://2021.berlinbuzzwords.de/session/document-classification-search-joins-vs-payloads", "speaker": "Kevin Watters"}, {"level": "Beginner", "track": "Search", "abstract": "Most documentation, technical tutorials, and quick demos are written in a certain way. But for the true beginners, the career-transitioners, or those crossing domains? That technical content is certainly not written for them! Funnily enough, these same shortcomings affect \u201ctechnical\u201d people too, especially when it comes to learning something new.\nIn this talk, I\u2019d like to explore the ways we can make our documentation better by considering more kinds of people. We\u2019ll discover common oversights and assumptions most documentation has built-in by default and learn how to fix them. We\u2019ll also strengthen our technical writing skills to ensure, to the best of our ability, that every anticipated reader of our documentation never feels lost or frustrated.\nBy the end of this talk, you\u2019ll leave and never write documentation in the same way again\u2026and that\u2019s a good thing!", "title": "Documentation: The Missing Pieces", "url": "https://2021.berlinbuzzwords.de/session/documentation-missing-pieces", "speaker": "Adrienne Tacke"}, {"level": "Intermediate", "track": "Search", "abstract": "Most of our search development work focuses on retrieving the right documents and showing them to the user in the best possible order. Additional features like spelling correction or auto-completion receive less attention and they are often neglected. We think that for the user, these features can have a major impact on the success of search - they are not encores!\nIn this workshop participants will learn how they can build features such as facets, auto-completion and auto-suggest, spelling correction and query relaxation for e-commerce search using the Solr search engine. We will also discuss the intuition and challenges behind these features as a starting point for participants to implement solutions beyond out-of-the-box components.\nThis workshop is presented by OpenSourceConnections.", "title": "Encores? - Going beyond matching and ranking of search results", "url": "https://2021.berlinbuzzwords.de/session/encores-going-beyond-matching-and-ranking-search-results", "speaker": "Ren\u00e9 Kriegler"}, {"level": "Intermediate", "track": "Search", "abstract": "Measuring Intent is useful and provides guidelines in the creation of product. Can intuitive and anticipatory products be engineered over a flat interpretation of intent or can that which is unmeasurable be accounted for? These important questions create conditions for ethical and irresistible products.\nThis session is presented by Empathy.co", "title": "Engineering Intent and Intent Measurability", "url": "https://2021.berlinbuzzwords.de/session/engineering-intent-and-intent-measurability", "speaker": "Angel Maldonado"}, {"level": "Beginner", "track": "Search", "abstract": "Datafari is among the few available open source Enterprise Search solutions.\nIt can be useful to index your hard drives, your organisation fileshares, but also many other types of sources and formats. Several connectors are available off the shelf (Sharepoint, SMB, DB, web, Alfresco...).\nSo what is the difference with Solr or openDistro ? It is an additional layer: Datafari is not just the search engine per se, it also embeds the crawling framework, a search UI, and an admin UI.\nIn this initiation workshop, we will first introduce Datafari and its architecture, and we will then directly dive into a quick install and indexing phase: how to install Datafari Community Edition, create first crawlers and configuration, and how to customise the UI.\nWith that, you can benefit from Apache Tika, Apache ManifoldCF, Apache Solr, and openDistro without the hassle of configuring and interconnecting all of them.", "title": "Enterprise Search 101 with Datafari Open Source", "url": "https://2021.berlinbuzzwords.de/session/enterprise-search-101-datafari-open-source-0", "speaker": "Julien Massiera"}, {"level": "Beginner", "track": "Stream", "abstract": "Code and data go together like tomato and basil; not many applications work without moving data in some way. As our applications modernise and evolve to become more event-driven, the requirements for data are changing. In this session we will explore Apache Kafka, a data streaming platform, to enable reliable real-time data integration for your applications.\nWe will look at the types of problems that Kafka is best at solving, and show how to use it in your own applications. Whether you have a new application or are looking to upgrade an existing one, this session includes advice on adding Kafka using the Python libraries and includes code examples (with bonus discussion of pizza toppings) to use.\nWith Kafka in place, many things are possible so this session also introduces Kafka Connect, a selection of pre-built connectors that you can use to route events between systems and integrate with other tools. This session is recommended for engineers and architects whose applications are ready for next-level data abilities.", "title": "Event-Driven applications: Apache Kafka and Python", "url": "https://2021.berlinbuzzwords.de/session/event-driven-applications-apache-kafka-and-python", "speaker": "Francesco Tisiot"}, {"level": "Intermediate", "track": "Stream", "abstract": "Kafka Streams is a robust and scalable library for developing event-driven micro-services backed by Apache Kafka.\nThis code-packed session will provide an introduction to Kafka Streams.\nJoin me to discuss and demo:\nserialization formats and serializers,\nthe Streams DSL for data processing,\ntime semantics and joins,\neffective testing with TopologyTestDriver and test-containers,\nerror handling strategies,\nconfigure Streams applications to minimize failover times,\nbest practices for working with state stores,\nusing state stores as data serving layer via interactive queries.", "title": "Event-driven micro-services with Kafka Streams", "url": "https://2021.berlinbuzzwords.de/session/event-driven-micro-services-kafka-streams", "speaker": "Christoph Schubert"}, {"level": "Intermediate", "track": "Scale", "abstract": "Machine learning doesn\u2019t have the same objectives as its users. While models look to optimize a function using the given data, humans look to gain insight into their problems. At best, these two objectives align; at worst, machine learning models make the front page of the news for unintended, but astonishing bias. Model explainability algorithms allow data scientists to understand not only what the model outcome is, but why it is being made. This talk will explain what model explainability is, who should care, and show participants how/when to use multiple types of explainability algorithms.\u00a0\nThis session shows the usefulness of a variety of algorithms, but also discusses the limitations. Told from a data scientist\u2019s point of view, this session provides a use case scenario exposing unintended bias using healthcare data. The audience will learn: the basics of model explainability, why this is a relevant issue, how model explainability offers insight into unintended bias, and know how to deploy explainability algorithms in Python with alibi, the open-source library from Seldon.", "title": "Explaining model explainability", "url": "https://2021.berlinbuzzwords.de/session/explaining-model-explainability", "speaker": "Isabel Zimmerman"}, {"level": "Intermediate", "track": "Search", "abstract": "This is a normal E-commerce use case that had\u00a0been relying on earlier standalone Solr and later Master-Slave for ages.\nEverything was running smoothly with no performance glitches until one day we had this requirement to support \u201cUnlimited Products\u201d in our shops.\nWe were confident that things would be smooth with full reindex 2 times a day but looking at the index pipeline duration of 4+ hrs, we realized\u00a0it needed some cloud magic.\nThis is where we started and transformed a normal batch index pipeline to a stream index pipeline leveraging Kafka and Redis and achieved a NRT indexing.\nAlong with that, we did not shy to give our infra a bonus upgrade from Solr 6.6.2 to Solr 8.7 and an add-on migration from the existing Master-Slave to Cloud.\nWe want to talk about our journey and important lessons learned during this process.", "title": "Exploring the alchemy of Streaming and Solr cloud !", "url": "https://2021.berlinbuzzwords.de/session/exploring-alchemy-streaming-and-solr-cloud", "speaker": "Atita Arora"}, {"level": "Intermediate", "track": "Scale", "abstract": "Kubernetes and software engineering practice are quietly revolutionizing data science by providing practitioners with better infrastructure and more disciplined habits, and many tools build on these primitives and practices to make machine learning deployments on Kubernetes simple, portable, and scalable.\u00a0 However, bringing engineering discipline to data science workflows turns out to be a thorny problem, and reproducible research is harder to achieve than we might assume.\u00a0 In this talk, we\u2019ll examine the problem of reproducible research from several angles and present tools we\u2019ve built on Kubernetes that address different facets of the problem. You\u2019ll see how to treat Jupyter notebooks as real software artifacts -- not merely as ad hoc environments for discovery -- and learn about what that mindset change entails.\u00a0 You\u2019ll see how we build workflows from notebooks, how we automatically generate model services with CI/CD pipelines, and the tools we use to generate and track metrics to identify concept drift.\u00a0 You\u2019ll learn about some surprising challenges of reproducibility and learn why some convenient model operationalization workflows might require heroic practitioner discipline to produce consistent results.", "title": "Facets of Reproducible research on Kubernetes", "url": "https://2021.berlinbuzzwords.de/session/facets-reproducible-research-kubernetes", "speaker": "Sophie Watson"}, {"level": "Beginner", "track": "Stream", "abstract": "OLAP data stores like Apache Pinot are emerging to serve low-latency analytical queries at web scale. With its columnar data format and rich indexing strategies, Pinot is a perfect fit for running complex, interactive queries on multi-dimensional data within milliseconds. In some cases, though, streaming data will require non-trivial pre-processing that is not supported in Pinot, like joins and pre-aggregations. What then?\u00a0\nIn this talk, we\u2019ll cover the benefits of combining Pinot and stream processing with Flink SQL to power near real-time OLAP use cases, and build a simple demo to analyze streaming Twitch data (#meta) \u2014 from ingestion to visualization!", "title": "Faster Analytics for Fast Data with Apache Pinot and Flink SQL", "url": "https://2021.berlinbuzzwords.de/session/faster-analytics-fast-data-apache-pinot-and-flink-sql", "speaker": "Chinmay Soman"}, {"level": "Intermediate", "track": "Search", "abstract": "Since version 3 of Apache Lucene and Solr and from the early beginning of Elasticsearch, the general recommendation was to use MMapDirectory as the implementation for index access on disk. But why is this so important?\nThis talk will first introduce the user about the technical details of memory mapping and why using other techniques slows down index access by a significant amount. Of course we no longer need to talk about 32/64bit Java VMs - everybody uses now 64 bits\u00a0with Elasticsearch and Solr, but with current Java versions, Lucene still has some 32bit-like limitations on accessing the on-disk index with memory mapping. We will discuss those limitations especially with growing index size up to terabytes, and afterwards, Uwe will give an introduction to the new\u00a0Java Foreign Memory Access API\u00a0(JEP 370, JEP 383, JEP 393), that first appeared\u00a0with Java 14, but still incubating.\nThe new API\u00a0sounds interesting and will remove all previous issues and limitations, but with Lucene's current design, the first and second JEP incubators\u00a0(Java 14, 15) would have been hard to implement. In close cooperation between Lucene developers and OpenJDK committers, starting with Java 16, the 3rd incubator will finally be ready to be used from Lucene:\u00a0A first preview of Lucene's implementation\u00a0was developed as a draft pull request. Uwe will show how future versions of Lucene will be backed by next\u00a0generation memory mapping and what needs to be done to make this usable in Solr and Elasticsearch - bringing you memory mapping for indexes with tens or maybe hundreds\u00a0of Terabytes in the future!", "title": "The future of Lucene's MMapDirectory: Why use it and what's coming with Java 16 and later?", "url": "https://2021.berlinbuzzwords.de/session/future-lucenes-mmapdirectory-why-use-it-and-whats-coming-java-16-and-later", "speaker": "Uwe Schindler"}, {"level": "Intermediate", "track": "Search", "abstract": "Apache Solr is a critical piece of infrastructure for most companies dealing with data. The systems that Solr often powers are critical, requiring high availability and disaster recovery.\n\u00a0\nOften users tend to undermine the importance of these features, but more importantly there\u2019s a need to better understand the need for these before it\u2019s too late. In the recent past, a lot of effort has been put in by the community to build features that allow for operating a reliable and highly available Solr setup.\n\u00a0\nDuring this talk, I will talk about the DR and HA options that Solr provides like incremental backups, backup to Blob store, and Cross-DC replication. I will explain the reasons that make them essential, and also explain what these features actually do under the covers.\n\u00a0\nAt the end of this talk, attendees would have a better understanding about HA and DR in general, and the options they have to improve the reliability of their Solr clusters. They would also be equipped to enable and use those features to operate a reliable Solr cluster that is disaster ready.", "title": "Highly available and disaster ready Apache Solr", "url": "https://2021.berlinbuzzwords.de/session/highly-available-and-disaster-ready-apache-solr", "speaker": "Anshum Gupta"}, {"level": "Intermediate", "track": "Store", "abstract": "If you're dealing with structured data at scale, it's a safe bet that you're depending on Apache Parquet in at least a few parts of your pipeline. Parquet is a sensible default choice for storing structured data at rest because of two major advantages: \u00a0its efficiency and its ubiquity. \u00a0While Parquet's storage efficiency enables dramatically improved time and space performance for query jobs, its ubiquity may be even more valuable. \u00a0Since Parquet readers and writers are available in a wide range of languages and ecosystems, the Parquet format can support a range of applications across the data lifecycle, including data engineering and ETL jobs, query engines, and machine learning pipelines.\nHowever, the ubiquity of Parquet readers and writers hides some complexity: \u00a0if you don't take care, some of the advantages of Parquet can be lost in translation as you move tables from Hadoop, Flink, or Spark jobs to Python machine learning code. \u00a0This talk will help you understand Parquet more fully in order to use it more effectively, with an eye towards the special challenges that might arise in polyglot environments. \u00a0We'll level-set with a quick overview of how Parquet works and why it's so efficient. \u00a0We'll then dive in to the type, encoding, and compression options available and discuss when each is most appropriate. \u00a0You'll learn how to interrogate and understand Parquet metadata, and you'll learn about some of the challenges you'll run into when sharing data between JVM-based data engineering pipelines and Python-based machine learning pipelines. \u00a0You'll leave this talk with a better understanding of Parquet and a roadmap pointing you away from some interoperability and performance pitfalls.", "title": "How to avoid columnar calamities:  what no one told you about Apache Parquet", "url": "https://2021.berlinbuzzwords.de/session/how-avoid-columnar-calamities-what-no-one-told-you-about-apache-parquet", "speaker": "William Benton"}, {"level": "Beginner", "track": "Search", "abstract": "Context\nIn almost all markets, online marketplaces are in charge. From ride sharing to food delivery to e-commerce. While these marketplaces are very good at matching offer and demand, this benefit comes at a cost: \u00a01) A lot of data is being collected, both from suppliers and consumers. 2) The marketplace owns the customer experience, taking away the opportunity from retailers to build a sustainable relationship with their customers. 3) In e-commerce, retailers face the additional challenge that the platform is also a competitor, but with the advantage of more data and no commission fees. \u00a0 \u00a0\n\u00a0\nProposed solution\nWe are proposing a decentralised solution that allows retailers to connect directly with customers through the vendor relationship manager-model (a.k.a. Me2B), allowing them to have more control over the customer experience.\nInstead of customers connecting to individual retailers or a marketplace, VRM enables retailers to connect to the customer\u2019s own environment. This enables customers to create their own marketplace (i.e. the customer becomes its own platform). With this model both retailers and customers gaine more control which opens up new possibilities for search and discovery.\n\u00a0In addition, we are focusing on a local first model, where local retailers are matched to local customers.\n\u00a0\nTalk\nThe following will be discussed:\nProposal for a search experience within the customers own environment\nThe challenges and possible solutions for this search experience\nImplications for business (benefits and challenges)\nImplications for customers (benefits and challenges)\n\u00a0\nAudience\nWe are challenging the audience with our view that there is an alternative to online marketplaces that is better for people and business and encourage them to think about the implication this has on search and discovery, both from a business and a technical perspective.", "title": "How Me2B can improve your search and discovery experience", "url": "https://2021.berlinbuzzwords.de/session/how-me2b-can-improve-your-search-and-discovery-experience", "speaker": "Pieter Vegt"}, {"level": "Intermediate", "track": "Search", "abstract": "Traditionally search has been all about \"how do I\u00a0find the one (or five) documents that answer my question?\". However,\u00a0what\u00a0about when I want to encourage serendipity and discovery (common in\u00a0research use cases)? In those cases, I care about\u00a0increasing the\u00a0diversity of my search results, while still ensuring the precision (i.e\u00a0relevance) is decent. This talk will provide a\u00a0simple formula for\u00a0measuring your search results diversity (using Apache Zeppelin as a notebook), and offer some concrete tips on\u00a0surfacing diverse search results.", "title": "How to measure Diversity of Search Results", "url": "https://2021.berlinbuzzwords.de/session/how-measure-diversity-search-results", "speaker": "Eric Pugh"}, {"level": "Intermediate", "track": "Search", "abstract": "Utilizing machine learning models to improve search has been an immensely active area for several years now. Some promises were kept, many others were broken. With the rise of Transformer models like BERT\u00a0we seem to finally be entering a chapter, where models not only perform well in the research lab but actually make their way into the production stack.\u00a0\nNow that almost every English Google search query is powered by a Transformer [1], it is clear that these models improve the search experience, and can do so at scale.\u00a0As Transformers only rely on text, the transition from web search to a custom enterprise search seems more tempting than ever.\nIn this talk, we will dive into some of the most promising methods and show how to ...\u00a0\n\n\t\u2026 improve document retrieval via dense passage retrieval\u00a0\n\n\n\t\u2026 return more granular search results by showing direct answers to user\u2019s questions\n\t\n\n\t\u2026 scale those pipelines via DAGs and Approx. Nearest Neighbour search (ANN) for production workloads\n\t\n\n\t\u2026 avoid common pitfalls when moving to production\n\t\nAll methods will be illustrated with code examples based on the open-source framework Haystack [2] so that participants can easily reproduce them at home and let the transformers into their production stack - one by one and carefully selected!\n[1] https://searchengineland.com/google-bert-used-on-almost-every-english-query-342193\n[2]\u00a0https://github.com/deepset-ai/haystack/", "title": "The Invasion of Transformers - Boosting Search with Latest NLP", "url": "https://2021.berlinbuzzwords.de/session/invasion-transformers-boosting-search-latest-nlp", "speaker": "Malte Pietsch"}, {"level": "Intermediate", "track": "Stream", "abstract": "Lambda Architecture has been a common way to build data pipelines for a long time, despite difficulties in maintaining two complex systems. An alternative, Kappa Architecture, was proposed in 2014, but many companies are still reluctant to switch to Kappa. And there is a reason for that: even though Kappa generally provides a simpler design and similar or lower latency, there are a lot of practical challenges in areas like exactly-once delivery, late-arriving data, historical backfill and reprocessing.\nIn this talk, I want to show how you can solve those challenges by embracing Apache Kafka as a foundation of your data pipeline and leveraging modern stream-processing frameworks like\u00a0Apache Flink.", "title": "It's Time To Stop Using Lambda Architecture", "url": "https://2021.berlinbuzzwords.de/session/its-time-stop-using-lambda-architecture", "speaker": "Yaroslav Tkachenko"}, {"level": "Beginner", "track": "Stream", "abstract": "We all love to play with the shiny toys, but an event stream with no events is a sorry sight. In this session you\u2019ll see how to create your own streaming dataset for Apache Kafka using Python and the Faker library. You\u2019ll learn how to create a random data producer and define the structure and rate of its message delivery. Randomly-generated data is often hilarious in its own right, and it adds just the right amount of fun to any Kafka and its integrations!", "title": "Kickstart your Kafka with Faker Data", "url": "https://2021.berlinbuzzwords.de/session/kickstart-your-kafka-faker-data", "speaker": "Francesco Tisiot"}, {"level": "Advanced", "track": "Scale", "abstract": "Kubernetes get the container running platform to use with really easy add or remove resources. For real 12 factor app this is strait forward and the good way to go.\nThe challenge comes if you have long running (stateful) apps or big data apps like hadoop, spark, flink,...\u00a0\nThis talk show strategies to meditate this challenges and take advantages out of it.\u00a0\nFor example instead of run small cluster 24h, run a 12 times bigger cluster only for short intervals(~2h). If your jobs are scalable you get you results up to 12 times faster and bigger business value for same resource consumption.\u00a0Or run a live recording, you need stay until event ends even it is a 24h. Run an AI training which don\u2019t usually work so well with snapshots for recovery.\nuse cloud in dynamic way (scale 100x of capacity in minutes is possible)\nper job cluster with the fitting sizing\u00a0\nleverage multiple node pools/groups\u00a0\nhelp from k8s operators for deployment\u00a0\nhow this can work together with workflows like airflow\nk8s cluster auto scaler, how to leverage him and pitfalls\nk8s scheduler, alternatives , options to consider", "title": "Kubernetes and the dynamic world in the cloud", "url": "https://2021.berlinbuzzwords.de/session/kubernetes-and-dynamic-world-cloud", "speaker": "Frank Conrad"}, {"level": "Intermediate", "track": "Stream", "abstract": "More and more services are running in Kubernetes so it means that we can migrate our current data pipelines to the new environment. In case of Flink we have multiple ways to do real-time data streaming: use Lyft or GCP operator, go with official deployment and customize it or choose the Ververica Platform or create something on your own. The presentation shows how to choose the right solution for technical requirements and business needs to run Flink in Kubernetes at great scale with no issues.", "title": "Kubernetes and real-time analytics - how to connect these two worlds with Apache Flink?", "url": "https://2021.berlinbuzzwords.de/session/kubernetes-and-real-time-analytics-how-connect-these-two-worlds-apache-flink", "speaker": "Albert Lewandowski"}, {"level": "Intermediate", "track": "Search", "abstract": "Implementing a machine learning model for ranking in an ecommerce search requires a well-designed approach to how the target metric is defined. The challenge in e-commerce is that \u201crelevance, given an intent to buy something\u201d differs from the pure question \u201cHow relevant is this product to a given query?\u201d. Thus, we cannot use crowd sourced data for training, but need to deduct relevance from the customer interactions we collect every day on our website. We call these calculated relevancies judgements. We use them to create the ranking gold standard to train our ranking model.\nWe determine the KPI (Key Performance Indicator) (clicks, orders, ...) and the mathematical modelling of judgements based on experiments. For a sampled set of search queries, we show 50 percent of our customers a ranking based on our judgements, the other 50 percent see the status quo. By presenting our customers a ranking based on a pure target metric we can evaluate the quality of our definition of relevance.\nIn this talk we will share what we learned about our customers, our products, and the advantages of fast iterations along the way of finding a good judgement model.\nThis talk is presented by Otto.", "title": "Learning to Judge", "url": "https://2021.berlinbuzzwords.de/session/learning-judge", "speaker": "Arne Vogt"}, {"level": "Intermediate", "track": "Search", "abstract": "Logs are everywhere. But they have gone through an interesting development over the years:\ngrep: This works well as long as you have a single instance to search on. Once you need to SSH into many machines and try to piece together the results of multiple grep commands, things tend not to work that well anymore.\nSplunk: Centralizing those logs and letting users search through them with a piped language in Splunk is the logical step to fix that issue. However, the more data you centralize, the slower this will get.\nELK: The solution to that idleness is using full-text search. Elasticsearch, in combination with Logstash and Kibana (plus Beats), gave logs a major performance boost. But at what cost?\nLoki: Reducing the scope and going back to a smart data structure combined with grep gives Loki the possibility to reduce costs while still providing good performance.\nClosing the gap: So what are the tradeoffs between the different systems, and are they potentially closing some gaps between performance and cost?\nJoin the discussion after the talk for \"the right amount\" of features, costs, and speed.", "title": "Log Management: From grep to Full-Text Search and Back", "url": "https://2021.berlinbuzzwords.de/session/log-management-grep-full-text-search-and-back", "speaker": "Philipp Krenn"}, {"level": "Beginner", "track": "Scale", "abstract": "To commemorate 20 years since Doug Cutting open sourced Lucene and donated it to Apache, I want to talk about ways everyone can pitch in to ensure the long term viability of the open source project. Someone looking at replication code for the first time in 20 years, might really understand the behavior of something like master-slave. Even though we understand it today, the onus is on the contributors and maintainers to constantly add more clarity to a project. Landing such a complex PR can be tricky and usually involves multiple people. Here's how I did it, and here are a few other areas where I have been focused to improve the long-term viability of the code base.", "title": "The maintainability of open source code 20 years from today.", "url": "https://2021.berlinbuzzwords.de/session/maintainability-open-source-code-20-years-today", "speaker": "Marcus Eagan"}, {"level": "Intermediate", "track": "Scale", "abstract": "Built and maintained by people like you, open source software powers the global economy. Sustainability of this ecosystem is pivotal to accelerate human progress; yet many organizations focus their community building efforts as a sales pipeline, or a program to deflect requests for support.\u00a0\nIn this talk I\u2019d share tools to engage your communities in a positive, nurturing way; with a servants approach to building community and centered on creating advocacy, loyalty, and ultimately business value.", "title": "The new OSS community: Enabling for advocates not customers", "url": "https://2021.berlinbuzzwords.de/session/new-oss-community-enabling-advocates-not-customers", "speaker": "Andrea Griffiths"}, {"level": "Intermediate", "track": "Stream", "abstract": "Companies new and old are all recognising the importance of a low-latency, scalable, fault-tolerant data backbone, in the form of the Apache Kafka streaming platform. With Kafka, developers can integrate multiple sources and systems, which enables low latency analytics, event-driven architectures and the population of multiple downstream systems.\nIn this talk, we\u2019ll look at one of the most common integration requirements - connecting databases to Kafka. We\u2019ll consider the concept that all data is a stream of events, including that residing within a database. We\u2019ll look at why we\u2019d want to stream data from a database, including driving applications in Kafka from events upstream. We\u2019ll discuss the different methods for connecting databases to Kafka, and the pros and cons of each. Techniques including Change-Data-Capture (CDC) and Kafka Connect will be covered, as well as an exploration of the power of ksqlDB for performing transformations such as joins on the inbound data.\nAttendees of this talk will learn:\nwhy events, not just state, matter\nthe difference between log-based CDC and query-based CDC\nhow to chose which CDC approach to use", "title": "No More Silos: Integrating Databases and Apache Kafka", "url": "https://2021.berlinbuzzwords.de/session/no-more-silos-integrating-databases-and-apache-kafka-1", "speaker": "Robin Moffatt"}, {"level": "Intermediate", "track": "Scale", "abstract": "Data is increasingly becoming core to many products. Whether to provide recommendations for users, getting insights on how they use the product or using machine learning to improve the experience. This creates a critical need for reliable data operations and understanding how data is flowing through our systems. Data pipelines must be auditable, reliable and run on time. This proves particularly difficult in a constantly changing, fast paced environment.\u00a0\nCollecting this lineage metadata as data pipelines are running provides understanding of dependencies between many teams consuming and producing data and how constant changes impact them. It is the underlying foundation that enables the many use cases related to data operations.\nThe OpenLineage project is an API standardizing this metadata across the ecosystem, reducing complexity and duplicate work in collecting lineage information. It enables many projects, consumers of lineage in the ecosystem whether they focus on operations, governance or security.\nMarquez is an open source projects part of the LF AI & Data foundation which instruments data pipelines to collect lineage and metadata and enable those use cases. It implements the OpenLineage API and provides context by making visible dependencies across organisations and technologies as they change over time.", "title": "Observability for data pipelines with OpenLineage", "url": "https://2021.berlinbuzzwords.de/session/observability-data-pipelines-openlineage", "speaker": "Julien Le Dem"}, {"level": "Beginner", "track": "Scale", "abstract": "If you're running a system at scale, you need tools\u00a0to maintain it. This talk gives a high level overview of what observability and monitoring mean, and how to use Prometheus, Loki, Cortex, and Tempo to monitor your stack.", "title": "Observability with Prometheus and beyond", "url": "https://2021.berlinbuzzwords.de/session/observability-prometheus-and-beyond", "speaker": "Myrle Krantz"}, {"level": "Beginner", "track": "Scale", "abstract": "Berlin Buzzwords 2021 kicks off!", "title": "Opening and Welcoming", "url": "https://2021.berlinbuzzwords.de/session/opening-and-welcoming", "speaker": "Nina M\u00fcller"}, {"level": "Beginner", "track": "Scale", "abstract": "Ray is quickly gaining momentum as a distributed computing platform that combines a powerful parallel compute model with a cloud native serverless-style scaling model. Open Data Hub (ODH) is a flexible and customizable federation of open source data science tools that is a great fit for taking advantage of Ray compute clusters.\nIn this talk, Erik will explain how to integrate Ray with Open Data Hub, by configuring ODH profiles that deploy on-demand Ray clusters for Jupyter notebooks. He\u2019ll demonstrate Ray in action as a scalable compute resource for ODH, and explore the potential use cases opened up by self-service notebooks backed by Ray distributed computing. Along the way he\u2019ll also discuss the logistics of adapting Ray to OpenShift\u2019s security features.\nAttendees will learn how Ray integrates with Open Data Hub\u2019s architecture, and how they can power ODH with Ray to solve distributed computing problems in the popular Jupyter environment.", "title": "Powering Open Data Hub with Ray", "url": "https://2021.berlinbuzzwords.de/session/powering-open-data-hub-ray", "speaker": "Erik Erlandson"}, {"level": "Intermediate", "track": "Stream", "abstract": "This presentation will be a lively discussion with hands-on coding to illustrate how to construct a reactive, event-driven data flow pipeline. We will use multiple library implementations of the Reactive Streams specification, such as Akka Streams, Eclipse Vert.x and RxJava. The sample use case will mimic a real-life example of data being collected from multiple distributed sources, which will then be fed to a legacy processor as \u00abwrapped\u00bb by a reactive microservice. The resulting data will flow to a \u00absink\u00bb to be prepared for further processing. We will highlight the strength of the Reactive Streams in controlling backpressure during processing.", "title": "Retrofit Your Java App with a Reactive Flow Pipeline", "url": "https://2021.berlinbuzzwords.de/session/retrofit-your-java-app-reactive-flow-pipeline", "speaker": "Fabio Tiriticco"}, {"level": "Intermediate", "track": "Scale", "abstract": "What is the mortal sin of software engineering? Wasting effort, time, and money building something that is never used.\u00a0\nHow does it happen? There are usually three causes: no product-market fit, overengineering, or lack of trust.\nUnless you run your own startup, there is little you can do about the product-market fit. Similarly, it makes no sense to discourage you from\u00a0\noverengineering at a conference that has \"buzzword\" in the name ;)\u00a0\nLet's focus on the lack of trust.\nHow many of you have built event collection pipelines and dashboards that duplicate features of Google Analytics? What happens when your data differs from values in GA? It does not matter how long you talk about data sampling and eventual consistency. People immediately assume that there is a bug in your pipeline.\nIn this talk, I'll show you how to build trustworthy data products. I start with defining business requirements and involving users. No, it is not an optional step. After that, I talk about ensuring quality during data ingestion and processing.\u00a0\nI admit that it will be a strange tech talk. No buzzwords. No new toys. A little bit of machine learning. Only things that will increase your job satisfaction and let you sleep calmly at night. Nobody wants that, right? ;)", "title": "Scale-up your job satisfaction, not your software", "url": "https://2021.berlinbuzzwords.de/session/scale-your-job-satisfaction-not-your-software", "speaker": "Bartosz Mikulski"}, {"level": "Intermediate", "track": "Scale", "abstract": "Imagine you have a collection made up of several million news stories spanning several years. Your users may want to know how many of yesterday's news stories matched the query \u201cAmerican President.\u201d That\u2019s easy. It's just a normal query to the search engine. However, let\u2019s say they want to get the same number for each day of the last year, or even each day of the last five years. That's a bit harder. Now, if you want to do this in less than a second, then it becomes really challenging. And, what if new data keeps coming into the collection every day and you need to scale to billions of news stories?\nIn this talk, we will show how we achieved responsive time series aggregation across billions of documents using Apache Solr facets. We will discuss how, by optimizing our cache hit-rate\u00a0on complex queries, we successfully reduced latency by a factor of ten (from tens of seconds to under one second) and increased throughput by 60 times (from 10 queries/minute to 10 queries/second).\nWe will review the experiments we followed, show how we used Apache Jmeter to quickly run the right experiments, discuss the data structures we exploited, and look at how we organized our caching policy using Eclipse Memory Analyzer to scale Apache Solr facets to the stars! Get ready to take off! \ud83d\ude80", "title": "Scaling Facets to the Stars \ud83c\udf1f", "url": "https://2021.berlinbuzzwords.de/session/scaling-facets-stars", "speaker": "Shikhar Srivastava"}, {"level": "Intermediate", "track": "Search", "abstract": "Kubernetes\u00a0is fast becoming the operating system for the Cloud and brings a ubiquity that has the potential for massive benefits for technology organizations. Applications/Microservices are moved to orchestration tools like Kubernetes to leverage features like horizontal autoscaling, fault tolerance, CICD, and more.\nApache Solr\u00a0is an open-source search engine platform built on an Apache Lucene library. It offers Apache Lucene's search capabilities in a user-friendly way. Lucidworks Inc runs over a thousand distributed-mode Apache Solr Clusters spread across several machines for a plethora of use-cases around Search and Analytics. The traffic demands a massive scale which creates scenarios of in-depth micro-management like operating systems upgrade,\u00a0scaling cluster dynamically, etc, affecting the overall search experience.\nThis talk is focussed on the intuition\u00a0on addressing scaling clusters horizontally and vertically, on the basis of query traffic load, data ingestion throughput or any other relevant metrics by extending capabilities of Kubernetes and Apache Solr to achieve true physical and logical autoscaling, satisfying modern era SLAs and infrastructure cost. The talk concludes with how the solution discussed opens up the future scope of fine-grained scaling of search clusters.", "title": "Scaling Search Clusters with Apache Solr and Kubernetes", "url": "https://2021.berlinbuzzwords.de/session/scaling-search-clusters-apache-solr-and-kubernetes-1", "speaker": "Amrit Sarkar"}, {"level": "Intermediate", "track": "Search", "abstract": "Search and ranking over datasets which are constantly evolving in real time is a challenging problem at scale. Updating the documents in the index with real time signals like inventory status and click through rates can improve the search experience considerably. The fields which needs to be updated at scale can be used as hard filters as part of the retrieval strategy or as another ranking signal.\u00a0\nIn this talk we\u2019ll present an overview of the real time indexing architecture of Vespa.ai which supports true in-place partial updates of searchable fields, including tensor fields. We also compare the real time indexing architecture of Vespa.ai with search engines built on the Apache Lucene library.", "title": "Search and Sushi; Freshness Counts", "url": "https://2021.berlinbuzzwords.de/session/search-and-sushi-freshness-counts", "speaker": "Jo Kristian Bergum"}, {"level": "Intermediate", "track": "Scale", "abstract": "Should you consume Kafka in a stream OR batch? When should you choose each one? What is more efficient, and cost effective? Should you even care?\nIn this talk we\u2019ll give you the tools and metrics to decide which solution you should apply when, and show you a real life example with cost & time comparisons.\nTo highlight the differences, we\u2019ll dive into a project we\u2019ve done, transitioning from reading Kafka in a stream to reading it in batch.\u00a0\nBy turning conventional thinking on its head and reading our multi-petabyte Kafka stream in batch using Spark and Airflow, we\u2019ve achieved a huge cost reduction of 65% while at the same time getting a more scalable and resilient solution.\nUsing the learnings and statistics we\u2019ve gained, we\u2019ll explore the tradeoffs and give you the metrics and intuition you\u2019ll need to make such decisions yourself.\u00a0\nWe\u2019ll cover:\n\n\tCosts of processing in stream compared to batch\n\t\n\n\tScaling up for bursts and reprocessing\u00a0\n\t\n\n\tMaking the tradeoff between wait times and costs\n\t\n\n\tRecovering from outages\u00a0\n\t\n\n\tAnd much more\u2026", "title": "Should you read Kafka as a stream or in batch? Should you even care?", "url": "https://2021.berlinbuzzwords.de/session/should-you-read-kafka-stream-or-batch-should-you-even-care", "speaker": "Ido Nadler"}, {"level": "Beginner", "track": "Store", "abstract": "Data Engineers face\u00a0many challenges with Data Lakes. GDPR requests, data quality issues, handling large metadata, merges and deletes are a few of the tough challenges usually every Data Engineer encounters with a Data Lake with formats like Parquet, ORC, Avro, etc. This session showcases how you can effortlessly apply updates, upserts and deletes on a Delta Lake table with a very few lines of code and use time travel to go back in time for reproducing experiments & reports very easily, how we can avoid challenges due to small files\u00a0as well.\u00a0Delta Lake was developed by Databricks and has been donated to Linux Foundation, the code for which could be found at http://delta.io. Delta Lake is being used by a huge number of companies across the world due to its advantages for Data Lakes. We will discuss, demo and showcase how Delta Lake can be helpful for your Data Lakes\u00a0because of which many enterprises have Delta Lake as the default data format in their architecture. We will will use SQL or its equivalent Python or Scala API to perform showcase various Delta Lake features.", "title": "Simplifying upserts and deletes on Delta Lake tables", "url": "https://2021.berlinbuzzwords.de/session/simplifying-upserts-and-deletes-delta-lake-tables", "speaker": "Prashanth Babu"}, {"level": "Beginner", "track": "Search", "abstract": "The steps from speech to text are quite simple in theory : you transform the waves into phonemes, then you group them together and decide which has the best probability of representing a meaningful word or phrase based on a dictionary.\nWe often use services available with our devices for this task: Google services if our device is based on Android or you are using Chrome, Apple services if the device is an iPhone, Amazon services if the device is compatible with Alexa and so on. But there are cases where you cannot or do not want to use this type of service. \u00a0\u00a0\nWe tried solving this problem with Elasticsearch. As the final step is searching throughout a dictionary of phonemes and finding the combination that best matches a real phrase, we can easily think of a solution based on an inverted index. In this talk we share our experience with implementing a prototype and give you all the tips and tricks for implementing such a system in your own infrastructure.", "title": "Speech to text with Elasticsearch", "url": "https://2021.berlinbuzzwords.de/session/speech-text-elasticsearch", "speaker": "Lucian Precup"}, {"level": "Beginner", "track": "Stream", "abstract": "Stateful Functions (StateFun), a project developed under the umbrella of Apache Flink, provides consistent messaging and distributed state management for stateful serverless applications. It does so in a vendor, platform and language agnostic manner - applications are composed of inter-messaging, polyglot functions that can be deployed on a mixture of your preferred FaaS platforms, as a Spring Boot application on Kubernetes, or really any deployment method typically used in modern cloud-native architectures.\nIn this session, you will learn about the core concepts behind the project and the abstractions that developers would work with, all up to date to the latest upcoming 3.0 release. For new users, the content of this talk will be a perfect place to get started with StateFun. For existing users, this will be a great opportunity to catch up with the latest advancements in the project, including improved ergonomics around zero-downtime upgrade capabilities of StateFun applications, type system for messages and function state, as well as an extended array of new language SDKs.", "title": "Stateful Functions 3.0: Towards a Cross-Platform Stateful Serverless Stack", "url": "https://2021.berlinbuzzwords.de/session/stateful-functions-30-towards-cross-platform-stateful-serverless-stack", "speaker": "Tzu-Li (Gordon) Tai"}, {"level": "Beginner", "track": "Store", "abstract": "Did you grow up on relational databases? Are document-based databases a bit of a mystery to you? This is the session for you! We\u2019ll compare terms and concepts, explain the benefits of document-based databases, and walk through the 3 key ways you need to change your mindset to successfully use document-based databases.", "title": "From Tables to Documents\u2014Changing Your Database Mindset", "url": "https://2021.berlinbuzzwords.de/session/tables-documents-changing-your-database-mindset", "speaker": "Lauren Schaefer"}, {"level": "Beginner", "track": "Search", "abstract": "Classifying images into Cats or Hotdogs may make for great AI demos, but for many of us, it has limited $DAYJOB uses. What if you have loads of documents, or inconsistently-written text, or a mash of information? Fear not - the latest AI / ML techniques for text can help!\nWith the help of Apache MXNet, scikit-learn, ElasticSearch and friends, we'll progress from a simple text-based ML system, to an advanced system with full linguistic understanding.\u00a0 We'll also cover some key concepts around building AI / ML systems, and some of the pitfalls that beginners risk encountering. Full example code provided!\nAlong the way, we'll look at why text has historically been hard AI / ML, what the latest techniques are, and then Open Source libraries / frameworks implementing them.\nThanks to the magic of cloud-hosted notebooks, you can follow along with the code as we go, and try some live coding if we all dare!", "title": "Taking AI/ML to the next level - Text!", "url": "https://2021.berlinbuzzwords.de/session/taking-aiml-next-level-text", "speaker": "Nick Burch"}, {"level": "Beginner", "track": "Search", "abstract": "As many text mining applications, automatic text categorization is usually implemented with flavors of Machine Learning algorithms, which are trained with an appropriate training set to build the model. This model contains the statistical data based on training set texts that will later allow the system to match an input document with the corresponding category.\u00a0\nBut wait\u2026 statistical data on text documents? Doesn\u2019t it remind you of our dear improved inverted index at the core of Apache Lucene? Maybe we could consider the index containing the training set documents as our trained model? And maybe a simple query against this index could give us the category (or categories) more likely to apply to a given document?\nIn this talk, we will demonstrate this approach and show that it can perform well and possibly \"for free\", as we sure all have Lucene based tools in our application portfolios!", "title": "Text categorization with Apache Lucene", "url": "https://2021.berlinbuzzwords.de/session/text-categorization-apache-lucene", "speaker": "Lucian Precup"}, {"level": "Intermediate", "track": "Search", "abstract": "Anything can be represented by a vector. Text can be represented by vectors describing the text's meaning. Images can be represented by the objects it contains. Users of a system can be represented by their interests and preferences. Even time-based entities such as video, sound, or user interactions can be represented by vectors. Finding the most similar vectors has all kinds of useful applications.\u00a0\nThere are many libraries to choose from for similarity search. However, in real-world applications, there are additional complications that need to be addressed. For instance, similarity search needs to scale up while ensuring that data indexed in the system is searchable immediately without any time-consuming index building in the background. Most importantly, however, additional search filters are often combined with the similarity search. This can severely limit the end result's quality, as post-filtering can prevent otherwise relevant results from surfacing.\u00a0\nIn this talk, we'll explore some real cases where combining approximate nearest neighbors (ANN) search with filtering causes problems. The solution is to integrate the ANN search with filtering, however, most libraries for nearest-neighbor search work in isolation and do not support this. To our knowledge, the only open-source platform that does is Vespa.ai, and we'll delve into how Vespa.ai solves this problem.", "title": "From text search and recommendation to ads and online dating; approximate nearest neighbors in real world applications", "url": "https://2021.berlinbuzzwords.de/session/text-search-and-recommendation-ads-and-online-dating-approximate-nearest-neighbors-real", "speaker": "Lester Solbakken"}, {"level": "Intermediate", "track": "Scale", "abstract": "Since prominent cases of racist and sexist \u201eartifical intelligence\u201c have got medial attention, the term of biased algorithms is more widely discussed. However the term bias sometimes is understood as something that can be overcome easily by gaining more or \u201ebetter\u201c data. I want to argue why the term \u201ebias\u201c and its implication of potentially \u201eunbiased\u201c technology is fundamentally misleading. For that I will connect the concept of model and modelification from computer science with sociological insights from science and technology studies. This illustrates how individual conceptions and necessary limitations of concrete use cases lead to restrictions of the models underlying the further technical development, which are rarely contested later on.\nAfter that, drawing on sociology of technology, I will discuss how technology shapes individual action and society at large in order to stress the importance of evaluating and rethinking technical configurations and the specific biases and shortcomings of the models algorithms and technological are based on.\nFor the illustration of how specific values shape the development of software and the software shapes its users, I will revisit empirical examples of the linux communities Arch, Debian and Ubuntu. As a second case I will discuss model assumptions of the Android permission management which strongly frame possibilities of app developers and the personal privacy management of the users.\nBased on empirical examples the talk illustrates the politics of technology and explains the underlying mechanisms in order to underline the responsibility which follows process of construction.", "title": "There is no such thing as bias-free tech. How models shape our world.", "url": "https://2021.berlinbuzzwords.de/session/there-no-such-thing-bias-free-tech-how-models-shape-our-world", "speaker": "Daniel Guagnin"}, {"level": "Intermediate", "track": "Scale", "abstract": "When throwing more hardware into your Elasticsearch cluster doesn't help and your management tells you that this year the search traffic in our eCommerce site will triple.\u00a0\nIn this talk, I will present several not-so-obvious Elasticsearch performance issues and provide some proven recipes on how to prevent/fix them.\nAlso, I'll share bits of the infrastructure of our search stack that empowered the team to experiment fast without impacting the production and scale Elasticsearch to handle 1M+ RPMs.", "title": "Tips and Tricks to Scale Elasticsearch for 1M RPMs and Beyond", "url": "https://2021.berlinbuzzwords.de/session/tips-and-tricks-scale-elasticsearch-1m-rpms-and-beyond", "speaker": "Dainius Jocas"}, {"level": "Intermediate", "track": "Search", "abstract": "LOVOO's (a social and dating platform) moderation team focuses on keeping the platform free of unwanted users. Our newest solution involves using unsupervised learning to cluster and visualize users' batches within a few clicks to moderate them according to the clustering algorithm's outcome.\n\u00a0\nBehind those clicks, there's a large collection of components supporting the system. These include a machine learning service that creates the model, our whole Antispam platform, Kafka, gRPC, databases, lots of data, and more.\n\u00a0\nThis talk is about the complete platform\u2014 its intention, motivation, architecture, and results.", "title": "Two clicks to cluster and block thousands of spammers", "url": "https://2021.berlinbuzzwords.de/session/two-clicks-cluster-and-block-thousands-spammers", "speaker": "Juan De Dios Santos Rivera"}, {"level": "Intermediate", "track": "Search", "abstract": "Apache Tika is used in big data document processing pipelines to extract text and metadata from numerous file formats. Text extraction is a critical component for search systems.\u00a0 While work on 2.0 has been ongoing for years, the Tika team released 2.0.0-ALPHA in January and will release 2.0.0 before Buzzwords 2021. In addition to dramatically increased modularization, there are new components to improve scaling, integration and robustness. This talk will offer an overview of the changes in Tika 2.0 with a deep dive on the new tika-pipes module that enables synchronous and asynchronous fetching from numerous data sources (jdbc, fileshare, S3), parsing and then emitting to other endpoints (fileshare, S3, Solr, Elasticsearch, etc).", "title": "What's new in Apache Tika 2.0 -- we mean it this time!", "url": "https://2021.berlinbuzzwords.de/session/whats-new-apache-tika-20-we-mean-it-time", "speaker": "Tim Allison"}]