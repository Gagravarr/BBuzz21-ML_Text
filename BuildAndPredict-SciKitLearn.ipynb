{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Setup step - load all our libraries\n",
        "# These are chosen for speed of development and understanding, not performance!\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import csv\n",
        "from collections import namedtuple, defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import metrics\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Try to import things for Notebook display/rendering\n",
        "try:\n",
        "    from IPython.display import display, HTML\n",
        "    notebook = True\n",
        "except ImportError:\n",
        "    notebook = False"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1623927475939
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the JSON files of the talk abstracts\n",
        "# Convert these into Panda DataFrames\n",
        "# If we didn't need to tweak things, could be done with pandas.read_json\n",
        "\n",
        "keys = (\"year\",\"title\",\"level\",\"track\",\"speaker\",\"url\",\"abstract\")\n",
        "years = (2021,2020,2019,2018,2017,2016,2015)\n",
        "\n",
        "data = {}\n",
        "for k in keys:\n",
        "    data[k] = []\n",
        "for year in years:\n",
        "    filename = \"data/%d/sessions.json\" % year\n",
        "    print(\"Loading %s\" % filename)\n",
        "    \n",
        "    with open(filename) as f:\n",
        "       d = json.load(f)\n",
        "       for talk in d:\n",
        "            talk[\"year\"] = year\n",
        "            for k in keys:\n",
        "                data[k].append( talk[k] )\n",
        "                \n",
        "print(\"Loaded %d talks from %d years\" % (len(data[\"year\"]),len(years)))\n",
        "talks = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Build an \"learn\" field, by combining the bits we want to learn on\n",
        "talks[\"learn\"] = talks[\"title\"] + \" \" + talks[\"track\"] + \" \" + talks[\"abstract\"]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# At this point, you might want to explore the data a bit\n",
        "talks.head(5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data/2021/sessions.json\n",
            "Loading data/2020/sessions.json\n",
            "Loading data/2019/sessions.json\n",
            "Loading data/2018/sessions.json\n",
            "Loading data/2017/sessions.json\n",
            "Loading data/2016/sessions.json\n",
            "Loading data/2015/sessions.json\n",
            "Loaded 421 talks from 7 years\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "   year                                              title         level  \\\n0  2021  5 min to discover an open source enterprise se...      Beginner   \n1  2021  The Acceleration of Cloud Engineering and AI -...      Beginner   \n2  2021  Applied MLOps to Maintain Model Freshness on K...  Intermediate   \n3  2021  \"Are You Sure?\": blending product comparisons ...      Advanced   \n4  2021                    Ask Me Anything: Vector Search!           All   \n\n    track            speaker  \\\n0  Search       Cédric Ulmer   \n1   Scale    Grace Francisco   \n2   Scale      Jeff Zemerick   \n3  Search  Patrick John Chia   \n4  Search         Dmitry Kan   \n\n                                                 url  \\\n0  https://2021.berlinbuzzwords.de/session/5-min-...   \n1  https://2021.berlinbuzzwords.de/session/accele...   \n2  https://2021.berlinbuzzwords.de/session/applie...   \n3  https://2021.berlinbuzzwords.de/session/are-yo...   \n4  https://2021.berlinbuzzwords.de/session/ask-me...   \n\n                                            abstract  \\\n0  So your boss wants you to setup a multisources...   \n1  Covid-19 put fuel to rising trends and acceler...   \n2  As machine learning becomes more pervasive acr...   \n3  How do shoppers pick a single product out of t...   \n4  Get to know about vector search and ask Dmitry...   \n\n                                               learn  \n0  5 min to discover an open source enterprise se...  \n1  The Acceleration of Cloud Engineering and AI -...  \n2  Applied MLOps to Maintain Model Freshness on K...  \n3  \"Are You Sure?\": blending product comparisons ...  \n4  Ask Me Anything: Vector Search! Search Get to ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021</td>\n      <td>5 min to discover an open source enterprise se...</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Cédric Ulmer</td>\n      <td>https://2021.berlinbuzzwords.de/session/5-min-...</td>\n      <td>So your boss wants you to setup a multisources...</td>\n      <td>5 min to discover an open source enterprise se...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021</td>\n      <td>The Acceleration of Cloud Engineering and AI -...</td>\n      <td>Beginner</td>\n      <td>Scale</td>\n      <td>Grace Francisco</td>\n      <td>https://2021.berlinbuzzwords.de/session/accele...</td>\n      <td>Covid-19 put fuel to rising trends and acceler...</td>\n      <td>The Acceleration of Cloud Engineering and AI -...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021</td>\n      <td>Applied MLOps to Maintain Model Freshness on K...</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Jeff Zemerick</td>\n      <td>https://2021.berlinbuzzwords.de/session/applie...</td>\n      <td>As machine learning becomes more pervasive acr...</td>\n      <td>Applied MLOps to Maintain Model Freshness on K...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021</td>\n      <td>\"Are You Sure?\": blending product comparisons ...</td>\n      <td>Advanced</td>\n      <td>Search</td>\n      <td>Patrick John Chia</td>\n      <td>https://2021.berlinbuzzwords.de/session/are-yo...</td>\n      <td>How do shoppers pick a single product out of t...</td>\n      <td>\"Are You Sure?\": blending product comparisons ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021</td>\n      <td>Ask Me Anything: Vector Search!</td>\n      <td>All</td>\n      <td>Search</td>\n      <td>Dmitry Kan</td>\n      <td>https://2021.berlinbuzzwords.de/session/ask-me...</td>\n      <td>Get to know about vector search and ask Dmitry...</td>\n      <td>Ask Me Anything: Vector Search! Search Get to ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623927492064
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build two different TF-IDF matricies\n",
        "# One which is word based, one which is character based\n",
        "# Do n-grams for both\n",
        "#  - for words this permits pseudo-phrase-queries\n",
        "#  - for characters this permits pseudo-stemming and typo fixing\n",
        "# Build these over just the \"learn\" combined column\n",
        "# Then generate an inter-document similarity\n",
        "TFIDF = namedtuple('TFIDF', 'tfidf matrix similarities settings')\n",
        "\n",
        "tf_settings_word = dict( analyzer=\"word\",    ngram_range=(1,2) )\n",
        "tf_settings_char = dict( analyzer=\"char_wb\", ngram_range=(3,4) )\n",
        "tf_settings_base = dict( sublinear_tf=True, min_df=0, stop_words='english' )\n",
        "\n",
        "def build_tfidf(tf_settings, text_to_process):\n",
        "    print(\"Building TFI for: %s\" % tf_settings)\n",
        "    tfs = dict(tf_settings)\n",
        "    tfs.update(tf_settings_base)\n",
        "\n",
        "    # Build the TF-IDF over all the talks\n",
        "    # Use title + category + abstract for our text\n",
        "    tfidf = TfidfVectorizer(**tfs)\n",
        "    tfidf_matrix = tfidf.fit_transform(text_to_process)\n",
        "    tfidf_matrix.shape\n",
        "    print(\" - TF-IDF has %d entries\" % tfidf_matrix.shape[1])\n",
        "    \n",
        "    # Build the similarities of each talk against every other talk\n",
        "    # We'll use this for scoring\n",
        "    tfidf_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    # Return this bundle ready for re-use\n",
        "    return TFIDF(tfidf, tfidf_matrix, tfidf_similarities, tfs)\n",
        "\n",
        "tfidf_word = build_tfidf(tf_settings_word, talks[\"learn\"])\n",
        "tfidf_char = build_tfidf(tf_settings_char, talks[\"learn\"])\n",
        "\n",
        "# Let's see what some of our terms are\n",
        "print(tfidf_word.tfidf.get_feature_names()[:10])\n",
        "print(tfidf_char.tfidf.get_feature_names()[:10])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building TFI for: {'analyzer': 'word', 'ngram_range': (1, 2)}\n",
            " - TF-IDF has 39337 entries\n",
            "Building TFI for: {'analyzer': 'char_wb', 'ngram_range': (3, 4)}\n",
            " - TF-IDF has 24164 entries\n",
            "['000', '000 000', '000 client', '000 merchants', '000 predictions', '000 requests', '06', '06 11', '0s', '0s actually']\n",
            "[' ! ', ' \"a', ' \"a ', ' \"ai', ' \"ar', ' \"b', ' \"ba', ' \"be', ' \"bi', ' \"bl']\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623927501506
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, build a ML model for each\n",
        "models = []\n",
        "for tfidf in (tfidf_word, tfidf_char):\n",
        "    print(\"Building model for: %s\" % tfidf.settings)\n",
        "    \n",
        "    # Build a model, using Multinomial Naive Bayse\n",
        "    # Model the text of the talk, to predict the talk's index\n",
        "    classifier = MultinomialNB()\n",
        "    model = make_pipeline(tfidf.tfidf, classifier)\n",
        "    learn_text = talks[\"learn\"]\n",
        "    model.fit( list(learn_text), list(learn_text.index) )\n",
        "\n",
        "    # Save for later predictions\n",
        "    models.append({\n",
        "        \"model\": model,\n",
        "        \"tfidf\": tfidf,\n",
        "        \"classifier\": classifier\n",
        "    })\n",
        "    print(\" - model built!\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Code to recommend talks based on a query\n",
        "# Will return the indexes of the talks, ranked\n",
        "def recommend(query, model_dict, max_hits=10):\n",
        "    # Ask the model to compare our query against every talk,\n",
        "    #  then pick the talk it thinks is the most similar\n",
        "    pred_idx = model_dict[\"model\"].predict([query])\n",
        "\n",
        "    # The prediction should be the index of that talk\n",
        "    print(\"Best match - talk %d\" % pred_idx)\n",
        "    print(talks.iloc[pred_idx,:][\"title\"])\n",
        "    \n",
        "    # Get the pairwise similarity scores of all other talks with that one\n",
        "    # Filter for ones high enough, and sort so highest scores come first\n",
        "    similarities = model_dict[\"tfidf\"].similarities[pred_idx]\n",
        "    sim_scores = list( [idx,s] for idx,s in enumerate(similarities[0]) if s > 0.01 )\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the indexes and scores of the x most similar talks\n",
        "    sim_scores = sim_scores[0:max_hits]\n",
        "    \n",
        "    # Grab those talks\n",
        "    indexes = [i[0] for i in sim_scores]\n",
        "    return talks.iloc[indexes]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Pretty-print our DataFrames\n",
        "def render(df):\n",
        "    if notebook:\n",
        "        display(HTML(df.to_html()))\n",
        "    else:\n",
        "        print(df)\n",
        "\n",
        "# Let's try it!\n",
        "queries = (\"apache tika\", \"ngram\", \"nlp\", \"storm spark\", \"bm25\")\n",
        "for q in queries:\n",
        "    print(\"\")\n",
        "    print(q)\n",
        "    print(\"\")\n",
        "    render( recommend(q, models[0],5) )\n",
        "    print(\"\")\n",
        "    render( recommend(q, models[1],5) )\n",
        "    print(\"\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model for: {'analyzer': 'word', 'ngram_range': (1, 2), 'sublinear_tf': True, 'min_df': 0, 'stop_words': 'english'}\n",
            " - model built!\n",
            "Building model for: {'analyzer': 'char_wb', 'ngram_range': (3, 4), 'sublinear_tf': True, 'min_df': 0, 'stop_words': 'english'}\n",
            " - model built!\n",
            "\n",
            "apache tika\n",
            "\n",
            "Best match - talk 75\n",
            "75    What's new in Apache Tika 2.0 -- we mean it th...\n",
            "Name: title, dtype: object\n",
            "\n",
            "Best match - talk 75\n",
            "75    What's new in Apache Tika 2.0 -- we mean it th...\n",
            "Name: title, dtype: object\n",
            "\n",
            "\n",
            "ngram\n",
            "\n",
            "Best match - talk 0\n",
            "0    5 min to discover an open source enterprise se...\n",
            "Name: title, dtype: object\n",
            "\n",
            "Best match - talk 108\n",
            "108    Opening Session\n",
            "Name: title, dtype: object\n",
            "\n",
            "\n",
            "nlp\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>75</th>\n      <td>2021</td>\n      <td>What's new in Apache Tika 2.0 -- we mean it this time!</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Tim Allison</td>\n      <td>https://2021.berlinbuzzwords.de/session/whats-new-apache-tika-20-we-mean-it-time</td>\n      <td>Apache Tika is used in big data document processing pipelines to extract text and metadata from numerous file formats. Text extraction is a critical component for search systems.  While work on 2.0 has been ongoing for years, the Tika team released 2.0.0-ALPHA in January and will release 2.0.0 before Buzzwords 2021. In addition to dramatically increased modularization, there are new components to improve scaling, integration and robustness. This talk will offer an overview of the changes in Tika 2.0 with a deep dive on the new tika-pipes module that enables synchronous and asynchronous fetching from numerous data sources (jdbc, fileshare, S3), parsing and then emitting to other endpoints (fileshare, S3, Solr, Elasticsearch, etc).</td>\n      <td>What's new in Apache Tika 2.0 -- we mean it this time! Search Apache Tika is used in big data document processing pipelines to extract text and metadata from numerous file formats. Text extraction is a critical component for search systems.  While work on 2.0 has been ongoing for years, the Tika team released 2.0.0-ALPHA in January and will release 2.0.0 before Buzzwords 2021. In addition to dramatically increased modularization, there are new components to improve scaling, integration and robustness. This talk will offer an overview of the changes in Tika 2.0 with a deep dive on the new tika-pipes module that enables synchronous and asynchronous fetching from numerous data sources (jdbc, fileshare, S3), parsing and then emitting to other endpoints (fileshare, S3, Solr, Elasticsearch, etc).</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>2021</td>\n      <td>Opening and Welcoming</td>\n      <td>Beginner</td>\n      <td>Scale</td>\n      <td>Nina Müller</td>\n      <td>https://2021.berlinbuzzwords.de/session/opening-and-welcoming</td>\n      <td>Berlin Buzzwords 2021 kicks off!</td>\n      <td>Opening and Welcoming Scale Berlin Buzzwords 2021 kicks off!</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>2015</td>\n      <td>What's with the 1s and 0s? Making sense of binary data at scale</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Nick Burch</td>\n      <td>https://2015.berlinbuzzwords.de/session/whats-1s-and-0s-making-sense-binary-data-scale-0.html</td>\n      <td>If you have one or two files, you can take the time to manually work out what they are, what they contain, and how to get the useful bits out (probably....). However, this approach really doesn't scale, mechanical turks or no! Luckily, there are open source projects and libraries out there which can help, and which can scale!\\nIn this talk, we'll first look at how we can work out what a given blob of 1s and 0s actually is, be it textual or binary. We'll then see how to extract common metadata from it, along with text, embedded resources, images, and maybe even the kitchen sink! We'll see how to use things like Apache Tika to do this, along with some other libraries to complement it. Once that part's all sorted, we'll look at how to roll this all out for a large-scale Search or Big Data setup, helping you turn those 1s and 0s into useful content at scale!\\n</td>\n      <td>What's with the 1s and 0s? Making sense of binary data at scale Search If you have one or two files, you can take the time to manually work out what they are, what they contain, and how to get the useful bits out (probably....). However, this approach really doesn't scale, mechanical turks or no! Luckily, there are open source projects and libraries out there which can help, and which can scale!\\nIn this talk, we'll first look at how we can work out what a given blob of 1s and 0s actually is, be it textual or binary. We'll then see how to extract common metadata from it, along with text, embedded resources, images, and maybe even the kitchen sink! We'll see how to use things like Apache Tika to do this, along with some other libraries to complement it. Once that part's all sorted, we'll look at how to roll this all out for a large-scale Search or Big Data setup, helping you turn those 1s and 0s into useful content at scale!\\n</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>2018</td>\n      <td>Scalably crashing JVMs, or why binary data to content is hard</td>\n      <td>Beginner</td>\n      <td>Scale</td>\n      <td>Nick Burch</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/scalably-crashing-jvms-or-why-binary-data-content-hard.html</td>\n      <td>Large amounts of unknown data seeks helpful tools to identify itself and generate content! Ideally without crashing too often...\\nWith one or two files, you can take time to manually identify them, and pull out useful content. With thousands of files, or the internet's worth, no amount of mechnical turks will scale this for you! Rolling your own will be slow, and probably crash your JVM... Luckily, there are open source tools and programs out there to help.\\nWe'll start by figuring out why identifying what a given blob of 1s and 0s represents is tricky. Then, we'll see how tools like Apache Tika can help identify, and extract common metadata, text and embedded resources. As we scale out, we'll see how things can go wrong. Finally, we'll see how best to handle Big Data quantities, without crashing your cluster! (Too often...)\\n</td>\n      <td>Scalably crashing JVMs, or why binary data to content is hard Scale Large amounts of unknown data seeks helpful tools to identify itself and generate content! Ideally without crashing too often...\\nWith one or two files, you can take time to manually identify them, and pull out useful content. With thousands of files, or the internet's worth, no amount of mechnical turks will scale this for you! Rolling your own will be slow, and probably crash your JVM... Luckily, there are open source tools and programs out there to help.\\nWe'll start by figuring out why identifying what a given blob of 1s and 0s represents is tricky. Then, we'll see how tools like Apache Tika can help identify, and extract common metadata, text and embedded resources. As we scale out, we'll see how things can go wrong. Finally, we'll see how best to handle Big Data quantities, without crashing your cluster! (Too often...)\\n</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>2021</td>\n      <td>Enterprise Search 101 with Datafari Open Source</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Julien Massiera</td>\n      <td>https://2021.berlinbuzzwords.de/session/enterprise-search-101-datafari-open-source-0</td>\n      <td>Datafari is among the few available open source Enterprise Search solutions.\\nIt can be useful to index your hard drives, your organisation fileshares, but also many other types of sources and formats. Several connectors are available off the shelf (Sharepoint, SMB, DB, web, Alfresco...).\\nSo what is the difference with Solr or openDistro ? It is an additional layer: Datafari is not just the search engine per se, it also embeds the crawling framework, a search UI, and an admin UI.\\nIn this initiation workshop, we will first introduce Datafari and its architecture, and we will then directly dive into a quick install and indexing phase: how to install Datafari Community Edition, create first crawlers and configuration, and how to customise the UI.\\nWith that, you can benefit from Apache Tika, Apache ManifoldCF, Apache Solr, and openDistro without the hassle of configuring and interconnecting all of them.</td>\n      <td>Enterprise Search 101 with Datafari Open Source Search Datafari is among the few available open source Enterprise Search solutions.\\nIt can be useful to index your hard drives, your organisation fileshares, but also many other types of sources and formats. Several connectors are available off the shelf (Sharepoint, SMB, DB, web, Alfresco...).\\nSo what is the difference with Solr or openDistro ? It is an additional layer: Datafari is not just the search engine per se, it also embeds the crawling framework, a search UI, and an admin UI.\\nIn this initiation workshop, we will first introduce Datafari and its architecture, and we will then directly dive into a quick install and indexing phase: how to install Datafari Community Edition, create first crawlers and configuration, and how to customise the UI.\\nWith that, you can benefit from Apache Tika, Apache ManifoldCF, Apache Solr, and openDistro without the hassle of configuring and interconnecting all of them.</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>75</th>\n      <td>2021</td>\n      <td>What's new in Apache Tika 2.0 -- we mean it this time!</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Tim Allison</td>\n      <td>https://2021.berlinbuzzwords.de/session/whats-new-apache-tika-20-we-mean-it-time</td>\n      <td>Apache Tika is used in big data document processing pipelines to extract text and metadata from numerous file formats. Text extraction is a critical component for search systems.  While work on 2.0 has been ongoing for years, the Tika team released 2.0.0-ALPHA in January and will release 2.0.0 before Buzzwords 2021. In addition to dramatically increased modularization, there are new components to improve scaling, integration and robustness. This talk will offer an overview of the changes in Tika 2.0 with a deep dive on the new tika-pipes module that enables synchronous and asynchronous fetching from numerous data sources (jdbc, fileshare, S3), parsing and then emitting to other endpoints (fileshare, S3, Solr, Elasticsearch, etc).</td>\n      <td>What's new in Apache Tika 2.0 -- we mean it this time! Search Apache Tika is used in big data document processing pipelines to extract text and metadata from numerous file formats. Text extraction is a critical component for search systems.  While work on 2.0 has been ongoing for years, the Tika team released 2.0.0-ALPHA in January and will release 2.0.0 before Buzzwords 2021. In addition to dramatically increased modularization, there are new components to improve scaling, integration and robustness. This talk will offer an overview of the changes in Tika 2.0 with a deep dive on the new tika-pipes module that enables synchronous and asynchronous fetching from numerous data sources (jdbc, fileshare, S3), parsing and then emitting to other endpoints (fileshare, S3, Solr, Elasticsearch, etc).</td>\n    </tr>\n    <tr>\n      <th>351</th>\n      <td>2016</td>\n      <td>Event Sourcing in Yammer</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Michal RutkowskiDmitry StratiychukPhilipp Fehre</td>\n      <td>https://2016.berlinbuzzwords.de/session/event-sourcing-yammer.html</td>\n      <td>Event Sourcing brings the promise of highly-scalable and loosely-coupled systems that are performant, reliable, and maintainable. It looked like a perfect solution for Yammer's reliability and performance challenges, but nothing comes for free!\\nOnly slightly over a year ago, Yammer's entire system was either based on synchronous calls or Ruby workers and RabbitMQ. For a while, we have been moving performance-critical components out of the Ruby on Rails monolith toward Dropwizard-based services. This has served us well, but with increased reliability requirements, the pressure to simplify and decouple our system's architecture also increased.\\nOver the course of the last year, we first created a prototype implementation of Event Sourcing and, after validating the idea, have been moving it to a managed Azure Event Hubs based solution. \\nWe are going to cover not only how to migrate to a new technology, but also look at how to change organizational thinking from a synchronous world to one of event streams. We will tell the war stories of our migration from a self-hosted Kafka cluster to a solution based in the cloud. \\n</td>\n      <td>Event Sourcing in Yammer Scale Event Sourcing brings the promise of highly-scalable and loosely-coupled systems that are performant, reliable, and maintainable. It looked like a perfect solution for Yammer's reliability and performance challenges, but nothing comes for free!\\nOnly slightly over a year ago, Yammer's entire system was either based on synchronous calls or Ruby workers and RabbitMQ. For a while, we have been moving performance-critical components out of the Ruby on Rails monolith toward Dropwizard-based services. This has served us well, but with increased reliability requirements, the pressure to simplify and decouple our system's architecture also increased.\\nOver the course of the last year, we first created a prototype implementation of Event Sourcing and, after validating the idea, have been moving it to a managed Azure Event Hubs based solution. \\nWe are going to cover not only how to migrate to a new technology, but also look at how to change organizational thinking from a synchronous world to one of event streams. We will tell the war stories of our migration from a self-hosted Kafka cluster to a solution based in the cloud. \\n</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>2015</td>\n      <td>What's with the 1s and 0s? Making sense of binary data at scale</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Nick Burch</td>\n      <td>https://2015.berlinbuzzwords.de/session/whats-1s-and-0s-making-sense-binary-data-scale-0.html</td>\n      <td>If you have one or two files, you can take the time to manually work out what they are, what they contain, and how to get the useful bits out (probably....). However, this approach really doesn't scale, mechanical turks or no! Luckily, there are open source projects and libraries out there which can help, and which can scale!\\nIn this talk, we'll first look at how we can work out what a given blob of 1s and 0s actually is, be it textual or binary. We'll then see how to extract common metadata from it, along with text, embedded resources, images, and maybe even the kitchen sink! We'll see how to use things like Apache Tika to do this, along with some other libraries to complement it. Once that part's all sorted, we'll look at how to roll this all out for a large-scale Search or Big Data setup, helping you turn those 1s and 0s into useful content at scale!\\n</td>\n      <td>What's with the 1s and 0s? Making sense of binary data at scale Search If you have one or two files, you can take the time to manually work out what they are, what they contain, and how to get the useful bits out (probably....). However, this approach really doesn't scale, mechanical turks or no! Luckily, there are open source projects and libraries out there which can help, and which can scale!\\nIn this talk, we'll first look at how we can work out what a given blob of 1s and 0s actually is, be it textual or binary. We'll then see how to extract common metadata from it, along with text, embedded resources, images, and maybe even the kitchen sink! We'll see how to use things like Apache Tika to do this, along with some other libraries to complement it. Once that part's all sorted, we'll look at how to roll this all out for a large-scale Search or Big Data setup, helping you turn those 1s and 0s into useful content at scale!\\n</td>\n    </tr>\n    <tr>\n      <th>275</th>\n      <td>2017</td>\n      <td>Integration Patterns for Big Data Applications</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Michael Häusler</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/integration-patterns-big-data-applications.html</td>\n      <td>Big Data technologies like distributed databases, queues, batch processors, and stream processors are fun and exciting to play with. Making them play nicely together can be challenging. Keeping it fun for engineers to continuously improve and operate them is hard. At ResearchGate, we run thousands of YARN applications every day to gain insights and to power user facing features. Of course, there are numerous integration challenges on the way:\\nintegrating batch and stream processors with operational systems\\ningesting data and playing back results while controlling performance crosstalk\\nrolling out new versions of synchronous, stream, and batch applications and their respective data schemas\\ncontrolling the amount of glue and adapter code between different technologies\\nmodeling cross-flow dependencies while handling failures gracefully and limiting their repercussions\\n\\nIn this talk we will discuss how ResearchGate has tackled those problems. We describe our ongoing journey in identifying patterns and principles to make our big data stack integrate well. Technologies to be covered will include MongoDB, Kafka, Hadoop (YARN), Hive (TEZ), Flink Batch, and Flink Streaming.\\n</td>\n      <td>Integration Patterns for Big Data Applications Scale Big Data technologies like distributed databases, queues, batch processors, and stream processors are fun and exciting to play with. Making them play nicely together can be challenging. Keeping it fun for engineers to continuously improve and operate them is hard. At ResearchGate, we run thousands of YARN applications every day to gain insights and to power user facing features. Of course, there are numerous integration challenges on the way:\\nintegrating batch and stream processors with operational systems\\ningesting data and playing back results while controlling performance crosstalk\\nrolling out new versions of synchronous, stream, and batch applications and their respective data schemas\\ncontrolling the amount of glue and adapter code between different technologies\\nmodeling cross-flow dependencies while handling failures gracefully and limiting their repercussions\\n\\nIn this talk we will discuss how ResearchGate has tackled those problems. We describe our ongoing journey in identifying patterns and principles to make our big data stack integrate well. Technologies to be covered will include MongoDB, Kafka, Hadoop (YARN), Hive (TEZ), Flink Batch, and Flink Streaming.\\n</td>\n    </tr>\n    <tr>\n      <th>241</th>\n      <td>2018</td>\n      <td>Apache Lucene and Java 9+ — Opportunities and Challenges for Apache Solr and Elasticsearch</td>\n      <td>Advanced</td>\n      <td>Search</td>\n      <td>Uwe Schindler</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/apache-lucene-and-java-9-opportunities-and-challenges-apache-solr-and-elasticsearch.html</td>\n      <td>Java 9, that came out on 21 September 2017, brings a lot of new features and speed improvements, but also many challenging problems for deploying Java applications. Apache Lucene/Solr 7, released two days before Java 9, was thoroughly tested with preview builds of the new Java release and was one of the first applications that were ready to be used with Java 9 from the beginning! Why was the adoption of the new Java 9 release so important for Lucene/Solr and Elasticsearch?\\nIn this talk, Uwe will present some of the speed improvements that Java 9 (and later) brings to Apache Lucene users, so one should upgrade as soon as possible. He will also present what changed behind the scenes to make Apache Lucene compatible to the Java 9 module system (Jigsaw) and how the testing was done. Finally he will present the plans of using Multi-Release JAR files in current Lucene release, so users can still stay with Java 8, but get significantly improved performance when used with later newer Java versions: Java 10 is already there when berlinbuzzwords start!\\n</td>\n      <td>Apache Lucene and Java 9+ — Opportunities and Challenges for Apache Solr and Elasticsearch Search Java 9, that came out on 21 September 2017, brings a lot of new features and speed improvements, but also many challenging problems for deploying Java applications. Apache Lucene/Solr 7, released two days before Java 9, was thoroughly tested with preview builds of the new Java release and was one of the first applications that were ready to be used with Java 9 from the beginning! Why was the adoption of the new Java 9 release so important for Lucene/Solr and Elasticsearch?\\nIn this talk, Uwe will present some of the speed improvements that Java 9 (and later) brings to Apache Lucene users, so one should upgrade as soon as possible. He will also present what changed behind the scenes to make Apache Lucene compatible to the Java 9 module system (Jigsaw) and how the testing was done. Finally he will present the plans of using Multi-Release JAR files in current Lucene release, so users can still stay with Java 8, but get significantly improved performance when used with later newer Java versions: Java 10 is already there when berlinbuzzwords start!\\n</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021</td>\n      <td>5 min to discover an open source enterprise search solution</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Cédric Ulmer</td>\n      <td>https://2021.berlinbuzzwords.de/session/5-min-discover-open-source-enterprise-search-solution</td>\n      <td>So your boss wants you to setup a multisources/multiformats search engine, and he thinks it requires 5 min to do so ? Well, the best we can do, is to show you what Datafari - an open source enterprise search solution - can (and cannot do) in 5 min, and then you will have more food to build a realistic scenario for your boss.</td>\n      <td>5 min to discover an open source enterprise search solution Search So your boss wants you to setup a multisources/multiformats search engine, and he thinks it requires 5 min to do so ? Well, the best we can do, is to show you what Datafari - an open source enterprise search solution - can (and cannot do) in 5 min, and then you will have more food to build a realistic scenario for your boss.</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>2021</td>\n      <td>Enterprise Search 101 with Datafari Open Source</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Julien Massiera</td>\n      <td>https://2021.berlinbuzzwords.de/session/enterprise-search-101-datafari-open-source-0</td>\n      <td>Datafari is among the few available open source Enterprise Search solutions.\\nIt can be useful to index your hard drives, your organisation fileshares, but also many other types of sources and formats. Several connectors are available off the shelf (Sharepoint, SMB, DB, web, Alfresco...).\\nSo what is the difference with Solr or openDistro ? It is an additional layer: Datafari is not just the search engine per se, it also embeds the crawling framework, a search UI, and an admin UI.\\nIn this initiation workshop, we will first introduce Datafari and its architecture, and we will then directly dive into a quick install and indexing phase: how to install Datafari Community Edition, create first crawlers and configuration, and how to customise the UI.\\nWith that, you can benefit from Apache Tika, Apache ManifoldCF, Apache Solr, and openDistro without the hassle of configuring and interconnecting all of them.</td>\n      <td>Enterprise Search 101 with Datafari Open Source Search Datafari is among the few available open source Enterprise Search solutions.\\nIt can be useful to index your hard drives, your organisation fileshares, but also many other types of sources and formats. Several connectors are available off the shelf (Sharepoint, SMB, DB, web, Alfresco...).\\nSo what is the difference with Solr or openDistro ? It is an additional layer: Datafari is not just the search engine per se, it also embeds the crawling framework, a search UI, and an admin UI.\\nIn this initiation workshop, we will first introduce Datafari and its architecture, and we will then directly dive into a quick install and indexing phase: how to install Datafari Community Edition, create first crawlers and configuration, and how to customise the UI.\\nWith that, you can benefit from Apache Tika, Apache ManifoldCF, Apache Solr, and openDistro without the hassle of configuring and interconnecting all of them.</td>\n    </tr>\n    <tr>\n      <th>323</th>\n      <td>2016</td>\n      <td>Community &amp; Commercialization: How to Build an Open Source Company in 2016</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Will Hayes</td>\n      <td>https://2016.berlinbuzzwords.de/session/community-commercialization-how-build-open-source-company-2016.html</td>\n      <td>Many companies tout themselves as “open source,” offering support for popular tools and technologies. But professional services alone don’t typically produce the kind of revenue required for long term business success. So what does it take to build a profitable and healthy open source company? In this session, Will Hayes, CEO of Lucidworks — the company built on and around Apache Lucene and Solr — will share how business success need not come at the expense of the open source community’s integrity. Using Lucidworks as a case study Hayes will explain the power of specificity in driving both revenue and committer happiness, exploring why open source companies should avoid commercializing product features with broad, cross-industry appeal, such as security, management and monitoring. These are fundamental features and monetizing them will harm the integrity of the open source community.\\nHayes will also explore the strategy of going to market with product features that serve a specific industry or use case — for example, with an enterprise search product, ongoing fraud detection for financial services customers — as this will produce business value and prevent community tension.\\n \\n</td>\n      <td>Community &amp; Commercialization: How to Build an Open Source Company in 2016 Search Many companies tout themselves as “open source,” offering support for popular tools and technologies. But professional services alone don’t typically produce the kind of revenue required for long term business success. So what does it take to build a profitable and healthy open source company? In this session, Will Hayes, CEO of Lucidworks — the company built on and around Apache Lucene and Solr — will share how business success need not come at the expense of the open source community’s integrity. Using Lucidworks as a case study Hayes will explain the power of specificity in driving both revenue and committer happiness, exploring why open source companies should avoid commercializing product features with broad, cross-industry appeal, such as security, management and monitoring. These are fundamental features and monetizing them will harm the integrity of the open source community.\\nHayes will also explore the strategy of going to market with product features that serve a specific industry or use case — for example, with an enterprise search product, ongoing fraud detection for financial services customers — as this will produce business value and prevent community tension.\\n \\n</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2019</td>\n      <td>Integrate your Search Engine with a Voice Assistant</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Lucian PrecupMargaux Wagner</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/integrate-your-search-engine-voice-assistant.html</td>\n      <td>The User Experience on a mobile phone is different of that of a tablet and still different of that of a computer screen. You have to adapt your graphical interface, the information displayed but also the calls to your services.\\nWhen the “User Interface” is a “voice”, the User Experience and the services your application provides need to adjust.\\nIn this presentation we will adapt, step by step, an Enterprise Search Engine to Google Assistant and, more generally, to a Virtual Voice Assistant. The challenge will be to find the one and only meaningful result with a minimum of round trips while taking into account all the constraints of this type of User Interaction (among which also: \"being funny with the answers\").\\n</td>\n      <td>Integrate your Search Engine with a Voice Assistant Search The User Experience on a mobile phone is different of that of a tablet and still different of that of a computer screen. You have to adapt your graphical interface, the information displayed but also the calls to your services.\\nWhen the “User Interface” is a “voice”, the User Experience and the services your application provides need to adjust.\\nIn this presentation we will adapt, step by step, an Enterprise Search Engine to Google Assistant and, more generally, to a Virtual Voice Assistant. The challenge will be to find the one and only meaningful result with a minimum of round trips while taking into account all the constraints of this type of User Interaction (among which also: \"being funny with the answers\").\\n</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>2020</td>\n      <td>From commercial search to owned search</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Jesus de los Bueis</td>\n      <td>https://2020.berlinbuzzwords.de/session/commercial-search-owned-search</td>\n      <td>Carrefour started the journey of digital transformation 6 years ago, transforming the e-commerce business into an omnichannel business using an “all in one” e-commerce platform where Search was one more piece of the puzzle.\\n\\nIn time, Carrefour technical team realized that through an open and flexible Search solution, the e-commerce platform could be constructed around it, making Search (and Navigation) the core as opposed to an add-on or an element on the periphery.\\n\\nThis approach, at first, implied several challenges, from strategic to technical ones:\\nWhat options do we have? Use a SaaS solution, build a search from scratch, Base our search in an existing solution, Look for a partner to help us to build it…\\nHow can we keep the intellectual property and the know-how inside the company?\\nWhat experience do we want to offer to our customers?\\nHow can we deal with a huge assortment of products from different natures and stores?\\nWhat insights do we expect to take form the search to help our business?\\nHow do we know whether the search is performing well?\\nWhat strategy do we have to follow for replacement the old search engine keeping, at the same time, the rest of the “all in one” e-commerce platform?\\nHow do we develop it and deploy it into production?\\nHow do we evolve it?</td>\n      <td>From commercial search to owned search Search Carrefour started the journey of digital transformation 6 years ago, transforming the e-commerce business into an omnichannel business using an “all in one” e-commerce platform where Search was one more piece of the puzzle.\\n\\nIn time, Carrefour technical team realized that through an open and flexible Search solution, the e-commerce platform could be constructed around it, making Search (and Navigation) the core as opposed to an add-on or an element on the periphery.\\n\\nThis approach, at first, implied several challenges, from strategic to technical ones:\\nWhat options do we have? Use a SaaS solution, build a search from scratch, Base our search in an existing solution, Look for a partner to help us to build it…\\nHow can we keep the intellectual property and the know-how inside the company?\\nWhat experience do we want to offer to our customers?\\nHow can we deal with a huge assortment of products from different natures and stores?\\nWhat insights do we expect to take form the search to help our business?\\nHow do we know whether the search is performing well?\\nWhat strategy do we have to follow for replacement the old search engine keeping, at the same time, the rest of the “all in one” e-commerce platform?\\nHow do we develop it and deploy it into production?\\nHow do we evolve it?</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>108</th>\n      <td>2020</td>\n      <td>Opening Session</td>\n      <td>All</td>\n      <td>Community</td>\n      <td>Nina Müller</td>\n      <td>https://2020.berlinbuzzwords.de/session/opening-session</td>\n      <td>Welcome to the first Berlin Buzzwords online conference. Learn more about our program structure, networking possibilities and more.</td>\n      <td>Opening Session Community Welcome to the first Berlin Buzzwords online conference. Learn more about our program structure, networking possibilities and more.</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>2021</td>\n      <td>Opening and Welcoming</td>\n      <td>Beginner</td>\n      <td>Scale</td>\n      <td>Nina Müller</td>\n      <td>https://2021.berlinbuzzwords.de/session/opening-and-welcoming</td>\n      <td>Berlin Buzzwords 2021 kicks off!</td>\n      <td>Opening and Welcoming Scale Berlin Buzzwords 2021 kicks off!</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>2020</td>\n      <td>Closing session</td>\n      <td>Beginner</td>\n      <td>Community</td>\n      <td>Nina Müller</td>\n      <td>https://2020.berlinbuzzwords.de/session/closing-session-0</td>\n      <td>Join us for some parting words from the organizers of Berlin Buzzwords, Haystack and MICES.</td>\n      <td>Closing session Community Join us for some parting words from the organizers of Berlin Buzzwords, Haystack and MICES.</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>2020</td>\n      <td>Joint opening session MICES and Haystack</td>\n      <td>All</td>\n      <td>Community</td>\n      <td>René Kriegler</td>\n      <td>https://2020.berlinbuzzwords.de/session/joint-opening-session-mices-and-haystack</td>\n      <td>We welcome you to joint virtual MICES and Haystack conferences 2020.</td>\n      <td>Joint opening session MICES and Haystack Community We welcome you to joint virtual MICES and Haystack conferences 2020.</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>2020</td>\n      <td>Barcamp</td>\n      <td>All</td>\n      <td>Community</td>\n      <td>Varun Thacker</td>\n      <td>https://2020.berlinbuzzwords.de/session/barcamp</td>\n      <td>Each year at Berlin Buzzwords we like to get together on the Sunday before the main conference schedule begins for a Barcamp and this year’s online conference is no exception!Barcamps are informal sessions, a kind of \"un-conference\", with a schedule decided on the day. It is all driven by the interests and expertise of those who attend so each one is different, but ours are always great!\\nAlthough the barcamp doesn't have a strict schedule, it won't be completely devoid of structure! #bbuzz barcamps are dynamic events, focused on the overall Berlin Buzzwords topics, tackling the same challenges but in a different format\\nJoin the discussion at berlinbuzzwords.slack.com</td>\n      <td>Barcamp Community Each year at Berlin Buzzwords we like to get together on the Sunday before the main conference schedule begins for a Barcamp and this year’s online conference is no exception!Barcamps are informal sessions, a kind of \"un-conference\", with a schedule decided on the day. It is all driven by the interests and expertise of those who attend so each one is different, but ours are always great!\\nAlthough the barcamp doesn't have a strict schedule, it won't be completely devoid of structure! #bbuzz barcamps are dynamic events, focused on the overall Berlin Buzzwords topics, tackling the same challenges but in a different format\\nJoin the discussion at berlinbuzzwords.slack.com</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best match - talk 347\n",
            "347    Gain speed and space with NLP in Solr\n",
            "Name: title, dtype: object\n",
            "\n",
            "Best match - talk 347\n",
            "347    Gain speed and space with NLP in Solr\n",
            "Name: title, dtype: object\n",
            "\n",
            "\n",
            "storm spark\n",
            "\n",
            "Best match - talk 420\n",
            "420    Low latency scalable web crawling on Apache Storm\n",
            "Name: title, dtype: object\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>347</th>\n      <td>2016</td>\n      <td>Gain speed and space with NLP in Solr</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Tobias Kässmann</td>\n      <td>https://2016.berlinbuzzwords.de/session/gain-speed-and-space-nlp-solr.html</td>\n      <td>With 100M+ products in a single Solr index it is hard work to keep response times as low as possible. A key factor is decreasing the index size and the number of terms indexed. We try to store the significant terms only. For long product descriptions this is challenging. Applying NLP at index time is a costly operation. In this talk I’ll present a smart algorithm that run fast enough to be applied at index time.\\n</td>\n      <td>Gain speed and space with NLP in Solr Search With 100M+ products in a single Solr index it is hard work to keep response times as low as possible. A key factor is decreasing the index size and the number of terms indexed. We try to store the significant terms only. For long product descriptions this is challenging. Applying NLP at index time is a costly operation. In this talk I’ll present a smart algorithm that run fast enough to be applied at index time.\\n</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>2015</td>\n      <td>Beyond significant terms</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>André Lynum</td>\n      <td>https://2015.berlinbuzzwords.de/session/beyond-significant-terms.html</td>\n      <td>In Comperio we work on projects that aim to learn from the documents and social activity published on the web. The challenge is to summarize recent activity, drawing together current events with past activity and finding new sources to learn from. We base our on Elasticsearch and its significant terms technology. In this presentation we show how we expand on the base functionality provided in Elasticsearch to focus on areas such as immediate trends, entity identification and topic building using additional techniques from Information Retrieval (IR) and Natural Language Processing (NLP).\\n</td>\n      <td>Beyond significant terms Search In Comperio we work on projects that aim to learn from the documents and social activity published on the web. The challenge is to summarize recent activity, drawing together current events with past activity and finding new sources to learn from. We base our on Elasticsearch and its significant terms technology. In this presentation we show how we expand on the base functionality provided in Elasticsearch to focus on areas such as immediate trends, entity identification and topic building using additional techniques from Information Retrieval (IR) and Natural Language Processing (NLP).\\n</td>\n    </tr>\n    <tr>\n      <th>260</th>\n      <td>2017</td>\n      <td>Update on the t-digest: Finding Faults in Real Data</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Ted Dunning</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/update-t-digest-finding-faults-real-data.html</td>\n      <td>Is your system working? Really? Average response times and throughputs don’t tell the whole story. To really understand what is happening, you probably need measurements like the 99.9%-ile response time. A growing number of systems are using the t-digest to do this. I will explain the algorithm with practical examples, talk about how it is much simpler and faster than before, talk about integration in systems like Elastic, Solar and streamlib, tell some real-world deployment stories and show some pretty pictures.</td>\n      <td>Update on the t-digest: Finding Faults in Real Data Search Is your system working? Really? Average response times and throughputs don’t tell the whole story. To really understand what is happening, you probably need measurements like the 99.9%-ile response time. A growing number of systems are using the t-digest to do this. I will explain the algorithm with practical examples, talk about how it is much simpler and faster than before, talk about integration in systems like Elastic, Solar and streamlib, tell some real-world deployment stories and show some pretty pictures.</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>2019</td>\n      <td>E-Commerce search at scale on Apache Lucene (tm)</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Michael SokolovMike McCandless</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/e-commerce-search-scale-apache-lucene-tm.html</td>\n      <td>After many years running its own in-house C++ search engine, Amazon is exploring moving its customer facing e-commerce product search to Apache Lucene (tm), serving millions of customers each day worldwide. Solr, Elasticsearch and other Lucene derivatives have been used widely for many years at Amazon, but until now the .com product search has been powered by a proprietary in-house engine. We'll discuss why we decided to adopt open source for this vital technology and dive deep into the technical challenges we faced in replicating our legacy engine's behavior, pointing out novel uses of Lucene along the way. Highlights will include:\\nOur open-source contributions: concurrent deletions and updates, custom term frequencies, improvement to taxonomy faceting\\nIndex replication on S3: a near-real-time segment-based index replication strategy backed by cheap cloud storage that provides excellent scalability and durability.\\nMerging: fully merging all index segments may be harmful! We'll discuss why our query latencies increase when the index is fully merged.\\nRanking: stringent performance requirements together with complex machine-learned ranking models make for an uneasy marriage. We'll explain how index sorting with early termination combined with multiphase ranking make it possible to have both.\\nOffers/families: how we model offers for a single product, and families with multiple products, using index-time joins.\\nScoring: custom term frequencies based on various machine-learned signals feeding into extensive custom scoring function library including compiled expressions giving performance that is competitive with C++ functions.\\nGarbage collection: we had 8 second stop-the-world pauses and had to dive deep to understand why and correct it. We're using Lucene 7.5.0, JDK11 with the deprecated concurrent garbage collector, CMS. We tried FSTPostingsFormat, then had to revert it. We reduced RAM used by our complex hierarchical index configuration.\\nQuery caching: we tried enabling Lucene's query cache, and it didn't help, but we saw nice gains from indexing commonly occurring sub-queries and optimizing the corresponding clauses at search time.\\n</td>\n      <td>E-Commerce search at scale on Apache Lucene (tm) Search After many years running its own in-house C++ search engine, Amazon is exploring moving its customer facing e-commerce product search to Apache Lucene (tm), serving millions of customers each day worldwide. Solr, Elasticsearch and other Lucene derivatives have been used widely for many years at Amazon, but until now the .com product search has been powered by a proprietary in-house engine. We'll discuss why we decided to adopt open source for this vital technology and dive deep into the technical challenges we faced in replicating our legacy engine's behavior, pointing out novel uses of Lucene along the way. Highlights will include:\\nOur open-source contributions: concurrent deletions and updates, custom term frequencies, improvement to taxonomy faceting\\nIndex replication on S3: a near-real-time segment-based index replication strategy backed by cheap cloud storage that provides excellent scalability and durability.\\nMerging: fully merging all index segments may be harmful! We'll discuss why our query latencies increase when the index is fully merged.\\nRanking: stringent performance requirements together with complex machine-learned ranking models make for an uneasy marriage. We'll explain how index sorting with early termination combined with multiphase ranking make it possible to have both.\\nOffers/families: how we model offers for a single product, and families with multiple products, using index-time joins.\\nScoring: custom term frequencies based on various machine-learned signals feeding into extensive custom scoring function library including compiled expressions giving performance that is competitive with C++ functions.\\nGarbage collection: we had 8 second stop-the-world pauses and had to dive deep to understand why and correct it. We're using Lucene 7.5.0, JDK11 with the deprecated concurrent garbage collector, CMS. We tried FSTPostingsFormat, then had to revert it. We reduced RAM used by our complex hierarchical index configuration.\\nQuery caching: we tried enabling Lucene's query cache, and it didn't help, but we saw nice gains from indexing commonly occurring sub-queries and optimizing the corresponding clauses at search time.\\n</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>2021</td>\n      <td>Search and Sushi; Freshness Counts</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Jo Kristian Bergum</td>\n      <td>https://2021.berlinbuzzwords.de/session/search-and-sushi-freshness-counts</td>\n      <td>Search and ranking over datasets which are constantly evolving in real time is a challenging problem at scale. Updating the documents in the index with real time signals like inventory status and click through rates can improve the search experience considerably. The fields which needs to be updated at scale can be used as hard filters as part of the retrieval strategy or as another ranking signal. \\nIn this talk we’ll present an overview of the real time indexing architecture of Vespa.ai which supports true in-place partial updates of searchable fields, including tensor fields. We also compare the real time indexing architecture of Vespa.ai with search engines built on the Apache Lucene library.</td>\n      <td>Search and Sushi; Freshness Counts Search Search and ranking over datasets which are constantly evolving in real time is a challenging problem at scale. Updating the documents in the index with real time signals like inventory status and click through rates can improve the search experience considerably. The fields which needs to be updated at scale can be used as hard filters as part of the retrieval strategy or as another ranking signal. \\nIn this talk we’ll present an overview of the real time indexing architecture of Vespa.ai which supports true in-place partial updates of searchable fields, including tensor fields. We also compare the real time indexing architecture of Vespa.ai with search engines built on the Apache Lucene library.</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>347</th>\n      <td>2016</td>\n      <td>Gain speed and space with NLP in Solr</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Tobias Kässmann</td>\n      <td>https://2016.berlinbuzzwords.de/session/gain-speed-and-space-nlp-solr.html</td>\n      <td>With 100M+ products in a single Solr index it is hard work to keep response times as low as possible. A key factor is decreasing the index size and the number of terms indexed. We try to store the significant terms only. For long product descriptions this is challenging. Applying NLP at index time is a costly operation. In this talk I’ll present a smart algorithm that run fast enough to be applied at index time.\\n</td>\n      <td>Gain speed and space with NLP in Solr Search With 100M+ products in a single Solr index it is hard work to keep response times as low as possible. A key factor is decreasing the index size and the number of terms indexed. We try to store the significant terms only. For long product descriptions this is challenging. Applying NLP at index time is a costly operation. In this talk I’ll present a smart algorithm that run fast enough to be applied at index time.\\n</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>2018</td>\n      <td>Learned Indexes: a New Idea for Efficient Data Access</td>\n      <td>Intermediate</td>\n      <td>Store</td>\n      <td>Robert Rodger</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/learned-indexes-new-idea-efficient-data-access.html</td>\n      <td>Indexes are what make efficient access for our data storage systems possible. Though traditionally implemented with highly-optimized tree-based data structures, this past December a group from Google proposed a novel idea: replace certain types of index structures with trained machine learning algorithms. After all, an index is nothing other than a model that maps a key to the position of a record; in this light, exchanging, say, your B-tree search with a deep neural network prediction seems at least possible, if not practical. Surprisingly, doing so can often lead to significant performance improvements, in terms of both time and memory consumption.\\nIn this talk we discuss how learned indexes accomplish this. We focus on neural networks, and in particular how recent trends in processor architecture design make them computationally competitive against tree search. We then have a look at how machine learning algorithms can be applied to the task of range indexation, how they can deliver error bound guarantees, and how their accuracy can be honed by layering them recursively. We finish with a review of the Google group's results on three realistic datasets and a brief mention of how machine learning can be applied to other indexation tasks.\\n</td>\n      <td>Learned Indexes: a New Idea for Efficient Data Access Store Indexes are what make efficient access for our data storage systems possible. Though traditionally implemented with highly-optimized tree-based data structures, this past December a group from Google proposed a novel idea: replace certain types of index structures with trained machine learning algorithms. After all, an index is nothing other than a model that maps a key to the position of a record; in this light, exchanging, say, your B-tree search with a deep neural network prediction seems at least possible, if not practical. Surprisingly, doing so can often lead to significant performance improvements, in terms of both time and memory consumption.\\nIn this talk we discuss how learned indexes accomplish this. We focus on neural networks, and in particular how recent trends in processor architecture design make them computationally competitive against tree search. We then have a look at how machine learning algorithms can be applied to the task of range indexation, how they can deliver error bound guarantees, and how their accuracy can be honed by layering them recursively. We finish with a review of the Google group's results on three realistic datasets and a brief mention of how machine learning can be applied to other indexation tasks.\\n</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>2015</td>\n      <td>Designing Concurrent Distributed Sequence Numbers for Elasticsearch</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Boaz Leskes</td>\n      <td>https://2015.berlinbuzzwords.de/session/designing-concurrent-distributed-sequence-numbers-elasticsearch.html</td>\n      <td>Sequence numbers assign a unique increasing number to every document change. They lay the foundations for higher level features such as a changes stream, or bringing a lagging replica up to speed quickly. Implementing them in a distributed system implies dealing with challenges far beyond the capabilities of a simple AtomicLong. They have to be robust enough to deal with problems like faulty servers, networking issues or sudden power outages. On top of that, they need to work in the highly concurrent indexing environment of systems like Elasticsearch.  This talk will take you through the journey of designing such a system.\\nWe will start by explaining the requirements. Then we'll evaluate solutions based on existing consensus algorithms, like ZooKeeper's ZAB and Raft, and why they are (in)sufficient for the task. Next we'll consider some alternate approaches, and finally end up with our proposed solution.\\nYou don't need to be a consensus expert to enjoy this talk. Hopefully, you will leave with a better appreciation of the complexities of distributed systems and be inspired to learn more.\\n</td>\n      <td>Designing Concurrent Distributed Sequence Numbers for Elasticsearch Scale Sequence numbers assign a unique increasing number to every document change. They lay the foundations for higher level features such as a changes stream, or bringing a lagging replica up to speed quickly. Implementing them in a distributed system implies dealing with challenges far beyond the capabilities of a simple AtomicLong. They have to be robust enough to deal with problems like faulty servers, networking issues or sudden power outages. On top of that, they need to work in the highly concurrent indexing environment of systems like Elasticsearch.  This talk will take you through the journey of designing such a system.\\nWe will start by explaining the requirements. Then we'll evaluate solutions based on existing consensus algorithms, like ZooKeeper's ZAB and Raft, and why they are (in)sufficient for the task. Next we'll consider some alternate approaches, and finally end up with our proposed solution.\\nYou don't need to be a consensus expert to enjoy this talk. Hopefully, you will leave with a better appreciation of the complexities of distributed systems and be inspired to learn more.\\n</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>2019</td>\n      <td>Reindexing in Record Time: How Shopify Indexes Over 800,000 Merchants' Data in Under 24 Hours</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Conor Landry</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/reindexing-record-time-how-shopify-indexes-over-800000-merchants-data-under-24-hours.html</td>\n      <td>Chances are, if you have shopped online, you’ve searched for the item you want to buy, placed that item in your digital cart, paid for that item, and had it delivered in record time. Each of those steps you took to enjoy your shiny, new item wouldn’t be possible without the help of search engines. Search engines help us find products, help merchants confirm your order, and ship it on time. How do we initially get all this data from slower, traditional databases into fast search engines?\\nIn this talk, Conor Landry focuses on how Shopify indexes product, customer, order, and merchant data from MySQL to Elasticsearch in near real-time and how to reindex over 50 terabytes of data in less than 24 hours and the roadblocks we’ve encountered. Conor describes the challenges faced when handling data which is critical to the livelihoods of small business owners and well-known brands as well as strategies used by Shopify when scaling a search indexation system for the long term.\\n \\n</td>\n      <td>Reindexing in Record Time: How Shopify Indexes Over 800,000 Merchants' Data in Under 24 Hours Search Chances are, if you have shopped online, you’ve searched for the item you want to buy, placed that item in your digital cart, paid for that item, and had it delivered in record time. Each of those steps you took to enjoy your shiny, new item wouldn’t be possible without the help of search engines. Search engines help us find products, help merchants confirm your order, and ship it on time. How do we initially get all this data from slower, traditional databases into fast search engines?\\nIn this talk, Conor Landry focuses on how Shopify indexes product, customer, order, and merchant data from MySQL to Elasticsearch in near real-time and how to reindex over 50 terabytes of data in less than 24 hours and the roadblocks we’ve encountered. Conor describes the challenges faced when handling data which is critical to the livelihoods of small business owners and well-known brands as well as strategies used by Shopify when scaling a search indexation system for the long term.\\n \\n</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>2016</td>\n      <td>Learning the learner: Using machine learning to  track performance of machine learning algorithms</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Ira Cohen</td>\n      <td>https://2016.berlinbuzzwords.de/session/learning-learner-using-machine-learning-track-performance-machine-learning-algorithms.html</td>\n      <td>So you’ve created some machine learning algorithms and tested them out in the lab, and they seem to be working fine. But how can you monitor them in production, especially if they are constantly learning and updating, and you have many of them? This was the challenge we faced at Anodot and I’ll talk about the interesting way we solved it. \\nAt Anodot, we have approximately 30 different types of machine learning algorithms, each one with its own parameters and tuning capabilities, designed to provide real time anomaly detection. Many of these are online learning algorithms, which get updated with every new piece of data that arrives. Adding to the complexity, the outputs of some of the algorithms act as the inputs others. These algorithms run constantly on the vast number of signals that are sent to our SaaS cloud (currently more than 35 million signals are reported to Anodot every 1 to 5 minutes). We knew from day one that it was crucial to track the performance of these algorithms, so we would know if something happened that improved or degraded their performance, but we were faced with a challenge – how to accomplish this?  \\nFirst, we collect time series metrics that constantly measure various performance indicators for each of the algorithms. We measure the number of anomalies we discover for each customer, their score distribution, the number of seasonal patterns discovered, classification changes and rates between of the various model selection algorithms, number of clusters and their quality from our various clustering algorithms, and much more.\\nBut, manual tracking of changes in these algorithm performance metrics is not feasible - there are too many models/algorithms to track manually (even with dashboards/reports). So, we discovered that by applying our anomaly detection algorithms to track these metrics, we quickly discover any change in their behaviour and see their abnormal patterns. When we get alerts on these abnormal changes, we  determine if it was because of some algorithm tuning, or perhaps it is a valid shift. \\nIn the talk I'll show multiple examples of how this approach helped us detect, fix and eventually design better learning algorithms. I'll also describe the general methodology for \"learning the learner\".\\n</td>\n      <td>Learning the learner: Using machine learning to  track performance of machine learning algorithms Scale So you’ve created some machine learning algorithms and tested them out in the lab, and they seem to be working fine. But how can you monitor them in production, especially if they are constantly learning and updating, and you have many of them? This was the challenge we faced at Anodot and I’ll talk about the interesting way we solved it. \\nAt Anodot, we have approximately 30 different types of machine learning algorithms, each one with its own parameters and tuning capabilities, designed to provide real time anomaly detection. Many of these are online learning algorithms, which get updated with every new piece of data that arrives. Adding to the complexity, the outputs of some of the algorithms act as the inputs others. These algorithms run constantly on the vast number of signals that are sent to our SaaS cloud (currently more than 35 million signals are reported to Anodot every 1 to 5 minutes). We knew from day one that it was crucial to track the performance of these algorithms, so we would know if something happened that improved or degraded their performance, but we were faced with a challenge – how to accomplish this?  \\nFirst, we collect time series metrics that constantly measure various performance indicators for each of the algorithms. We measure the number of anomalies we discover for each customer, their score distribution, the number of seasonal patterns discovered, classification changes and rates between of the various model selection algorithms, number of clusters and their quality from our various clustering algorithms, and much more.\\nBut, manual tracking of changes in these algorithm performance metrics is not feasible - there are too many models/algorithms to track manually (even with dashboards/reports). So, we discovered that by applying our anomaly detection algorithms to track these metrics, we quickly discover any change in their behaviour and see their abnormal patterns. When we get alerts on these abnormal changes, we  determine if it was because of some algorithm tuning, or perhaps it is a valid shift. \\nIn the talk I'll show multiple examples of how this approach helped us detect, fix and eventually design better learning algorithms. I'll also describe the general methodology for \"learning the learner\".\\n</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>420</th>\n      <td>2015</td>\n      <td>Low latency scalable web crawling on Apache Storm</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Julien Nioche</td>\n      <td>https://2015.berlinbuzzwords.de/session/low-latency-scalable-web-crawling-apache-storm.html</td>\n      <td>In this talk I will introduce Storm-Crawler https://github.com/DigitalPebble/storm-crawler, a collection of resources for building low-latency, large scale web crawlers on Apache Storm. We will compare with similar projects like Apache Nutch and present several use cases where the storm-crawler is being used.  In particular we will see how the Storm-crawler can be used with ElasticSearch and Kibana for crawling and indexing web pages.\\n</td>\n      <td>Low latency scalable web crawling on Apache Storm Search In this talk I will introduce Storm-Crawler https://github.com/DigitalPebble/storm-crawler, a collection of resources for building low-latency, large scale web crawlers on Apache Storm. We will compare with similar projects like Apache Nutch and present several use cases where the storm-crawler is being used.  In particular we will see how the Storm-crawler can be used with ElasticSearch and Kibana for crawling and indexing web pages.\\n</td>\n    </tr>\n    <tr>\n      <th>346</th>\n      <td>2016</td>\n      <td>Real Time Marketing with Kafka, Storm, Cassandra and a pinch of Spark</td>\n      <td>Beginner</td>\n      <td>Scale</td>\n      <td>Volker Janz</td>\n      <td>https://2016.berlinbuzzwords.de/session/real-time-marketing-kafka-storm-cassandra-and-pinch-spark.html</td>\n      <td>The combination of Apache Kafka as a event bus, Apache Storm for real- or neartime processing, Apache Cassandra as an operational storage layer as well as Apache Spark to perform analytical queries against this storage turned out to be a extremely well performing system.\\nWith increasing marketing costs per registration, it is even more important to keep players within the game as well as provide them with attractive offers aiming to increase the customer lifetime value and also create a better game experience.\\nTo that end, we introduced interstitials that offer premium features or discounts for the player at InnoGames. Even though this is already a useful instrument, we aimed to customize those interstitials according to the behavior of the player. Therefore, we created a system that works with generic messages that contain data about user interactions, in real- or neartime -- later referred to as events. The system builds up a player profile that contains all game-relevant information about the players in a central location.\\nThe system is also able to react to events, fetch information about the corresponding player from the profile in a matter of milliseconds and send out an interstitial based on this information.\\nThe system consists of an Apache Kafka component used as an event bus, an Apache Storm topology to update the profile and trigger marketing actions based on events as well as an Apache Cassandra cluster which serves as a storage layer for the profile. In addition, we set up a Apache Spark cluster along Cassandra to run analytical queries against the data in Cassandra based on this article from DataStax (https://academy.datastax.com/demos/getting-started-apache-spark-and-cassandra). This combination is quite efficient but additionally we added a Spark REST JobServer (https://github.com/spark-jobserver/spark-jobserver) and extended it so that we can read the player profile from Cassandra, cache it within the Spark context and reuse this context for several jobs. This increases the performance for analytical queries significantly.\\nThe whole stack combines the possibility of fast event-based operations along with powerful analytical queries using Spark DataFrames. This concept can therefore not only be applied to a marketing system like the one we built, but also to a variety of different use cases. Another key technology used within the system is the Nashorn JavaScript engine included with Java 8. It is used within Storm to work with the player profile and the incoming events. In this way we are able to define new marketing action at runtime and have a very flexible and generic data processing bolt within our Storm topology.\\n</td>\n      <td>Real Time Marketing with Kafka, Storm, Cassandra and a pinch of Spark Scale The combination of Apache Kafka as a event bus, Apache Storm for real- or neartime processing, Apache Cassandra as an operational storage layer as well as Apache Spark to perform analytical queries against this storage turned out to be a extremely well performing system.\\nWith increasing marketing costs per registration, it is even more important to keep players within the game as well as provide them with attractive offers aiming to increase the customer lifetime value and also create a better game experience.\\nTo that end, we introduced interstitials that offer premium features or discounts for the player at InnoGames. Even though this is already a useful instrument, we aimed to customize those interstitials according to the behavior of the player. Therefore, we created a system that works with generic messages that contain data about user interactions, in real- or neartime -- later referred to as events. The system builds up a player profile that contains all game-relevant information about the players in a central location.\\nThe system is also able to react to events, fetch information about the corresponding player from the profile in a matter of milliseconds and send out an interstitial based on this information.\\nThe system consists of an Apache Kafka component used as an event bus, an Apache Storm topology to update the profile and trigger marketing actions based on events as well as an Apache Cassandra cluster which serves as a storage layer for the profile. In addition, we set up a Apache Spark cluster along Cassandra to run analytical queries against the data in Cassandra based on this article from DataStax (https://academy.datastax.com/demos/getting-started-apache-spark-and-cassandra). This combination is quite efficient but additionally we added a Spark REST JobServer (https://github.com/spark-jobserver/spark-jobserver) and extended it so that we can read the player profile from Cassandra, cache it within the Spark context and reuse this context for several jobs. This increases the performance for analytical queries significantly.\\nThe whole stack combines the possibility of fast event-based operations along with powerful analytical queries using Spark DataFrames. This concept can therefore not only be applied to a marketing system like the one we built, but also to a variety of different use cases. Another key technology used within the system is the Nashorn JavaScript engine included with Java 8. It is used within Storm to work with the player profile and the incoming events. In this way we are able to define new marketing action at runtime and have a very flexible and generic data processing bolt within our Storm topology.\\n</td>\n    </tr>\n    <tr>\n      <th>327</th>\n      <td>2016</td>\n      <td>Introducing Kafka Streams, the new stream processing library of Apache Kafka</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Michael Noll</td>\n      <td>https://2016.berlinbuzzwords.de/session/introducing-kafka-streams-new-stream-processing-library-apache-kafka.html</td>\n      <td>In the past few years Apache Kafka has established itself as the world's most popular real-time, large-scale messaging system. It is used across a wide range of industries by thousands of companies such as Netflix, Cisco, PayPal, Twitter, and many others.\\nIn this session I am introducing the audience to Kafka Streams, which is the latest addition to the Apache Kafka project.  Kafka Streams is a stream processing library natively integrated with Kafka. It has a very low barrier to entry, easy operationalization, and a high-level DSL for writing stream processing applications. As such it is the most convenient yet scalable option to process and analyze data that is backed by Kafka.  We will provide the audience with an overview of Kafka Streams including its design and API, typical use cases, code examples, and an outlook of its upcoming roadmap.  We will also compare Kafka Streams' light-weight library approach with heavier, framework-based tools such as Apache Storm and Spark Streaming, which require you to understand and operate a whole different infrastructure for processing real-time data in Kafka.\\n</td>\n      <td>Introducing Kafka Streams, the new stream processing library of Apache Kafka Scale In the past few years Apache Kafka has established itself as the world's most popular real-time, large-scale messaging system. It is used across a wide range of industries by thousands of companies such as Netflix, Cisco, PayPal, Twitter, and many others.\\nIn this session I am introducing the audience to Kafka Streams, which is the latest addition to the Apache Kafka project.  Kafka Streams is a stream processing library natively integrated with Kafka. It has a very low barrier to entry, easy operationalization, and a high-level DSL for writing stream processing applications. As such it is the most convenient yet scalable option to process and analyze data that is backed by Kafka.  We will provide the audience with an overview of Kafka Streams including its design and API, typical use cases, code examples, and an outlook of its upcoming roadmap.  We will also compare Kafka Streams' light-weight library approach with heavier, framework-based tools such as Apache Storm and Spark Streaming, which require you to understand and operate a whole different infrastructure for processing real-time data in Kafka.\\n</td>\n    </tr>\n    <tr>\n      <th>359</th>\n      <td>2016</td>\n      <td>Help I need a stream processor - learning how to chose between Spark, Flink, Samza, and Storm</td>\n      <td>Advanced</td>\n      <td>Scale</td>\n      <td>Andrew Psaltis</td>\n      <td>https://2016.berlinbuzzwords.de/session/help-i-need-stream-processor-learning-how-chose-between-spark-flink-samza-and-storm.html</td>\n      <td>Today if a byte of data were a gallon of water, in only 10 seconds there would be enough data to fill an average home, in 2020 it will only take 2 seconds. With this explosive growth comes the demand from consumers and businesses to leverage and act on what is happening right now. Without stream processing these demands will never be met, and there will be no big data and no Internet of Things. It is only a matter of time before you will be faced with building a real-time streaming pipeline. As soon as you embark on this journey, you will be faced with a myriad of questions. A major key decision you will need to quickly answer is which stream-processing framework should you use? When you survey the landscape you will find many contenders. In this session we will focus on the most popular open source frameworks, in particular: Apache Spark Streaming, Apache Storm, Apache Flink, and Apache Samza. We will dive into each of these tools and tease out all of the essential pieces you need to consider, compare and contrast them and end up with an understanding of how to evaluate each as well as future products. \\n</td>\n      <td>Help I need a stream processor - learning how to chose between Spark, Flink, Samza, and Storm Scale Today if a byte of data were a gallon of water, in only 10 seconds there would be enough data to fill an average home, in 2020 it will only take 2 seconds. With this explosive growth comes the demand from consumers and businesses to leverage and act on what is happening right now. Without stream processing these demands will never be met, and there will be no big data and no Internet of Things. It is only a matter of time before you will be faced with building a real-time streaming pipeline. As soon as you embark on this journey, you will be faced with a myriad of questions. A major key decision you will need to quickly answer is which stream-processing framework should you use? When you survey the landscape you will find many contenders. In this session we will focus on the most popular open source frameworks, in particular: Apache Spark Streaming, Apache Storm, Apache Flink, and Apache Samza. We will dive into each of these tools and tease out all of the essential pieces you need to consider, compare and contrast them and end up with an understanding of how to evaluate each as well as future products. \\n</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>2015</td>\n      <td>Designing NRT(NearRealTime) stream processing systems: Using Storm</td>\n      <td>Beginner</td>\n      <td>Scale</td>\n      <td>Konark Modi</td>\n      <td>https://2015.berlinbuzzwords.de/session/designing-nrtnearrealtime-stream-processing-systems-using-storm.html</td>\n      <td>The essence of near-real-time stream processing is to compute huge volumes of data as it is received. This talk will focus on creating a pipeline for collecting huge volumes of data anfd processing near-real time using Storm. \\nStorm is a high-volume, continuous, reliable stream processing system developed at BackType and open-sourced by Twitter. Storm is being widely used in lot of organizations and has variety of uses-cases like:    \\n* Realtime analytics     \\n* Distributed RPC     \\n* ETL etc.   \\n \\n  During the course of 40 minutes using an example of Real-time Wikipedia edit we will try and understand:   \\n * Basic concepts of stream-processing.  \\n   * High level understanding of components involved in Storm.     \\n* Writing producer in Python which will will push in Queue the real-time edit feed from Wikipedia.     \\n * Write storm topologies in python to consume feed and process real-time metrics like:           \\n      * Number of articles edited.       \\n      * Category wise count of articles being edited.      \\n      * Distinct people editing the articles       \\n      * GeoLocation counters etc.   \\n * Technological challenges revolving around near-real time stream processing systems:    \\n * Achieve low latency for processing as compared to batch processing.      \\n * State-management in workers to maintain aggregated counts like counting edits for same category of articles.         \\n * Handling failures and crashes\\n * Deployment Startergies.    \\n \\n</td>\n      <td>Designing NRT(NearRealTime) stream processing systems: Using Storm Scale The essence of near-real-time stream processing is to compute huge volumes of data as it is received. This talk will focus on creating a pipeline for collecting huge volumes of data anfd processing near-real time using Storm. \\nStorm is a high-volume, continuous, reliable stream processing system developed at BackType and open-sourced by Twitter. Storm is being widely used in lot of organizations and has variety of uses-cases like:    \\n* Realtime analytics     \\n* Distributed RPC     \\n* ETL etc.   \\n \\n  During the course of 40 minutes using an example of Real-time Wikipedia edit we will try and understand:   \\n * Basic concepts of stream-processing.  \\n   * High level understanding of components involved in Storm.     \\n* Writing producer in Python which will will push in Queue the real-time edit feed from Wikipedia.     \\n * Write storm topologies in python to consume feed and process real-time metrics like:           \\n      * Number of articles edited.       \\n      * Category wise count of articles being edited.      \\n      * Distinct people editing the articles       \\n      * GeoLocation counters etc.   \\n * Technological challenges revolving around near-real time stream processing systems:    \\n * Achieve low latency for processing as compared to batch processing.      \\n * State-management in workers to maintain aggregated counts like counting edits for same category of articles.         \\n * Handling failures and crashes\\n * Deployment Startergies.    \\n \\n</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best match - talk 312\n",
            "312    Apache Spark? If only it worked.\n",
            "Name: title, dtype: object\n",
            "\n",
            "\n",
            "bm25\n",
            "\n",
            "Best match - talk 342\n",
            "342    BM25 demystified\n",
            "Name: title, dtype: object\n",
            "\n",
            "Best match - talk 342\n",
            "342    BM25 demystified\n",
            "Name: title, dtype: object\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>312</th>\n      <td>2017</td>\n      <td>Apache Spark? If only it worked.</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Marcin Szymaniuk</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/apache-spark-if-only-it-worked.html</td>\n      <td>Do you have plans to start working with Apache Spark? Are you already working with Spark but you haven’t gotten the expected performance and stability and you are not sure where to look for a fix?\\nSpark has a very nice API and it promises high performance for crunching large datasets. It’s really easy to write an app in Spark, unfortunately, it’s also easy to write one which doesn’t perform the way you would expect or just fails for no obvious reason.\\nThis talk will consist of multiple common problems you might face when running Spark at full scale and, of course, solutions for solving them. Each of the problems I will cover will come with well-described background and examples so that it will be understood by people with no Spark experience. However, people who are working with Spark are the main audience. The ultimate objective is to give the audience a practical framework for optimizing the most common problems with Spark applications.\\nClasses of problems in the presentation:\\nDealing with skewed data\\nSpark on YARN and its memory model\\nCaching\\nSizing executors\\nLocality\\n</td>\n      <td>Apache Spark? If only it worked. Scale Do you have plans to start working with Apache Spark? Are you already working with Spark but you haven’t gotten the expected performance and stability and you are not sure where to look for a fix?\\nSpark has a very nice API and it promises high performance for crunching large datasets. It’s really easy to write an app in Spark, unfortunately, it’s also easy to write one which doesn’t perform the way you would expect or just fails for no obvious reason.\\nThis talk will consist of multiple common problems you might face when running Spark at full scale and, of course, solutions for solving them. Each of the problems I will cover will come with well-described background and examples so that it will be understood by people with no Spark experience. However, people who are working with Spark are the main audience. The ultimate objective is to give the audience a practical framework for optimizing the most common problems with Spark applications.\\nClasses of problems in the presentation:\\nDealing with skewed data\\nSpark on YARN and its memory model\\nCaching\\nSizing executors\\nLocality\\n</td>\n    </tr>\n    <tr>\n      <th>225</th>\n      <td>2018</td>\n      <td>DataFrames in Spark - the analysts perspective.</td>\n      <td>Beginner</td>\n      <td>Scale</td>\n      <td>Marcin Szymaniuk</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/dataframes-spark-analysts-perspective.html</td>\n      <td>Are you a data analyst who works with Spark and often gets confused by failures you don’t understand? Have you seen a bunch of presentations or blog posts about Spark performance but you are still not certain how to apply the hints you have been given in practice?\\nSpark is commonly used by people who are not experts in programming but they know SQL and sometimes basic Python. They treat Spark as a tool for getting business value from the the data. And that is how it should be! Although it’s common that queries they run do not work for any obvious reason. This talk is designed for such Spark users and will be focused on common problems with Spark (especially DataFrames and SQL) which can be solved by anyone familiar with SQL. You don’t need to read bytecode to understand the techniques presented and apply them in practice!\\nThis talk will be a case study of multiple DataFrame queries in Spark which initially do not work. I will not only explain how to fix them, but we will go through the solution step-by-step so you will learn what to pay attention to and how to apply similar techniques to your codebase!\\n</td>\n      <td>DataFrames in Spark - the analysts perspective. Scale Are you a data analyst who works with Spark and often gets confused by failures you don’t understand? Have you seen a bunch of presentations or blog posts about Spark performance but you are still not certain how to apply the hints you have been given in practice?\\nSpark is commonly used by people who are not experts in programming but they know SQL and sometimes basic Python. They treat Spark as a tool for getting business value from the the data. And that is how it should be! Although it’s common that queries they run do not work for any obvious reason. This talk is designed for such Spark users and will be focused on common problems with Spark (especially DataFrames and SQL) which can be solved by anyone familiar with SQL. You don’t need to read bytecode to understand the techniques presented and apply them in practice!\\nThis talk will be a case study of multiple DataFrame queries in Spark which initially do not work. I will not only explain how to fix them, but we will go through the solution step-by-step so you will learn what to pay attention to and how to apply similar techniques to your codebase!\\n</td>\n    </tr>\n    <tr>\n      <th>362</th>\n      <td>2016</td>\n      <td>MapReduce is not dead, it just smells funny!</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Christoph Schmitz</td>\n      <td>https://2016.berlinbuzzwords.de/session/mapreduce-not-dead-it-just-smells-funny.html</td>\n      <td>Following the news from the big data world, you might get the impression that Java MapReduce is way past its prime and newer frameworks such as Spark or Flink are the way to go, no questions asked.\\nHaving run big data workloads in a production environment on some of the largest web portals in Germany since 2007, I will argue that in many use cases, from an implementation or performance standpoint the actual choice of parallelization framework does not matter as much as you might think. Understanding your application domain, implementing a sound domain model, and optimizing your data flows based on that domain knowledge will be much more effective when trying to improve performance or write more elegant code than switching to the latest distributed computing framework.\\nIn this session, I will discuss use-cases from our production systems and show how business logic is implemented that is efficient, yet agnostic of the actual computing framework. I will demonstrate how it interacts with MapReduce, how other frameworks such as Spark would work in its stead, and how domain-specific optimizations can work hand in hand with the computing framework of choice.\\n</td>\n      <td>MapReduce is not dead, it just smells funny! Scale Following the news from the big data world, you might get the impression that Java MapReduce is way past its prime and newer frameworks such as Spark or Flink are the way to go, no questions asked.\\nHaving run big data workloads in a production environment on some of the largest web portals in Germany since 2007, I will argue that in many use cases, from an implementation or performance standpoint the actual choice of parallelization framework does not matter as much as you might think. Understanding your application domain, implementing a sound domain model, and optimizing your data flows based on that domain knowledge will be much more effective when trying to improve performance or write more elegant code than switching to the latest distributed computing framework.\\nIn this session, I will discuss use-cases from our production systems and show how business logic is implemented that is efficient, yet agnostic of the actual computing framework. I will demonstrate how it interacts with MapReduce, how other frameworks such as Spark would work in its stead, and how domain-specific optimizations can work hand in hand with the computing framework of choice.\\n</td>\n    </tr>\n    <tr>\n      <th>261</th>\n      <td>2017</td>\n      <td>The Shape of Revolutions: What Makes a Difference?</td>\n      <td>Beginner</td>\n      <td>Stream</td>\n      <td>Ellen Friedman</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/shape-revolutions-what-makes-difference.html</td>\n      <td>There’s a revolution underway in how people work with data. Streaming data is no longer seen as a special use case – and that’s a good thing because streaming is a better fit to the way life happens. Innovative technologies for robust stream processing are changing what you can reasonably expect to do with stream-based applications, particularly when low latency is required. Apache Flink is one such emerging technology, and its popularity is growing.\\nInnovation by those who design and develop new technologies is, however, just one-half of an effective data revolution. It’s not just the people who build the disruptive technologies who must have vision if significant change is to occur: the users of those new technologies must also have vision if the revolution is to have real impact.\\nUsing specific examples from a variety of projects involving streaming data, this talk focuses on how innovation with real impact can happen, including the shift in thinking and in engineering culture that underlie successful change in how we work with data at scale. For instance, people are beginning to recognize that stream-first architectures are useful even beyond real-time processing.  Another big idea has to do with where data lives: the big data revolution showed us that data structures spanning more than one machine are a good thing -- now a new revolution involves data structures that span more than one continent (geo-distribution) and that go from on-premise to cloud.\\nWe’ll also look at how streaming data supports flexible practices such as a microservices style that have huge implications in IoT, in A/B testing, deployment of machine learning models and other large-scale analytical workflows. Finally we will how the right technologies and the right design can make life easier for developers and for system administrators by creating a separation of concerns.\\nThis talk should be useful for audiences at all levels of experience.</td>\n      <td>The Shape of Revolutions: What Makes a Difference? Stream There’s a revolution underway in how people work with data. Streaming data is no longer seen as a special use case – and that’s a good thing because streaming is a better fit to the way life happens. Innovative technologies for robust stream processing are changing what you can reasonably expect to do with stream-based applications, particularly when low latency is required. Apache Flink is one such emerging technology, and its popularity is growing.\\nInnovation by those who design and develop new technologies is, however, just one-half of an effective data revolution. It’s not just the people who build the disruptive technologies who must have vision if significant change is to occur: the users of those new technologies must also have vision if the revolution is to have real impact.\\nUsing specific examples from a variety of projects involving streaming data, this talk focuses on how innovation with real impact can happen, including the shift in thinking and in engineering culture that underlie successful change in how we work with data at scale. For instance, people are beginning to recognize that stream-first architectures are useful even beyond real-time processing.  Another big idea has to do with where data lives: the big data revolution showed us that data structures spanning more than one machine are a good thing -- now a new revolution involves data structures that span more than one continent (geo-distribution) and that go from on-premise to cloud.\\nWe’ll also look at how streaming data supports flexible practices such as a microservices style that have huge implications in IoT, in A/B testing, deployment of machine learning models and other large-scale analytical workflows. Finally we will how the right technologies and the right design can make life easier for developers and for system administrators by creating a separation of concerns.\\nThis talk should be useful for audiences at all levels of experience.</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>2018</td>\n      <td>Fast Access To Your Complex Data - Avro, JSON, ORC, and Parquet</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Owen O'Malley</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/fast-access-your-complex-data-avro-json-orc-and-parquet.html</td>\n      <td>The landscape for storing your big data is quite complex, with several competing formats and different implementations of each format. Understanding your use of the data is critical for picking the format. Depending on your use case, the different formats perform very differently. Although you can use a hammer to drive a screw, it isn’t fast or easy to do so.\\nThe use cases that we’ve examined are:\\nreading all of the columns\\nreading a few of the columns\\nfiltering using a filter predicate\\nwriting the data\\n\\nWhile previous work has compared the size and speed from Hive, this presentation will present benchmarks from Spark including the new work that radically improves the performance of Spark on ORC. This presentation will also include tips and suggestions to optimize the performance of your application while reading and writing the data.\\nFinally, the value of having open source benchmarks that are available to all interested parties is hugely important and all of the code is available from Apache.  \\n</td>\n      <td>Fast Access To Your Complex Data - Avro, JSON, ORC, and Parquet Scale The landscape for storing your big data is quite complex, with several competing formats and different implementations of each format. Understanding your use of the data is critical for picking the format. Depending on your use case, the different formats perform very differently. Although you can use a hammer to drive a screw, it isn’t fast or easy to do so.\\nThe use cases that we’ve examined are:\\nreading all of the columns\\nreading a few of the columns\\nfiltering using a filter predicate\\nwriting the data\\n\\nWhile previous work has compared the size and speed from Hive, this presentation will present benchmarks from Spark including the new work that radically improves the performance of Spark on ORC. This presentation will also include tips and suggestions to optimize the performance of your application while reading and writing the data.\\nFinally, the value of having open source benchmarks that are available to all interested parties is hugely important and all of the code is available from Apache.  \\n</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>342</th>\n      <td>2016</td>\n      <td>BM25 demystified</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Britta Weber</td>\n      <td>https://2016.berlinbuzzwords.de/session/bm25-demystified.html</td>\n      <td>Lucene will change the default scoring from TF/IDF to BM25 in the next major release. So unless you really enjoy surprises you better learn about it now! TF/IDF was easy enough to understand intuitively but how is it with BM25? What do all these parameters do? And what do people mean when they say it is \"probabilistic\"? In this talk I will tell the story of how we came from the Probability Ranking Principle to BM25 with a minimum of math and a maximum of explaining. I will also show how BM25 differs from TF/IDF, what it means in practice and give and intuition on what the parameters of this method actually do. You will leave this talk feeling good about Lucene changing the default. And of course you will learn many fancy buzzwords to show off with during the breaks.\\n</td>\n      <td>BM25 demystified Search Lucene will change the default scoring from TF/IDF to BM25 in the next major release. So unless you really enjoy surprises you better learn about it now! TF/IDF was easy enough to understand intuitively but how is it with BM25? What do all these parameters do? And what do people mean when they say it is \"probabilistic\"? In this talk I will tell the story of how we came from the Probability Ranking Principle to BM25 with a minimum of math and a maximum of explaining. I will also show how BM25 differs from TF/IDF, what it means in practice and give and intuition on what the parameters of this method actually do. You will leave this talk feeling good about Lucene changing the default. And of course you will learn many fancy buzzwords to show off with during the breaks.\\n</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>2017</td>\n      <td>BM25 is so Yesterday: Modern Techniques for Better Search Relevance</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Grant Ingersoll</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/bm25-so-yesterday-modern-techniques-better-search-relevance.html</td>\n      <td>Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene.  And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance.  This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it.  We’ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr.  We’ll finish with a sneak peak into using deep learning and word2vec in a search context.\\n</td>\n      <td>BM25 is so Yesterday: Modern Techniques for Better Search Relevance Search Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene.  And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance.  This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it.  We’ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr.  We’ll finish with a sneak peak into using deep learning and word2vec in a search context.\\n</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>2017</td>\n      <td>Apache Lucene 7 - What's coming next?</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Uwe Schindler</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/apache-lucene-7-whats-coming-next.html</td>\n      <td>At the beginning of the year 2017, the Apache Lucene team decided to focus on releasing Apache Lucene 7. Around Berlin Buzzwords, the new version will be available for testing.\\nThis talk will present the new and changed features of Lucene 7: As TF-IDF is no longer the default, several query special cases like query normalization and the so-called \"coord factor\" were removed. Those were workaround for problems that are specific to TF-IDF like not strong enough term frequency saturation, but can be completely ignored with other ranking functions like BM25. The user has to be prepared that scores may differ and the absolute values of scores are meaningless, breaking applications. The problem with query normalization and coordination factors was correct query rewriting, but now many more optimizations can be done to handle optional, filtered, and mandatory query clauses: Lucene 7 will be faster if it finds duplicate clauses. The talk will also present recent Lucene 6 features like graph token streams and how they are used in Lucene 7.\\nThe talk will also present future plans to support the Java 9 module system and the current state of Java 9 support inside Apache Lucene, because it is expected that Lucene/Solr and Elasticsearch users will one of the first communities that will migrate to Java 9, because recent hotspot optimizations will execute queries and allow doc values access with much higher performance.\\n</td>\n      <td>Apache Lucene 7 - What's coming next? Search At the beginning of the year 2017, the Apache Lucene team decided to focus on releasing Apache Lucene 7. Around Berlin Buzzwords, the new version will be available for testing.\\nThis talk will present the new and changed features of Lucene 7: As TF-IDF is no longer the default, several query special cases like query normalization and the so-called \"coord factor\" were removed. Those were workaround for problems that are specific to TF-IDF like not strong enough term frequency saturation, but can be completely ignored with other ranking functions like BM25. The user has to be prepared that scores may differ and the absolute values of scores are meaningless, breaking applications. The problem with query normalization and coordination factors was correct query rewriting, but now many more optimizations can be done to handle optional, filtered, and mandatory query clauses: Lucene 7 will be faster if it finds duplicate clauses. The talk will also present recent Lucene 6 features like graph token streams and how they are used in Lucene 7.\\nThe talk will also present future plans to support the Java 9 module system and the current state of Java 9 support inside Apache Lucene, because it is expected that Lucene/Solr and Elasticsearch users will one of the first communities that will migrate to Java 9, because recent hotspot optimizations will execute queries and allow doc values access with much higher performance.\\n</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>2018</td>\n      <td>Machine Learning for Apache Solr, Elasticsearch &amp; Vespa</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Mitali Jha</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/machine-learning-apache-solr-elasticsearch-vespa.html</td>\n      <td>Apache Solr, Elasticsearch and Vespa are three of the most popular general purpose search engines that are used in search, recommendations and analytics based applications. Machine learning is a critical aid to improving relevance beyond the native ranking algorithms (e.g. TF-IDF, BM25 etc.) by leveraging editorial judgements and user behaviour and factoring them into ranking of results. This is known as \"learning to rank\" (LTR) or \"machine learned ranking\" (MLR) etc.\\nIn this talk, the speaker presents a comparison of machine learning frameworks across all of these search engines [1]. The comparison is followed by a quick demonstration on how to use all of these frameworks. This talk is aimed at those who are looking to decide upon which search engine to use in their applications based on the machine learned ranking capabilities available for it, and the ease of using such features.\\n[1] 1. LTR based re-ranking module in Solr, https://lucene.apache.org/solr/guide/7_2/learning-to-rank.html       2. Elasticsearch's LTR plugin, https://github.com/o19s/elasticsearch-learning-to-rank       3. Vespa's MLR functionality, http://docs.vespa.ai/documentation/ranking.html\\n \\n</td>\n      <td>Machine Learning for Apache Solr, Elasticsearch &amp; Vespa Search Apache Solr, Elasticsearch and Vespa are three of the most popular general purpose search engines that are used in search, recommendations and analytics based applications. Machine learning is a critical aid to improving relevance beyond the native ranking algorithms (e.g. TF-IDF, BM25 etc.) by leveraging editorial judgements and user behaviour and factoring them into ranking of results. This is known as \"learning to rank\" (LTR) or \"machine learned ranking\" (MLR) etc.\\nIn this talk, the speaker presents a comparison of machine learning frameworks across all of these search engines [1]. The comparison is followed by a quick demonstration on how to use all of these frameworks. This talk is aimed at those who are looking to decide upon which search engine to use in their applications based on the machine learned ranking capabilities available for it, and the ease of using such features.\\n[1] 1. LTR based re-ranking module in Solr, https://lucene.apache.org/solr/guide/7_2/learning-to-rank.html       2. Elasticsearch's LTR plugin, https://github.com/o19s/elasticsearch-learning-to-rank       3. Vespa's MLR functionality, http://docs.vespa.ai/documentation/ranking.html\\n \\n</td>\n    </tr>\n    <tr>\n      <th>260</th>\n      <td>2017</td>\n      <td>Update on the t-digest: Finding Faults in Real Data</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Ted Dunning</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/update-t-digest-finding-faults-real-data.html</td>\n      <td>Is your system working? Really? Average response times and throughputs don’t tell the whole story. To really understand what is happening, you probably need measurements like the 99.9%-ile response time. A growing number of systems are using the t-digest to do this. I will explain the algorithm with practical examples, talk about how it is much simpler and faster than before, talk about integration in systems like Elastic, Solar and streamlib, tell some real-world deployment stories and show some pretty pictures.</td>\n      <td>Update on the t-digest: Finding Faults in Real Data Search Is your system working? Really? Average response times and throughputs don’t tell the whole story. To really understand what is happening, you probably need measurements like the 99.9%-ile response time. A growing number of systems are using the t-digest to do this. I will explain the algorithm with practical examples, talk about how it is much simpler and faster than before, talk about integration in systems like Elastic, Solar and streamlib, tell some real-world deployment stories and show some pretty pictures.</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>342</th>\n      <td>2016</td>\n      <td>BM25 demystified</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Britta Weber</td>\n      <td>https://2016.berlinbuzzwords.de/session/bm25-demystified.html</td>\n      <td>Lucene will change the default scoring from TF/IDF to BM25 in the next major release. So unless you really enjoy surprises you better learn about it now! TF/IDF was easy enough to understand intuitively but how is it with BM25? What do all these parameters do? And what do people mean when they say it is \"probabilistic\"? In this talk I will tell the story of how we came from the Probability Ranking Principle to BM25 with a minimum of math and a maximum of explaining. I will also show how BM25 differs from TF/IDF, what it means in practice and give and intuition on what the parameters of this method actually do. You will leave this talk feeling good about Lucene changing the default. And of course you will learn many fancy buzzwords to show off with during the breaks.\\n</td>\n      <td>BM25 demystified Search Lucene will change the default scoring from TF/IDF to BM25 in the next major release. So unless you really enjoy surprises you better learn about it now! TF/IDF was easy enough to understand intuitively but how is it with BM25? What do all these parameters do? And what do people mean when they say it is \"probabilistic\"? In this talk I will tell the story of how we came from the Probability Ranking Principle to BM25 with a minimum of math and a maximum of explaining. I will also show how BM25 differs from TF/IDF, what it means in practice and give and intuition on what the parameters of this method actually do. You will leave this talk feeling good about Lucene changing the default. And of course you will learn many fancy buzzwords to show off with during the breaks.\\n</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>2017</td>\n      <td>BM25 is so Yesterday: Modern Techniques for Better Search Relevance</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Grant Ingersoll</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/bm25-so-yesterday-modern-techniques-better-search-relevance.html</td>\n      <td>Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene.  And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance.  This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it.  We’ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr.  We’ll finish with a sneak peak into using deep learning and word2vec in a search context.\\n</td>\n      <td>BM25 is so Yesterday: Modern Techniques for Better Search Relevance Search Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene.  And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance.  This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it.  We’ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr.  We’ll finish with a sneak peak into using deep learning and word2vec in a search context.\\n</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>2017</td>\n      <td>Apache Lucene 7 - What's coming next?</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Uwe Schindler</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/apache-lucene-7-whats-coming-next.html</td>\n      <td>At the beginning of the year 2017, the Apache Lucene team decided to focus on releasing Apache Lucene 7. Around Berlin Buzzwords, the new version will be available for testing.\\nThis talk will present the new and changed features of Lucene 7: As TF-IDF is no longer the default, several query special cases like query normalization and the so-called \"coord factor\" were removed. Those were workaround for problems that are specific to TF-IDF like not strong enough term frequency saturation, but can be completely ignored with other ranking functions like BM25. The user has to be prepared that scores may differ and the absolute values of scores are meaningless, breaking applications. The problem with query normalization and coordination factors was correct query rewriting, but now many more optimizations can be done to handle optional, filtered, and mandatory query clauses: Lucene 7 will be faster if it finds duplicate clauses. The talk will also present recent Lucene 6 features like graph token streams and how they are used in Lucene 7.\\nThe talk will also present future plans to support the Java 9 module system and the current state of Java 9 support inside Apache Lucene, because it is expected that Lucene/Solr and Elasticsearch users will one of the first communities that will migrate to Java 9, because recent hotspot optimizations will execute queries and allow doc values access with much higher performance.\\n</td>\n      <td>Apache Lucene 7 - What's coming next? Search At the beginning of the year 2017, the Apache Lucene team decided to focus on releasing Apache Lucene 7. Around Berlin Buzzwords, the new version will be available for testing.\\nThis talk will present the new and changed features of Lucene 7: As TF-IDF is no longer the default, several query special cases like query normalization and the so-called \"coord factor\" were removed. Those were workaround for problems that are specific to TF-IDF like not strong enough term frequency saturation, but can be completely ignored with other ranking functions like BM25. The user has to be prepared that scores may differ and the absolute values of scores are meaningless, breaking applications. The problem with query normalization and coordination factors was correct query rewriting, but now many more optimizations can be done to handle optional, filtered, and mandatory query clauses: Lucene 7 will be faster if it finds duplicate clauses. The talk will also present recent Lucene 6 features like graph token streams and how they are used in Lucene 7.\\nThe talk will also present future plans to support the Java 9 module system and the current state of Java 9 support inside Apache Lucene, because it is expected that Lucene/Solr and Elasticsearch users will one of the first communities that will migrate to Java 9, because recent hotspot optimizations will execute queries and allow doc values access with much higher performance.\\n</td>\n    </tr>\n    <tr>\n      <th>229</th>\n      <td>2018</td>\n      <td>Getting Started with Query Understanding</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Giovanni Fernandez-Kincade</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/getting-started-query-understanding.html</td>\n      <td>A user types “black clutch” into your search engine. Do they mean the handbag, the automobile part, or something else entirely?\\nSearch is about matching the intent of the user with the information they need. For decades, “relevance” in information retrieval systems has meant things like BM25, TFIDF, field boosting, document boosting, etc. These simple heuristics and strategies have served us well, but ultimately fall short because they fail to semantically model intent. Our systems don’t actually understand what users want, they just hope a few magic numbers will get us close enough.\\nQuery Understanding is about using real intelligence to put users first. In this session, we’ll talk about what Query Understanding is, why it’s important, and some practical strategies for making your search experience smarter.\\n</td>\n      <td>Getting Started with Query Understanding Search A user types “black clutch” into your search engine. Do they mean the handbag, the automobile part, or something else entirely?\\nSearch is about matching the intent of the user with the information they need. For decades, “relevance” in information retrieval systems has meant things like BM25, TFIDF, field boosting, document boosting, etc. These simple heuristics and strategies have served us well, but ultimately fall short because they fail to semantically model intent. Our systems don’t actually understand what users want, they just hope a few magic numbers will get us close enough.\\nQuery Understanding is about using real intelligence to put users first. In this session, we’ll talk about what Query Understanding is, why it’s important, and some practical strategies for making your search experience smarter.\\n</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>2017</td>\n      <td>Getting Started with Query Understanding</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td></td>\n      <td>https://2017.berlinbuzzwords.de/17/session/getting-started-query-understanding.html</td>\n      <td>A user types “black clutch” into your search engine. Do they mean the handbag, the automobile part, or something else entirely?\\nSearch is about matching the intent of the user with the information they need. For decades, “relevance” in information retrieval systems has meant things like BM25, TFIDF, field boosting, document boosting, etc. These simple heuristics and strategies have served us well, but ultimately fall short because they fail to semantically model intent. Our systems don’t actually understand what users want, they just hope a few magic numbers will get us close enough.\\nQuery Understanding is about using real intelligence to put users first. In this session, we’ll talk about what Query Understanding is, why it’s important, and some practical strategies for making your search experience smarter.\\n</td>\n      <td>Getting Started with Query Understanding Search A user types “black clutch” into your search engine. Do they mean the handbag, the automobile part, or something else entirely?\\nSearch is about matching the intent of the user with the information they need. For decades, “relevance” in information retrieval systems has meant things like BM25, TFIDF, field boosting, document boosting, etc. These simple heuristics and strategies have served us well, but ultimately fall short because they fail to semantically model intent. Our systems don’t actually understand what users want, they just hope a few magic numbers will get us close enough.\\nQuery Understanding is about using real intelligence to put users first. In this session, we’ll talk about what Query Understanding is, why it’s important, and some practical strategies for making your search experience smarter.\\n</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623927512421
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means clustering\n",
        "# \n",
        "# Using the word-based TF-IDF, figure out what's the optimal number of\n",
        "#  clusters to reduce our 20k-30k terms down into\n",
        "#\n",
        "# Note - we can't use lots of the scoring (eg completeness or homogeneity\n",
        "#  or V-measure as we don't have a ground-truth)\n",
        "\n",
        "KM = namedtuple('KM', 'k km scoff tfidf')\n",
        "\n",
        "# Try a range of k-sizes\n",
        "# A wide range is slow but lets us check what's best\n",
        "# If we know what's almost right, a narrow range is fine!\n",
        "krange = range(51, 52)\n",
        "#krange = range(25, talks.shape[0]-25)\n",
        "\n",
        "# Try each one with a few different inits, to avoid getting stuck in local-minima\n",
        "kms = []\n",
        "for k in krange:\n",
        "    tfidf = tfidf_word\n",
        "\n",
        "    print(\"Building k-means cluster of size %d\" % k)\n",
        "    km = KMeans(n_clusters=k, init='k-means++', max_iter=100, \n",
        "                n_init=15, verbose=False)\n",
        "    km.fit(tfidf.matrix)\n",
        "    # TODO Do we need to include labels / talk indexes?\n",
        "\n",
        "    scoff = metrics.silhouette_score(tfidf.matrix, km.labels_, sample_size=1000)\n",
        "    print(\" - Silhouette Coefficient: %0.4f\" % scoff)\n",
        "\n",
        "    kms.append( KM(k,km,scoff,tfidf) )\n",
        "\n",
        "# Save all the scores, so we can draw a graph\n",
        "with open(\"outputs/cluster-scores.csv\",\"w\") as c:\n",
        "   cw = csv.writer(c)\n",
        "   cw.writerow((\"k\",\"Silhouette Coefficient\"))\n",
        "   for k in kms:\n",
        "      cw.writerow((k.k, k.scoff))\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Visualise the Silhouette Coefficient vs K\n",
        "fig = plt.figure(figsize=(15,9))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot([k.k for k in kms], [k.scoff for k in kms])\n",
        "ax.set(title=\"Silhouette Coefficient vs K\", xlabel=\"Cluster Size (K)\")\n",
        "if len(kms) > 3:\n",
        "   plt.show()\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Which one had the best Silhouette Coefficient?\n",
        "kms.sort(key=lambda x: x.scoff, reverse=True)\n",
        "best_km = kms[0]\n",
        "print(\"\")\n",
        "print(\"Best K-Means found with a cluster-size (k) of %d\" % best_km.k)\n",
        "print(\"That had a Silhouette Coefficient: %0.4f\" % best_km.scoff)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building k-means cluster of size 51\n",
            " - Silhouette Coefficient: 0.0083\n",
            "\n",
            "Best K-Means found with a cluster-size (k) of 51\n",
            "That had a Silhouette Coefficient: 0.0083\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1080x648 with 1 Axes>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA30AAAImCAYAAADntOM5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7ht93zv8c9X4lLkgmxKLqJEe+K2D7uhFGnaRuIWFE1KBa3gkaNS2oaWU21pq9qKS6VuRZGoiiNIT1CPamkqOxUhJE0okrOVxJ24he/5Y47NzOrae81kb1lZv/V6Pc961pxj/MaYvzEznsTbGHOu6u4AAAAwpmut9gQAAAD40RF9AAAAAxN9AAAAAxN9AAAAAxN9AAAAAxN9AAAAAxN9ACysqh5RVe+ce95VdZvp8aur6o9Wb3ZrQ1Xdo6ouqKqvV9WDqupmVfW+qvpaVf15VT2jql6xwH5OrKpnXh1zBmBtE30AXEFV/WxVfaCqvlJVX6yq91fVTydJd7++uw9d7TnOq6r3VtWvL1n2gxi9ivusqnpyVX20qr5RVRdX1Zuq6g47PuP8QZIXd/cNu/v/JDkmyaVJdu/up3b3c7v717e/i6S7n9Ddf7ijk6mqg6vq4h3dz5V9naq6TlWdMp1fu/+oXx9gPdt1tScAwDXH9D++357kiUn+Lsl1ktwzybdXc16r4IQk90vyuCTvT7JLkgdPyz6yg/u+ZZJzlzz/WHf3Du53zaiq6yZ5c5LrJzm0u7+xylMCGJorfQDMu22SdPdJ3f297v5md7+zu89Jkqp6dFX9y3a2v1FVvWO6VfHfqurWW1dU1d2r6szpCuKZVXX3uXWfqqpfmHv++1X1urnnd5uuPn65qj5cVQdPy5+TWZS+eLpd8sVV9b5psw9Py355Gnv/qjp72scHquqOyx1AVR2Q5ElJjuru93T3t7v7sukq559MY/aoqtdW1SVV9emq+r2qutbcPh5bVR+vqi9V1elVdctp+SeS/ESSt01zOynJ0Ul+e3r+C8sc+9Yrr1+uqouq6tHT8ivcTru945ve36dV1TnT+//GqrpeVd0gyT8kucX0+l+vqlsseT/uVlX/VVW7zC17cFVtPScOqqrNVfXVqvpcVf3Fcu/r3LbXT/K2JNdOcj/BB/CjJ/oAmPcfSb5XVa+pqsOr6kZXcvujkjw7yY2SXJjkOUlSVTdO8o4kL0xykyR/keQdVXWTlXZYVXtP2/5RkhsneVqSN1fVhu7+3ST/nOTY6XbJY7v7XtOmd5qWvbGq7pzkVUkeP73+Xyc5dbritNTPJ7m4uz+4nWm9KMkemQXcvZM8Ksljpvk+KMkzkjwkyYZpficlSXffOslnkjxgmttRSV6f5HnT83cvOfb9MouyF0372pjk7GXeo0WO7+FJDktyqyR3TPLoKbgOT7Jlev0bdveW+X139xlJvpHkkLnFv5LkDdPjE5Kc0N27J7l1ZleIt+W60/F8K8kDu/ub2xkLwE4i+gD4ge7+apKfTdJJXp7kkqo6taputuAuTunuD3b35ZnFzMZp+f2SXNDdf9vdl3f3SUnOS/KABfb5yCSndfdp3f397n5Xks1J7nslDu1xSf66u/9tuoL5msxuWb3bMmNvkuSz29rRdMXrl5M8vbu/1t2fSvLnSX51GvL4JH/c3R+f3ofnJtm49WrflfSIJO+errx+t7u/0N3/LfoWPL4XdveW7v5iZlfaNi6zn205KbOgT1Xtltl7f9K07rtJblNVe3X316dI3JbdkvxMktd093q7ZRhg1Yg+AK5gipVHd/c+SW6f5BZJXrDg5v819/iyJDecHt8iyaeXjP10kr0X2Octkzxsum3xy1X15czC9OYLzmnrPp66ZB/7TvNa6gsr7HuvzD7rOH8888dyyyQnzL3OF5NUFjvWpfZN8okFxi1yfNv6Z7OINyR5yHTl8CFJ/r27tx7/r2V2W/B5022799/Ofi5NcmSS11TVfa7E6wOwA0QfANvU3ecleXVm8bcjtmQWJvP2S/L/psffyOxLPbb68bnHFyX52+7ec+7nBls/X5fZVcmVXJTkOUv2cf3piuNS/5hkn6ratI19XZrZ1a3545k/louSPH7Ja/1Yd39ggXkuN+9brzjqyh3fUiu+f939sczC9vBc8dbOdPcF022qN03yp0n+fvqs4Lb2dUpmVyb/vqp+boH5AbCDRB8AP1BVP1VVT62qfabn+2Z2W9/2btlbxGlJbltVv1JVu05frnJgZt8Umsw+p3ZkVV17iq2Hzm37uiQPqKr7VNUu0xeQHLx1jkk+l9ln6+YtXfbyJE+oqrvWzA2q6n7TrYpX0N0XJPmrJCdNr3Od6TWPrKrju/t7mX1u7TlVtdt02+ZvTvNMkhOTPL2qbpf84EtfHnZV3rTMbpH9hap6+PS+3aSqlrstc+HjW8bnktykqvZYYdwbkjw5yb2SvGnrwqp65PT5yu8n+fK0+Hvb29EUo8cmeWtV3WOBOQKwA0QfAPO+luSuSf6tqr6RWex9NMlTd2Sn3f2FJPef9vOFJL+d5P7dfek05JmZXdH6UmZfBDN/JemiJEdk9uUol2R2Veu38sP/hp2Q5KE1+6bMF07Lfj+zWwi/XFUP7+7NmV1devH0GhcmefR2pvzkaexLMguZT2T2JxveNq3/X5ldnfxkkn+Z5vuqab5vyeyK18lV9dXM3r/DF3unrqi7P5PZ5+eemtltomcnudMy467s8c1ve15mn8/75PR+LXfLa6YxByd5z9w/t2T25TDnVtXXM/tncWR3f2uB133NdFzvqKqDFpkrAFdNraM/CwQAALDuuNIHAAAwMNEHAAAwMNEHAAAwMNEHAAAwMNEHAAAwsF1XewI7w1577dX777//ak8DAABgVZx11lmXdveG5dYNEX37779/Nm/evNrTAAAAWBVV9eltrXN7JwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMBEHwAAwMAWir6qOqyqzq+qC6vq+GXWV1W9cFp/TlXdeaVtq2pjVZ1RVWdX1eaqOmhu3R2r6l+r6tyq+khVXW9HDxQAAGA9WjH6qmqXJC9JcniSA5McVVUHLhl2eJIDpp9jkrx0gW2fl+TZ3b0xybOm56mqXZO8LskTuvt2SQ5O8t2rfogAAADr1yJX+g5KcmF3f7K7v5Pk5CRHLBlzRJLX9swZSfasqpuvsG0n2X16vEeSLdPjQ5Oc090fTpLu/kJ3f+8qHh8AAMC6tusCY/ZOctHc84uT3HWBMXuvsO1TkpxeVc/PLD7vPi2/bZKuqtOTbEhycnc/b+mkquqYzK4qZr/99lvgMAAAANafRa701TLLesEx29v2iUmO6+59kxyX5JXT8l2T/GySR0y/H1xVP//fdtL9su7e1N2bNmzYsPJRAAAArEOLRN/FSfade75Pfngr5kpjtrft0UlOmR6/KbNbQbfu65+6+9LuvizJaUnuHAAAAK60RaLvzCQHVNWtquo6SY5McuqSMacmedT0LZ53S/KV7v7sCttuSXLv6fEhSS6YHp+e5I5Vdf3pS13uneRjV/H4AAAA1rUVP9PX3ZdX1bGZxdguSV7V3edW1ROm9SdmdjXuvkkuTHJZksdsb9tp149LcsIUdt/K9Pm87v5SVf1FZsHYSU7r7nfsrAMGAABYT6p76cfz1p5Nmzb15s2bV3saAAAAq6KqzuruTcutW+iPswMAALA2iT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBLRR9VXVYVZ1fVRdW1fHLrK+qeuG0/pyquvNK21bVxqo6o6rOrqrNVXXQkn3uV1Vfr6qn7cgBAgAArGcrRl9V7ZLkJUkOT3JgkqOq6sAlww5PcsD0c0ySly6w7fOSPLu7NyZ51vR83l8m+YercEwAAABMFrnSd1CSC7v7k939nSQnJzliyZgjkry2Z85IsmdV3XyFbTvJ7tPjPZJs2bqzqnpQkk8mOfcqHhcAAABJdl1gzN5JLpp7fnGSuy4wZu8Vtn1KktOr6vmZxefdk6SqbpDkd5L8YhK3dgIAAOyARa701TLLesEx29v2iUmO6+59kxyX5JXT8mcn+cvu/vp2J1V1zPRZwM2XXHLJ9oYCAACsW4tc6bs4yb5zz/fJ3K2YK4y5zna2PTrJb0yP35TkFdPjuyZ5aFU9L8meSb5fVd/q7hfPv2B3vyzJy5Jk06ZNSyMUAACALHal78wkB1TVrarqOkmOTHLqkjGnJnnU9C2ed0vyle7+7Arbbkly7+nxIUkuSJLuvmd379/d+yd5QZLnLg0+AAAAFrPilb7uvryqjk1yepJdkryqu8+tqidM609MclqS+ya5MMllSR6zvW2nXT8uyQlVtWuSb2X2rZ8AAADsRNW99u+M3LRpU2/evHm1pwEAALAqquqs7t603LqF/jg7AAAAa5PoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGJjoAwAAGNhC0VdVh1XV+VV1YVUdv8z6qqoXTuvPqao7r7RtVW2sqjOq6uyq2lxVB03Lf7Gqzqqqj0y/D9kZBwoAALAerRh9VbVLkpckOTzJgUmOqqoDlww7PMkB088xSV66wLbPS/Ls7t6Y5FnT8yS5NMkDuvsOSY5O8rdX+egAAADWuUWu9B2U5MLu/mR3fyfJyUmOWDLmiCSv7ZkzkuxZVTdfYdtOsvv0eI8kW5Kkuz/U3Vum5ecmuV5VXfcqHh8AAMC6tusCY/ZOctHc84uT3HWBMXuvsO1TkpxeVc/PLD7vvsxr/1KSD3X3txeYJwAAAEsscqWvllnWC47Z3rZPTHJcd++b5Lgkr7zCDqtul+RPkzx+2UlVHTN9FnDzJZdcsp3pAwAArF+LRN/FSfade75PplsxFxizvW2PTnLK9PhNmd0KmiSpqn2SvCXJo7r7E8tNqrtf1t2bunvThg0bFjgMAACA9WeR6DszyQFVdauquk6SI5OcumTMqUkeNX2L592SfKW7P7vCtluS3Ht6fEiSC5KkqvZM8o4kT+/u9+/AsQEAAKx7K36mr7svr6pjk5yeZJckr+ruc6vqCdP6E5OcluS+SS5MclmSx2xv22nXj0tyQlXtmuRbmX3rZ5Icm+Q2SZ5ZVc+clh3a3Z/f4aMFAABYZ6p76cfz1p5Nmzb15s2bV3saAAAAq6KqzuruTcutW+iPswMAALA2iT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBLRR9VXVYVZ1fVRdW1fHLrK+qeuG0/pyquvNK21bVxqo6o6rOrqrNVXXQ3LqnT+PPr6r77OhBAgAArFcrRl9V7ZLkJUkOT3JgkqOq6sAlww5PcsD0c0ySly6w7fOSPLu7NyZ51vQ80/ojk9wuyWFJ/mraDwAAAFfSIlf6DkpyYXd/sru/k+TkJEcsGXNEktf2zBlJ9qyqm6+wbSfZfXq8R5Itc/s6ubu/3d3/meTCaT8AAABcSbsuMGbvJBfNPb84yV0XGLP3Cts+JcnpVfX8zOLz7nP7OmOZfV1BVR2T2VXF7LfffgscBgAAwPqzyJW+WmZZLzhme9s+Mclx3b1vkuOSvPJKvF66+2Xdvam7N23YsGHZiQMAAKx3i0TfxUn2nXu+T354K+ZKY7a37dFJTpkevyk/vIVzkdcDAABgAYtE35lJDqiqW1XVdTL7kpVTl4w5Ncmjpm/xvFuSr3T3Z1fYdkuSe0+PD0lywdy+jqyq61bVrTL7cpgPXsXjAwAAWNdW/Exfd19eVccmOT3JLkle1d3nVtUTpvUnJjktyX0z+9KVy5I8ZnvbTrt+XJITqmrXJN/K9Pm8ad9/l+RjSS5P8qTu/t7OOmAAAID1pLr/28fl1pxNmzb15s2bV3saAAAAq6KqzuruTcutW+iPswMAALA2iT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBiT4AAICBVXev9hx2WFVdkuTTqz0PfmT2SnLpak+C4Tiv2NmcU+xszil2NufU2G7Z3RuWWzFE9DG2qtrc3ZtWex6MxXnFzuacYmdzTrGzOafWL7d3AgAADEz0AQAADEz0sRa8bLUnwJCcV+xszil2NucUO5tzap3ymT4AAICBudIHAAAwMNHHNU5V7VJVH6qqt0/P71RV/1pVH6mqt1XV7qs9R9aWqvrUdP6cXVWbp2U3rqp3VdUF0+8brfY8WTu2cU49rKrOrarvV5Vvx+NK2cY59WdVdV5VnVNVb6mqPVd7nqwt2ziv/nA6p86uqndW1S1We5786Ik+rol+I8nH556/Isnx3X2HJG9J8lurMivWup/r7o1zX1V9fJJ/7O4Dkvzj9ByujKXn1EeTPCTJ+1ZxTqxtS8+pdyW5fXffMcl/JHn66k2NNWzpefVn3X3H7t6Y5O1JnrWKc+NqIvq4RqmqfZLcL7PQ2+on88P/EfWuJL90dc+LIR2R5DXT49ckedAqzoUBdPfHu/v81Z4H4+jud3b35dPTM5Lss5rzYQzd/dW5pzdI4gs+1gHRxzXNC5L8dpLvzy37aJIHTo8flmTfq3tSrHmd5J1VdVZVHTMtu1l3fzZJpt83XbXZsRYtd07BjljpnHpskn+4mufE2rfseVVVz6mqi5I8Iq70rQuij2uMqrp/ks9391lLVj02yZOq6qwkuyX5ztU+Oda6e3T3nZMcntm5dK/VnhBrnnOKnW2b51RV/W6Sy5O8frUmx5q17HnV3b/b3ftmdk4du5oT5Ooh+rgmuUeSB1bVp5KcnOSQqnpdd5/X3Yd2912SnJTkE6s5Sdae7t4y/f58Zp8LPSjJ56rq5kky/f786s2QtWYb5xRcZds6p6rq6CT3T/KI9ne2uJIW+HfVG+JjM+uC6OMao7uf3t37dPf+SY5M8p7ufmRV3TRJqupaSX4vyYmrOE3WmKq6QVXttvVxkkMzu2X41CRHT8OOTvLW1Zkha812zim4SrZ1TlXVYUl+J8kDu/uy1Zwja892zqsD5oY9MMl5qzE/rl67rvYEYAFHVdWTpsenJPmb1ZwMa87NkrylqpLZv/Pe0N3/t6rOTPJ3VfVrST6T2edFYRHbOqcenORFSTYkeUdVnd3d91nFebJ2bOucujDJdZO8a1p3Rnc/YfWmyRqzrfPqzVX1k5l9f8Knkzin1oFypwAAAMC43N4JAAAwMNEHAAAwMNEHAAAwMNEHAAAwMNEHAAAwMNEHAAAwMNEHwJpTVT9eVSdX1Seq6mNVdVpV3baq9q+qq/SH0qvq0VV1ix2c182q6u1V9eGt85qW36Kq/n5H9j3tp6rqPVW1+/T863Pr7ltVF1TVflV1bFU9ZkdfD4AxiD4A1pSa/aXhtyR5b3ffursPTPKMzP4Q8Y54dJIrFX1VteuSRX+Q5F3dfadpXscnSXdv6e6H7uD8kuS+ST7c3V9dMo+fz+wPwx/W3Z9J8qokT94JrwfAAEQfAGvNzyX5bnefuHVBd5/d3f88P2i6cvfiuedvr6qDq2qXqnp1VX20qj5SVcdV1UOTbEry+qo6u6p+rKruUlX/VFVnVdXpVXXzaT/vrarnVtU/JfmNJXO7eZKL5+Z1zrTND65AVtUrptc4u6ouqar/PS3/rao6s6rOqapnb+PYH5HkrUuO855JXp7kft39iel1L0vyqao6aMH3FICBLf1/KAHgmu72Sc7age03Jtm7u2+fJFW1Z3d/uaqOTfK07t5cVdfO7MrZEd19SVX9cpLnJHnstI89u/vey+z7JUneOO3r3b2mhG8AAAKeSURBVEn+pru3zA/o7l+fXveWSU5P8uqqOjTJAUkOSlJJTq2qe3X3+5bs/x5JHj/3/LqZReDB3X3ekrGbk9wzyQcXfF8AGJQrfQCsN59M8hNV9aKqOizJV5cZ85OZxeW7qursJL+XZJ+59W9cbsfdfXqSn8jsyttPJflQVW1YOq6qrpfkTUmO7e5PJzl0+vlQkn+ftj1gmZe4cXd/be75d5N8IMmvLTP287mSt6sCMCbRB8Bac26Suyww7vJc8b9z10uS7v5SkjsleW+SJyV5xTLbVpJzu3vj9HOH7j50bv03tvWi3f3F7n5Dd/9qkjOT3GuZYScmOaW73z33en8893q36e5XLndMVTV/TN9P8vAkP11Vz1gy9npJvrmteQKwfog+ANaa9yS5blU9buuCqvrpqlp6u+WnkmysqmtV1b6Z3TqZqtorybW6+81JnpnkztP4ryXZbXp8fpINVfUz0zbXrqrbrTSxqjqkqq4/Pd4tya2TfGbJmCcl2a27/2Ru8elJHltVN5zG7F1VN13mJc7P7EriD0yf37t/kkdU1fwVv9smuUrfZArAWHymD4A1pbu7qh6c5AVVdXySb2UWeE9ZMvT9Sf4zyUcyi59/n5bvneRv5q6YPX36/eokJ1bVN5P8TJKHJnlhVe2R2X8vX5DZVcbtuUuSF1fV1quMr+juM6tq/7kxT0vy3em20SQ5sbtPrKr/keRfZ19Omq8neWRmt2jOe0eSg5NcuOQ9+eJ0q+r7qurS7n5rZp//29YXwgCwjlR3r/YcAIAFTN8g+tru/sUVxv3PJL853WIKwDrn9k4AWCO6+7NJXr71j7Nvx16Z3boKAK70AQAAjMyVPgAAgIGJPgAAgIGJPgAAgIGJPgAAgIGJPgAAgIH9f5m/H07+8v7/AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623927542463
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What does our \"Best Clustering\" look like?\n",
        "# What ended up where?\n",
        "\n",
        "# Report some top features for a few clusters\n",
        "def cluster_terms(km, top_clusters=4):\n",
        "    # Work out the middle of each cluster\n",
        "    order_centroids = km.km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "    # Print the top 10 features for the first few clusters\n",
        "    terms = km.tfidf.tfidf.get_feature_names()\n",
        "    for i in range(top_clusters):\n",
        "        print(\" - Cluster %d:\" % i, end='')\n",
        "        for ind in order_centroids[i, :10]:\n",
        "            print('  %s ' % terms[ind], end='')\n",
        "        print()\n",
        "    print()\n",
        "cluster_terms(best_km)\n",
        "\n",
        "# If you want to visualise the top features of the clusters, rather\n",
        "# than our simple printing them out, see\n",
        "# https://buhrmann.github.io/tfidf-analysis.html\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Predict the best talks for a query based on the clusters\n",
        "\n",
        "# Identify which cluster each talk belongs to\n",
        "talk_clusters = best_km.km.predict(best_km.tfidf.matrix)\n",
        "cluster_talks = defaultdict(list)\n",
        "for talk_id, cluster_id in enumerate(talk_clusters):\n",
        "    cluster_talks[cluster_id].append(talk_id)\n",
        "\n",
        "#  - Work out which cluster our query best maps to\n",
        "#  - Identify the talks in that cluster\n",
        "#  - Find the centre of the cluster\n",
        "#  - Do a regular similarity to work out the \"closest\" to the centre\n",
        "def recommend_km(query, km, max_hits=10):\n",
        "    query_tf = km.tfidf.tfidf.transform([query])\n",
        "    cluster_id = int(km.km.predict(query_tf))\n",
        "    print(\"Best cluster for '%s' is %d\" % (query,cluster_id))\n",
        "    \n",
        "    c_centre_tfidf = km.km.cluster_centers_[cluster_id]\n",
        "    c_talk_ids = cluster_talks[cluster_id]\n",
        "    print(\" - That cluster contains talks: %s\" % c_talk_ids)\n",
        "\n",
        "    # Compare all talks with this cluster centre\n",
        "    similarities = linear_kernel(c_centre_tfidf.reshape(1, -1), km.tfidf.matrix)\n",
        "\n",
        "    # Order, with a boost for ones in the cluster\n",
        "    tscores = [[i,s] for i,s in enumerate(similarities[0])]\n",
        "    for idx, score in tscores:\n",
        "        if idx in c_talk_ids:\n",
        "            tscores[idx][1] = score+0.01\n",
        "    tscores = sorted(tscores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the indexes and scores of the x most similar talks\n",
        "    tscores = tscores[0:max_hits]\n",
        "    \n",
        "    # Grab those talks\n",
        "    indexes = [i[0] for i in tscores]\n",
        "    return talks.iloc[indexes]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Let's try it!\n",
        "for q in queries:\n",
        "    print(\"\")\n",
        "    print(q)\n",
        "    print(\"\")\n",
        "    render( recommend_km(q, best_km, 10) )\n",
        "    print(\"\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Cluster 0:  smack   osv   distribution   using stack   big data   big   stack   container   containers   partly \n",
            " - Cluster 1:  ml   model   machine   machine learning   learning   deployment   production   models   training   end \n",
            " - Cluster 2:  data streaming   streaming   stream   data   events   flink   apache flink   application   kafka   applications \n",
            " - Cluster 3:  lucene   index   lucene search   apache lucene   search   billions   tweets   apache   documents   challenges faced \n",
            "\n",
            "\n",
            "apache tika\n",
            "\n",
            "Best cluster for 'apache tika' is 5\n",
            " - That cluster contains talks: [42, 95, 140, 145, 157, 158, 160, 164, 183, 192, 203, 219, 221, 225, 258, 265, 266, 268, 281, 302, 324, 358, 362, 385, 419]\n",
            "\n",
            "\n",
            "ngram\n",
            "\n",
            "Best cluster for 'ngram' is 5\n",
            " - That cluster contains talks: [42, 95, 140, 145, 157, 158, 160, 164, 183, 192, 203, 219, 221, 225, 258, 265, 266, 268, 281, 302, 324, 358, 362, 385, 419]\n",
            "\n",
            "\n",
            "nlp\n",
            "\n",
            "Best cluster for 'nlp' is 6\n",
            " - That cluster contains talks: [9, 92, 106, 143, 155, 161, 177, 179, 188, 190, 194, 204, 207, 208, 269, 304, 306, 342, 404]\n",
            "\n",
            "\n",
            "storm spark\n",
            "\n",
            "Best cluster for 'storm spark' is 5\n",
            " - That cluster contains talks: [42, 95, 140, 145, 157, 158, 160, 164, 183, 192, 203, 219, 221, 225, 258, 265, 266, 268, 281, 302, 324, 358, 362, 385, 419]\n",
            "\n",
            "\n",
            "bm25\n",
            "\n",
            "Best cluster for 'bm25' is 6\n",
            " - That cluster contains talks: [9, 92, 106, 143, 155, 161, 177, 179, 188, 190, 194, 204, 207, 208, 269, 304, 306, 342, 404]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>158</th>\n      <td>2019</td>\n      <td>Python, Java, or Go: It's Your Choice with Apache Beam</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Maximilian MichelsIsmaël Mejía</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/python-java-or-go-its-your-choice-apache-beam.html</td>\n      <td>With the rise of Tensorflow and libraries like Numpy, Python has become a popular choice for data processing. Applications built with Python are commonly single-node applications and need to be parallelized in order to scale for big amounts of data. Turns out, JVM-based languages are often the only choice to leverage the power of large-scale data processing tools like Apache Flink or Apache Spark.\\nThis talk introduces Apache Beam, an open-source data processing framework for large-scale batch and stream processing which is designed with portability in mind. Apache Beam lets you use languages like Python, Go, Java, and Scala for data processing. Even better, the resulting programs can be run on the execution engine of your choice.\\nWe will show how easy it is to run data processing jobs on Apache Beam and provide insight into different aspects of Apache Beam's portability architecture. In particular how Beam programs\\nexecute on top of different execution engines like Apache Spark, Apache Flink, or Google Cloud Dataflow\\nsupport multiple languages like Python, Go, and Java\\n\\nApache Beam's portability avoids being locked into a single execution engine or programming language. Moreover, portability enables completely new use cases, e.g. to create data processing jobs which mix multiple languages, to reuse Java IO connectors for loading/storing data from a Python job, or to use libraries (e.g. for machine learning) that do not exist in the main language of the data processing job.\\nPlease join us to learn more about the future of data processing where users are free to choose their programming language and execution engine.\\n \\n</td>\n      <td>Python, Java, or Go: It's Your Choice with Apache Beam Stream With the rise of Tensorflow and libraries like Numpy, Python has become a popular choice for data processing. Applications built with Python are commonly single-node applications and need to be parallelized in order to scale for big amounts of data. Turns out, JVM-based languages are often the only choice to leverage the power of large-scale data processing tools like Apache Flink or Apache Spark.\\nThis talk introduces Apache Beam, an open-source data processing framework for large-scale batch and stream processing which is designed with portability in mind. Apache Beam lets you use languages like Python, Go, Java, and Scala for data processing. Even better, the resulting programs can be run on the execution engine of your choice.\\nWe will show how easy it is to run data processing jobs on Apache Beam and provide insight into different aspects of Apache Beam's portability architecture. In particular how Beam programs\\nexecute on top of different execution engines like Apache Spark, Apache Flink, or Google Cloud Dataflow\\nsupport multiple languages like Python, Go, and Java\\n\\nApache Beam's portability avoids being locked into a single execution engine or programming language. Moreover, portability enables completely new use cases, e.g. to create data processing jobs which mix multiple languages, to reuse Java IO connectors for loading/storing data from a Python job, or to use libraries (e.g. for machine learning) that do not exist in the main language of the data processing job.\\nPlease join us to learn more about the future of data processing where users are free to choose their programming language and execution engine.\\n \\n</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2020</td>\n      <td>First Steps with Apache Beam: Writing Portable Pipelines using Java, Python, Go</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Austin Bennett</td>\n      <td>https://2020.berlinbuzzwords.de/session/first-steps-apache-beam-writing-portable-pipelines-using-java-python-go</td>\n      <td>Apache Beam is an open source unified model for defining data processing pipelines (Batch and strEAM), which allows you to write your pipeline in your language of choice and run it with minimal effort on the execution engine (ex: Apache Spark, Apache Flink, Google Cloud Dataflow) of choice.  In this practical session we will get hands-on writing Beam pipelines, and as well as discuss the fundamentals of Beam programming model, and SDKs (Python, Go, Java).\\nPrerequisites\\nYou will need to install IntelliJ IDEA and/or PyCharm with the EduTools plugin, and with the kata(s) installed in the language of their choice to work through exercises in the online platform.\\nInstructions can be found on the blogpost here: https://beam.apache.org/blog/beam-kata-release/</td>\n      <td>First Steps with Apache Beam: Writing Portable Pipelines using Java, Python, Go Stream Apache Beam is an open source unified model for defining data processing pipelines (Batch and strEAM), which allows you to write your pipeline in your language of choice and run it with minimal effort on the execution engine (ex: Apache Spark, Apache Flink, Google Cloud Dataflow) of choice.  In this practical session we will get hands-on writing Beam pipelines, and as well as discuss the fundamentals of Beam programming model, and SDKs (Python, Go, Java).\\nPrerequisites\\nYou will need to install IntelliJ IDEA and/or PyCharm with the EduTools plugin, and with the kata(s) installed in the language of their choice to work through exercises in the online platform.\\nInstructions can be found on the blogpost here: https://beam.apache.org/blog/beam-kata-release/</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>2017</td>\n      <td>Using Apache Beam to create a unified benchmarking framework for streaming and batch systems</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Ismaël Mejía</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/using-apache-beam-create-unified-benchmarking-framework-streaming-and-batch-systems.html</td>\n      <td>In the relational database world, there are different benchmarks to evaluate the correctness and performance of different databases, for example the TPC suites, In this talk we will motivate the need for a benchmark framework to evaluate both stream and batch processing systems.\\n\\nWe will introduce Nexmark, a framework to evaluate queries over data streams and discuss its implementation on Apache Beam, and the properties that make Apache Beam the perfect tool to develop a benchmark framework. Nexmark was an integration test donated by Google as part of the Apache Beam incubation process and we have been working to evolve it since. Nexmark not only bridges the gap for evaluating data processing frameworks, but also serves as a rich integration test of the correct implementation of both the Beam runners (for systems like Apache Spark, Apache Flink and Apache Apex), and the new features of the Beam SDK that we will also present.</td>\n      <td>Using Apache Beam to create a unified benchmarking framework for streaming and batch systems Stream In the relational database world, there are different benchmarks to evaluate the correctness and performance of different databases, for example the TPC suites, In this talk we will motivate the need for a benchmark framework to evaluate both stream and batch processing systems.\\n\\nWe will introduce Nexmark, a framework to evaluate queries over data streams and discuss its implementation on Apache Beam, and the properties that make Apache Beam the perfect tool to develop a benchmark framework. Nexmark was an integration test donated by Google as part of the Apache Beam incubation process and we have been working to evolve it since. Nexmark not only bridges the gap for evaluating data processing frameworks, but also serves as a rich integration test of the correct implementation of both the Beam runners (for systems like Apache Spark, Apache Flink and Apache Apex), and the new features of the Beam SDK that we will also present.</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2019</td>\n      <td>Writing a Distributed Ray Tracer with Apache Beam, Abridged</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Robert Burke</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/writing-distributed-ray-tracer-apache-beam-abridged.html</td>\n      <td>Ray Tracing is an embarrassingly parallel way to render high quality images, but distributing work between machines is hard. Apache Beam is a model for efficient distributed data processing. In this talk I map Ray Tracing onto the Apache Beam Go SDK.\\nApache Beam SDKs portably abstract computations and data into DoFns and PCollections, and allow you to construct a graph of how data flows and connects. Then any compatible Runner can optimize that graph for computation, and distribute work to the runners.\\nI'll give a overview of Ray Tracing  and introduce the Apache Beam model, it's history,  demonstrating mapping one to the other. I'll explain the benefits and limits of the model and how to get the most out of your pipelines, and show it running on compatible runners like Apache Flink and Google Cloud Dataflow. Further, I'll show using the same pipeline running in batch and streaming modes, with minimal changes.\\n</td>\n      <td>Writing a Distributed Ray Tracer with Apache Beam, Abridged Stream Ray Tracing is an embarrassingly parallel way to render high quality images, but distributing work between machines is hard. Apache Beam is a model for efficient distributed data processing. In this talk I map Ray Tracing onto the Apache Beam Go SDK.\\nApache Beam SDKs portably abstract computations and data into DoFns and PCollections, and allow you to construct a graph of how data flows and connects. Then any compatible Runner can optimize that graph for computation, and distribute work to the runners.\\nI'll give a overview of Ray Tracing  and introduce the Apache Beam model, it's history,  demonstrating mapping one to the other. I'll explain the benefits and limits of the model and how to get the most out of your pipelines, and show it running on compatible runners like Apache Flink and Google Cloud Dataflow. Further, I'll show using the same pipeline running in batch and streaming modes, with minimal changes.\\n</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>2019</td>\n      <td>Apache Beam pipelines at 100TB+ scale using Apache Spark.</td>\n      <td>Advanced</td>\n      <td>Scale</td>\n      <td>David Moravek</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/apache-beam-pipelines-100tb-scale-using-apache-spark.html</td>\n      <td>At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.\\nThis talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.\\n</td>\n      <td>Apache Beam pipelines at 100TB+ scale using Apache Spark. Scale At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.\\nThis talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.\\n</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>2019</td>\n      <td>Streaming your shared ride</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Thomas Weise</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/streaming-your-shared-ride.html</td>\n      <td>Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.\\nEnablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.\\nTopics covered in this talk include:\\nOverview of use cases and platform architecture\\nStreaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping\\nStateful streaming computation with scalability, high availability and low latency processing on Apache Flink\\nDevelopment frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python\\nPython with Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink\\nKubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications\\n</td>\n      <td>Streaming your shared ride Stream Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.\\nEnablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.\\nTopics covered in this talk include:\\nOverview of use cases and platform architecture\\nStreaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping\\nStateful streaming computation with scalability, high availability and low latency processing on Apache Flink\\nDevelopment frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python\\nPython with Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink\\nKubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications\\n</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>2016</td>\n      <td>Google Dataflow: The new open model for batch and stream processing</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Felipe Hoffa</td>\n      <td>https://2016.berlinbuzzwords.de/session/google-dataflow-new-open-model-batch-and-stream-processing.html</td>\n      <td>In 2004 Google published the MapReduce paper, a programming model that kick-started big data as we know it. Ten years later, Google introduced Dataflow - a new paradigm for big data, integrating batch and stream processing in one common abstraction. This time the offer was more than a paper, but also an open source Java SDK and a cloud managed service to run it. In 2016 big data players like Cask, Cloudera, Data Artisans, PayPal, Slack, Talend joined Google to propose Dataflow for incubation at the Apache Software Foundation - now accepted as Apache Beam. Dataflow is here, not only unifying batch and streaming, but also the big data world.\\nIn this talk we are going to review Dataflow's differentiating elements and why they matter.  We’ll demonstrate Dataflow’s capabilities through a real-time demo with practical insights on how to manage and visualize streaming data flows.\\n</td>\n      <td>Google Dataflow: The new open model for batch and stream processing Scale In 2004 Google published the MapReduce paper, a programming model that kick-started big data as we know it. Ten years later, Google introduced Dataflow - a new paradigm for big data, integrating batch and stream processing in one common abstraction. This time the offer was more than a paper, but also an open source Java SDK and a cloud managed service to run it. In 2016 big data players like Cask, Cloudera, Data Artisans, PayPal, Slack, Talend joined Google to propose Dataflow for incubation at the Apache Software Foundation - now accepted as Apache Beam. Dataflow is here, not only unifying batch and streaming, but also the big data world.\\nIn this talk we are going to review Dataflow's differentiating elements and why they matter.  We’ll demonstrate Dataflow’s capabilities through a real-time demo with practical insights on how to manage and visualize streaming data flows.\\n</td>\n    </tr>\n    <tr>\n      <th>358</th>\n      <td>2016</td>\n      <td>Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark.</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>David Whiting</td>\n      <td>https://2016.berlinbuzzwords.de/session/big-data-small-code-using-java-8-and-apache-crunch-quickly-develop-concise-efficient.html</td>\n      <td>New execution platforms may be popping up all the time with the intention of being the \"hot new thing\" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. Apache Crunch is a simple framework on top of MapReduce (with support for running on Spark as well) which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.\\n</td>\n      <td>Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark. Scale New execution platforms may be popping up all the time with the intention of being the \"hot new thing\" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. Apache Crunch is a simple framework on top of MapReduce (with support for running on Spark as well) which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.\\n</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>2015</td>\n      <td>Hive on Spark</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Szehon Ho</td>\n      <td>https://2015.berlinbuzzwords.de/session/hive-spark.html</td>\n      <td>Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop.  Until recently, MapReduce was the only Hadoop execution engine for Hive queries. But today, alternative execution engines are available — such as Apache Spark and Apache Tez.  The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.eins zwo\\nIn this talk we'll discuss the Hive on Spark project.  Topics include the motivations, such as improving Hive user experience and streamlining operational management for Spark shops, some background and comparisons of MapRededuce and Spark, and the technical process of porting a complex real-world application from MapReduce to Spark.  A demo will also be presented.\\n</td>\n      <td>Hive on Spark Scale Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop.  Until recently, MapReduce was the only Hadoop execution engine for Hive queries. But today, alternative execution engines are available — such as Apache Spark and Apache Tez.  The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.eins zwo\\nIn this talk we'll discuss the Hive on Spark project.  Topics include the motivations, such as improving Hive user experience and streamlining operational management for Spark shops, some background and comparisons of MapRededuce and Spark, and the technical process of porting a complex real-world application from MapReduce to Spark.  A demo will also be presented.\\n</td>\n    </tr>\n    <tr>\n      <th>266</th>\n      <td>2017</td>\n      <td>Distributed and Native Hybrid optimizations for Machine Learning Workloads</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Suneel Marthi</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/distributed-and-native-hybrid-optimizations-machine-learning-workloads.html</td>\n      <td>Data scientists love tools like R and Scikit-Learn, as they offer a convenient and familiar syntax for analysis tasks. However, these systems are limited to operating serially on data sets that can fit on a single node and do not allow for distributed execution.\\nMahout-Samsara is a linear algebra environment that offers both an easy-to-use Scala DSL and efficient distributed execution for linear algebra operations. Data scientists transitioning from R to Mahout can use the Samsara DSL for large-scale data sets with familiar R-like semantics. Machine Learning and Deep Learning algorithms built with the Mahout-Samsara DSL are automatically parallelized and optimized to execute on distributed processing engines like Apache Spark and Apache Flink accelerated natively by CUDA, OpenCL and OpenMP.\\nIn this talk, we will look at Mahout's distributed linear algebra capabilities and demonstrate an EigenFaces classification using Distributed SSVD executing on a GPU cluster. This talk will also demonstrate the ease with which one can roll-out new Machine Learning Algorithms using Mahout-Samsara DSL. \\nML practitioners will come away from this talk with a better understanding of how Samsara's linear algebra environment can help simplify developing highly scalable, CPU/GPU accelerated ML and DL algorithms by focusing solely on the declarative specification of the algorithm without having to worry about the implementation details of a scalable distributed engine or having to learn to program with native math libraries.\\n</td>\n      <td>Distributed and Native Hybrid optimizations for Machine Learning Workloads Scale Data scientists love tools like R and Scikit-Learn, as they offer a convenient and familiar syntax for analysis tasks. However, these systems are limited to operating serially on data sets that can fit on a single node and do not allow for distributed execution.\\nMahout-Samsara is a linear algebra environment that offers both an easy-to-use Scala DSL and efficient distributed execution for linear algebra operations. Data scientists transitioning from R to Mahout can use the Samsara DSL for large-scale data sets with familiar R-like semantics. Machine Learning and Deep Learning algorithms built with the Mahout-Samsara DSL are automatically parallelized and optimized to execute on distributed processing engines like Apache Spark and Apache Flink accelerated natively by CUDA, OpenCL and OpenMP.\\nIn this talk, we will look at Mahout's distributed linear algebra capabilities and demonstrate an EigenFaces classification using Distributed SSVD executing on a GPU cluster. This talk will also demonstrate the ease with which one can roll-out new Machine Learning Algorithms using Mahout-Samsara DSL. \\nML practitioners will come away from this talk with a better understanding of how Samsara's linear algebra environment can help simplify developing highly scalable, CPU/GPU accelerated ML and DL algorithms by focusing solely on the declarative specification of the algorithm without having to worry about the implementation details of a scalable distributed engine or having to learn to program with native math libraries.\\n</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>158</th>\n      <td>2019</td>\n      <td>Python, Java, or Go: It's Your Choice with Apache Beam</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Maximilian MichelsIsmaël Mejía</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/python-java-or-go-its-your-choice-apache-beam.html</td>\n      <td>With the rise of Tensorflow and libraries like Numpy, Python has become a popular choice for data processing. Applications built with Python are commonly single-node applications and need to be parallelized in order to scale for big amounts of data. Turns out, JVM-based languages are often the only choice to leverage the power of large-scale data processing tools like Apache Flink or Apache Spark.\\nThis talk introduces Apache Beam, an open-source data processing framework for large-scale batch and stream processing which is designed with portability in mind. Apache Beam lets you use languages like Python, Go, Java, and Scala for data processing. Even better, the resulting programs can be run on the execution engine of your choice.\\nWe will show how easy it is to run data processing jobs on Apache Beam and provide insight into different aspects of Apache Beam's portability architecture. In particular how Beam programs\\nexecute on top of different execution engines like Apache Spark, Apache Flink, or Google Cloud Dataflow\\nsupport multiple languages like Python, Go, and Java\\n\\nApache Beam's portability avoids being locked into a single execution engine or programming language. Moreover, portability enables completely new use cases, e.g. to create data processing jobs which mix multiple languages, to reuse Java IO connectors for loading/storing data from a Python job, or to use libraries (e.g. for machine learning) that do not exist in the main language of the data processing job.\\nPlease join us to learn more about the future of data processing where users are free to choose their programming language and execution engine.\\n \\n</td>\n      <td>Python, Java, or Go: It's Your Choice with Apache Beam Stream With the rise of Tensorflow and libraries like Numpy, Python has become a popular choice for data processing. Applications built with Python are commonly single-node applications and need to be parallelized in order to scale for big amounts of data. Turns out, JVM-based languages are often the only choice to leverage the power of large-scale data processing tools like Apache Flink or Apache Spark.\\nThis talk introduces Apache Beam, an open-source data processing framework for large-scale batch and stream processing which is designed with portability in mind. Apache Beam lets you use languages like Python, Go, Java, and Scala for data processing. Even better, the resulting programs can be run on the execution engine of your choice.\\nWe will show how easy it is to run data processing jobs on Apache Beam and provide insight into different aspects of Apache Beam's portability architecture. In particular how Beam programs\\nexecute on top of different execution engines like Apache Spark, Apache Flink, or Google Cloud Dataflow\\nsupport multiple languages like Python, Go, and Java\\n\\nApache Beam's portability avoids being locked into a single execution engine or programming language. Moreover, portability enables completely new use cases, e.g. to create data processing jobs which mix multiple languages, to reuse Java IO connectors for loading/storing data from a Python job, or to use libraries (e.g. for machine learning) that do not exist in the main language of the data processing job.\\nPlease join us to learn more about the future of data processing where users are free to choose their programming language and execution engine.\\n \\n</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2020</td>\n      <td>First Steps with Apache Beam: Writing Portable Pipelines using Java, Python, Go</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Austin Bennett</td>\n      <td>https://2020.berlinbuzzwords.de/session/first-steps-apache-beam-writing-portable-pipelines-using-java-python-go</td>\n      <td>Apache Beam is an open source unified model for defining data processing pipelines (Batch and strEAM), which allows you to write your pipeline in your language of choice and run it with minimal effort on the execution engine (ex: Apache Spark, Apache Flink, Google Cloud Dataflow) of choice.  In this practical session we will get hands-on writing Beam pipelines, and as well as discuss the fundamentals of Beam programming model, and SDKs (Python, Go, Java).\\nPrerequisites\\nYou will need to install IntelliJ IDEA and/or PyCharm with the EduTools plugin, and with the kata(s) installed in the language of their choice to work through exercises in the online platform.\\nInstructions can be found on the blogpost here: https://beam.apache.org/blog/beam-kata-release/</td>\n      <td>First Steps with Apache Beam: Writing Portable Pipelines using Java, Python, Go Stream Apache Beam is an open source unified model for defining data processing pipelines (Batch and strEAM), which allows you to write your pipeline in your language of choice and run it with minimal effort on the execution engine (ex: Apache Spark, Apache Flink, Google Cloud Dataflow) of choice.  In this practical session we will get hands-on writing Beam pipelines, and as well as discuss the fundamentals of Beam programming model, and SDKs (Python, Go, Java).\\nPrerequisites\\nYou will need to install IntelliJ IDEA and/or PyCharm with the EduTools plugin, and with the kata(s) installed in the language of their choice to work through exercises in the online platform.\\nInstructions can be found on the blogpost here: https://beam.apache.org/blog/beam-kata-release/</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>2017</td>\n      <td>Using Apache Beam to create a unified benchmarking framework for streaming and batch systems</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Ismaël Mejía</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/using-apache-beam-create-unified-benchmarking-framework-streaming-and-batch-systems.html</td>\n      <td>In the relational database world, there are different benchmarks to evaluate the correctness and performance of different databases, for example the TPC suites, In this talk we will motivate the need for a benchmark framework to evaluate both stream and batch processing systems.\\n\\nWe will introduce Nexmark, a framework to evaluate queries over data streams and discuss its implementation on Apache Beam, and the properties that make Apache Beam the perfect tool to develop a benchmark framework. Nexmark was an integration test donated by Google as part of the Apache Beam incubation process and we have been working to evolve it since. Nexmark not only bridges the gap for evaluating data processing frameworks, but also serves as a rich integration test of the correct implementation of both the Beam runners (for systems like Apache Spark, Apache Flink and Apache Apex), and the new features of the Beam SDK that we will also present.</td>\n      <td>Using Apache Beam to create a unified benchmarking framework for streaming and batch systems Stream In the relational database world, there are different benchmarks to evaluate the correctness and performance of different databases, for example the TPC suites, In this talk we will motivate the need for a benchmark framework to evaluate both stream and batch processing systems.\\n\\nWe will introduce Nexmark, a framework to evaluate queries over data streams and discuss its implementation on Apache Beam, and the properties that make Apache Beam the perfect tool to develop a benchmark framework. Nexmark was an integration test donated by Google as part of the Apache Beam incubation process and we have been working to evolve it since. Nexmark not only bridges the gap for evaluating data processing frameworks, but also serves as a rich integration test of the correct implementation of both the Beam runners (for systems like Apache Spark, Apache Flink and Apache Apex), and the new features of the Beam SDK that we will also present.</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2019</td>\n      <td>Writing a Distributed Ray Tracer with Apache Beam, Abridged</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Robert Burke</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/writing-distributed-ray-tracer-apache-beam-abridged.html</td>\n      <td>Ray Tracing is an embarrassingly parallel way to render high quality images, but distributing work between machines is hard. Apache Beam is a model for efficient distributed data processing. In this talk I map Ray Tracing onto the Apache Beam Go SDK.\\nApache Beam SDKs portably abstract computations and data into DoFns and PCollections, and allow you to construct a graph of how data flows and connects. Then any compatible Runner can optimize that graph for computation, and distribute work to the runners.\\nI'll give a overview of Ray Tracing  and introduce the Apache Beam model, it's history,  demonstrating mapping one to the other. I'll explain the benefits and limits of the model and how to get the most out of your pipelines, and show it running on compatible runners like Apache Flink and Google Cloud Dataflow. Further, I'll show using the same pipeline running in batch and streaming modes, with minimal changes.\\n</td>\n      <td>Writing a Distributed Ray Tracer with Apache Beam, Abridged Stream Ray Tracing is an embarrassingly parallel way to render high quality images, but distributing work between machines is hard. Apache Beam is a model for efficient distributed data processing. In this talk I map Ray Tracing onto the Apache Beam Go SDK.\\nApache Beam SDKs portably abstract computations and data into DoFns and PCollections, and allow you to construct a graph of how data flows and connects. Then any compatible Runner can optimize that graph for computation, and distribute work to the runners.\\nI'll give a overview of Ray Tracing  and introduce the Apache Beam model, it's history,  demonstrating mapping one to the other. I'll explain the benefits and limits of the model and how to get the most out of your pipelines, and show it running on compatible runners like Apache Flink and Google Cloud Dataflow. Further, I'll show using the same pipeline running in batch and streaming modes, with minimal changes.\\n</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>2019</td>\n      <td>Apache Beam pipelines at 100TB+ scale using Apache Spark.</td>\n      <td>Advanced</td>\n      <td>Scale</td>\n      <td>David Moravek</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/apache-beam-pipelines-100tb-scale-using-apache-spark.html</td>\n      <td>At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.\\nThis talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.\\n</td>\n      <td>Apache Beam pipelines at 100TB+ scale using Apache Spark. Scale At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.\\nThis talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.\\n</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>2019</td>\n      <td>Streaming your shared ride</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Thomas Weise</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/streaming-your-shared-ride.html</td>\n      <td>Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.\\nEnablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.\\nTopics covered in this talk include:\\nOverview of use cases and platform architecture\\nStreaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping\\nStateful streaming computation with scalability, high availability and low latency processing on Apache Flink\\nDevelopment frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python\\nPython with Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink\\nKubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications\\n</td>\n      <td>Streaming your shared ride Stream Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.\\nEnablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.\\nTopics covered in this talk include:\\nOverview of use cases and platform architecture\\nStreaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping\\nStateful streaming computation with scalability, high availability and low latency processing on Apache Flink\\nDevelopment frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python\\nPython with Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink\\nKubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications\\n</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>2016</td>\n      <td>Google Dataflow: The new open model for batch and stream processing</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Felipe Hoffa</td>\n      <td>https://2016.berlinbuzzwords.de/session/google-dataflow-new-open-model-batch-and-stream-processing.html</td>\n      <td>In 2004 Google published the MapReduce paper, a programming model that kick-started big data as we know it. Ten years later, Google introduced Dataflow - a new paradigm for big data, integrating batch and stream processing in one common abstraction. This time the offer was more than a paper, but also an open source Java SDK and a cloud managed service to run it. In 2016 big data players like Cask, Cloudera, Data Artisans, PayPal, Slack, Talend joined Google to propose Dataflow for incubation at the Apache Software Foundation - now accepted as Apache Beam. Dataflow is here, not only unifying batch and streaming, but also the big data world.\\nIn this talk we are going to review Dataflow's differentiating elements and why they matter.  We’ll demonstrate Dataflow’s capabilities through a real-time demo with practical insights on how to manage and visualize streaming data flows.\\n</td>\n      <td>Google Dataflow: The new open model for batch and stream processing Scale In 2004 Google published the MapReduce paper, a programming model that kick-started big data as we know it. Ten years later, Google introduced Dataflow - a new paradigm for big data, integrating batch and stream processing in one common abstraction. This time the offer was more than a paper, but also an open source Java SDK and a cloud managed service to run it. In 2016 big data players like Cask, Cloudera, Data Artisans, PayPal, Slack, Talend joined Google to propose Dataflow for incubation at the Apache Software Foundation - now accepted as Apache Beam. Dataflow is here, not only unifying batch and streaming, but also the big data world.\\nIn this talk we are going to review Dataflow's differentiating elements and why they matter.  We’ll demonstrate Dataflow’s capabilities through a real-time demo with practical insights on how to manage and visualize streaming data flows.\\n</td>\n    </tr>\n    <tr>\n      <th>358</th>\n      <td>2016</td>\n      <td>Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark.</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>David Whiting</td>\n      <td>https://2016.berlinbuzzwords.de/session/big-data-small-code-using-java-8-and-apache-crunch-quickly-develop-concise-efficient.html</td>\n      <td>New execution platforms may be popping up all the time with the intention of being the \"hot new thing\" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. Apache Crunch is a simple framework on top of MapReduce (with support for running on Spark as well) which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.\\n</td>\n      <td>Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark. Scale New execution platforms may be popping up all the time with the intention of being the \"hot new thing\" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. Apache Crunch is a simple framework on top of MapReduce (with support for running on Spark as well) which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.\\n</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>2015</td>\n      <td>Hive on Spark</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Szehon Ho</td>\n      <td>https://2015.berlinbuzzwords.de/session/hive-spark.html</td>\n      <td>Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop.  Until recently, MapReduce was the only Hadoop execution engine for Hive queries. But today, alternative execution engines are available — such as Apache Spark and Apache Tez.  The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.eins zwo\\nIn this talk we'll discuss the Hive on Spark project.  Topics include the motivations, such as improving Hive user experience and streamlining operational management for Spark shops, some background and comparisons of MapRededuce and Spark, and the technical process of porting a complex real-world application from MapReduce to Spark.  A demo will also be presented.\\n</td>\n      <td>Hive on Spark Scale Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop.  Until recently, MapReduce was the only Hadoop execution engine for Hive queries. But today, alternative execution engines are available — such as Apache Spark and Apache Tez.  The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.eins zwo\\nIn this talk we'll discuss the Hive on Spark project.  Topics include the motivations, such as improving Hive user experience and streamlining operational management for Spark shops, some background and comparisons of MapRededuce and Spark, and the technical process of porting a complex real-world application from MapReduce to Spark.  A demo will also be presented.\\n</td>\n    </tr>\n    <tr>\n      <th>266</th>\n      <td>2017</td>\n      <td>Distributed and Native Hybrid optimizations for Machine Learning Workloads</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Suneel Marthi</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/distributed-and-native-hybrid-optimizations-machine-learning-workloads.html</td>\n      <td>Data scientists love tools like R and Scikit-Learn, as they offer a convenient and familiar syntax for analysis tasks. However, these systems are limited to operating serially on data sets that can fit on a single node and do not allow for distributed execution.\\nMahout-Samsara is a linear algebra environment that offers both an easy-to-use Scala DSL and efficient distributed execution for linear algebra operations. Data scientists transitioning from R to Mahout can use the Samsara DSL for large-scale data sets with familiar R-like semantics. Machine Learning and Deep Learning algorithms built with the Mahout-Samsara DSL are automatically parallelized and optimized to execute on distributed processing engines like Apache Spark and Apache Flink accelerated natively by CUDA, OpenCL and OpenMP.\\nIn this talk, we will look at Mahout's distributed linear algebra capabilities and demonstrate an EigenFaces classification using Distributed SSVD executing on a GPU cluster. This talk will also demonstrate the ease with which one can roll-out new Machine Learning Algorithms using Mahout-Samsara DSL. \\nML practitioners will come away from this talk with a better understanding of how Samsara's linear algebra environment can help simplify developing highly scalable, CPU/GPU accelerated ML and DL algorithms by focusing solely on the declarative specification of the algorithm without having to worry about the implementation details of a scalable distributed engine or having to learn to program with native math libraries.\\n</td>\n      <td>Distributed and Native Hybrid optimizations for Machine Learning Workloads Scale Data scientists love tools like R and Scikit-Learn, as they offer a convenient and familiar syntax for analysis tasks. However, these systems are limited to operating serially on data sets that can fit on a single node and do not allow for distributed execution.\\nMahout-Samsara is a linear algebra environment that offers both an easy-to-use Scala DSL and efficient distributed execution for linear algebra operations. Data scientists transitioning from R to Mahout can use the Samsara DSL for large-scale data sets with familiar R-like semantics. Machine Learning and Deep Learning algorithms built with the Mahout-Samsara DSL are automatically parallelized and optimized to execute on distributed processing engines like Apache Spark and Apache Flink accelerated natively by CUDA, OpenCL and OpenMP.\\nIn this talk, we will look at Mahout's distributed linear algebra capabilities and demonstrate an EigenFaces classification using Distributed SSVD executing on a GPU cluster. This talk will also demonstrate the ease with which one can roll-out new Machine Learning Algorithms using Mahout-Samsara DSL. \\nML practitioners will come away from this talk with a better understanding of how Samsara's linear algebra environment can help simplify developing highly scalable, CPU/GPU accelerated ML and DL algorithms by focusing solely on the declarative specification of the algorithm without having to worry about the implementation details of a scalable distributed engine or having to learn to program with native math libraries.\\n</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>306</th>\n      <td>2017</td>\n      <td>BM25 is so Yesterday: Modern Techniques for Better Search Relevance</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Grant Ingersoll</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/bm25-so-yesterday-modern-techniques-better-search-relevance.html</td>\n      <td>Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene.  And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance.  This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it.  We’ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr.  We’ll finish with a sneak peak into using deep learning and word2vec in a search context.\\n</td>\n      <td>BM25 is so Yesterday: Modern Techniques for Better Search Relevance Search Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene.  And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance.  This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it.  We’ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr.  We’ll finish with a sneak peak into using deep learning and word2vec in a search context.\\n</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>2018</td>\n      <td>Machine Learning for Apache Solr, Elasticsearch &amp; Vespa</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Mitali Jha</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/machine-learning-apache-solr-elasticsearch-vespa.html</td>\n      <td>Apache Solr, Elasticsearch and Vespa are three of the most popular general purpose search engines that are used in search, recommendations and analytics based applications. Machine learning is a critical aid to improving relevance beyond the native ranking algorithms (e.g. TF-IDF, BM25 etc.) by leveraging editorial judgements and user behaviour and factoring them into ranking of results. This is known as \"learning to rank\" (LTR) or \"machine learned ranking\" (MLR) etc.\\nIn this talk, the speaker presents a comparison of machine learning frameworks across all of these search engines [1]. The comparison is followed by a quick demonstration on how to use all of these frameworks. This talk is aimed at those who are looking to decide upon which search engine to use in their applications based on the machine learned ranking capabilities available for it, and the ease of using such features.\\n[1] 1. LTR based re-ranking module in Solr, https://lucene.apache.org/solr/guide/7_2/learning-to-rank.html       2. Elasticsearch's LTR plugin, https://github.com/o19s/elasticsearch-learning-to-rank       3. Vespa's MLR functionality, http://docs.vespa.ai/documentation/ranking.html\\n \\n</td>\n      <td>Machine Learning for Apache Solr, Elasticsearch &amp; Vespa Search Apache Solr, Elasticsearch and Vespa are three of the most popular general purpose search engines that are used in search, recommendations and analytics based applications. Machine learning is a critical aid to improving relevance beyond the native ranking algorithms (e.g. TF-IDF, BM25 etc.) by leveraging editorial judgements and user behaviour and factoring them into ranking of results. This is known as \"learning to rank\" (LTR) or \"machine learned ranking\" (MLR) etc.\\nIn this talk, the speaker presents a comparison of machine learning frameworks across all of these search engines [1]. The comparison is followed by a quick demonstration on how to use all of these frameworks. This talk is aimed at those who are looking to decide upon which search engine to use in their applications based on the machine learned ranking capabilities available for it, and the ease of using such features.\\n[1] 1. LTR based re-ranking module in Solr, https://lucene.apache.org/solr/guide/7_2/learning-to-rank.html       2. Elasticsearch's LTR plugin, https://github.com/o19s/elasticsearch-learning-to-rank       3. Vespa's MLR functionality, http://docs.vespa.ai/documentation/ranking.html\\n \\n</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>2019</td>\n      <td>Why data-driven methods will shape the future of relevance search</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Pedro Balage</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/why-data-driven-methods-will-shape-future-relevance-search.html</td>\n      <td>You already have your search engine in place, users have started using it and now see that your results could do better. You hire some engineers to get you through this and they included a synonym here, an exception there, and asked you to create a taxonomy to organize your items. After a few months,  everything seems to work fine except for a few minor issues that you got your team working on. \\nNow, your company needs to expand to meet the new demands of your business, so you get a wider range of products in your catalog. Your search results start to show some issues and your team is having a hard time controlling all of your search parameters and maintaining rules and exceptions. You know that improving your results will be the key to the success of your company. You now get yourself the biggest decision to make: should I hire more relevance engineers to handle it?\\nThe first part of this talk will show why it is so complicated to scale an engineering approach of relevance based only on synonyms, taxonomies, rules, and exceptions. As a consequence of these limitations, the second part will focus on the latest advancements of natural language processing and information retrieval to show that all you need are good data scientists responsible for providing new data-driven solutions to your needs without causing any change into your search stack. \\nWith the vast amounts of data collected, you can find applications of it for all your search needs. The talk will guide you through topics such as language modeling for autocompletion, deep neural networks for named-entity recognition, network embeddings for relevance score, and query-product embeddings to improve the discoverability of your most exquisite items. All of this, optimized by your learning-to-rank algorithm and enabling you to personalization capabilities.\\nWith all of these data-driven approaches, your search engine will be actively learning from the interaction of your users and there will be no need to hire an entire department of relevance engineers. \\n</td>\n      <td>Why data-driven methods will shape the future of relevance search Search You already have your search engine in place, users have started using it and now see that your results could do better. You hire some engineers to get you through this and they included a synonym here, an exception there, and asked you to create a taxonomy to organize your items. After a few months,  everything seems to work fine except for a few minor issues that you got your team working on. \\nNow, your company needs to expand to meet the new demands of your business, so you get a wider range of products in your catalog. Your search results start to show some issues and your team is having a hard time controlling all of your search parameters and maintaining rules and exceptions. You know that improving your results will be the key to the success of your company. You now get yourself the biggest decision to make: should I hire more relevance engineers to handle it?\\nThe first part of this talk will show why it is so complicated to scale an engineering approach of relevance based only on synonyms, taxonomies, rules, and exceptions. As a consequence of these limitations, the second part will focus on the latest advancements of natural language processing and information retrieval to show that all you need are good data scientists responsible for providing new data-driven solutions to your needs without causing any change into your search stack. \\nWith the vast amounts of data collected, you can find applications of it for all your search needs. The talk will guide you through topics such as language modeling for autocompletion, deep neural networks for named-entity recognition, network embeddings for relevance score, and query-product embeddings to improve the discoverability of your most exquisite items. All of this, optimized by your learning-to-rank algorithm and enabling you to personalization capabilities.\\nWith all of these data-driven approaches, your search engine will be actively learning from the interaction of your users and there will be no need to hire an entire department of relevance engineers. \\n</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>2019</td>\n      <td>Building an AI/ML powered text search system</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Nick Burch</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/building-aiml-powered-text-search-system.html</td>\n      <td>There are some great Open Source text search / information retrieval systems, such as Apache SOLR and ElasticSearch. But could an AI / ML powered solution do better? And how would you even go about building one?\\nBased on our experiences building a knowledge base / Q&amp;A system, we'll guide you through the process. Learn how to get your text into a format that AI / ML techniques can work on, and how to build a simple model and recommender. Then it's Deep Learning and Neural Networks, and finally updating the models with real user feedback. Oh, and comparing it to a traditional search engine, to see if it's actually any better...\\nNo AI or hard-core-search experience needed, we'll show you the code and techniques required to create your own!\\n</td>\n      <td>Building an AI/ML powered text search system Search There are some great Open Source text search / information retrieval systems, such as Apache SOLR and ElasticSearch. But could an AI / ML powered solution do better? And how would you even go about building one?\\nBased on our experiences building a knowledge base / Q&amp;A system, we'll guide you through the process. Learn how to get your text into a format that AI / ML techniques can work on, and how to build a simple model and recommender. Then it's Deep Learning and Neural Networks, and finally updating the models with real user feedback. Oh, and comparing it to a traditional search engine, to see if it's actually any better...\\nNo AI or hard-core-search experience needed, we'll show you the code and techniques required to create your own!\\n</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2021</td>\n      <td>Beyond Artificial Intelligence for Search</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Lucian Precup</td>\n      <td>https://2021.berlinbuzzwords.de/session/beyond-artificial-intelligence-search</td>\n      <td>It is proven that for relatively well-structured data, like in e-commerce for example, a hand tailored search configuration can easily outperform machine learning approaches for relevance. The search configuration considers the different searchable fields, a business taxonomy and ontology, some domain related synonyms, a few specific landing pages, boosts and some business numerical criteria.\\nIn the same way, we describe an approach for relevance in the case of large-scale search engines which is not based on classical \"PageRank\" and machine learning approaches. We propose a model based on social interactions between communities and individuals that are using or configuring the search engine. We then compare this model with machine learning powered approaches.</td>\n      <td>Beyond Artificial Intelligence for Search Search It is proven that for relatively well-structured data, like in e-commerce for example, a hand tailored search configuration can easily outperform machine learning approaches for relevance. The search configuration considers the different searchable fields, a business taxonomy and ontology, some domain related synonyms, a few specific landing pages, boosts and some business numerical criteria.\\nIn the same way, we describe an approach for relevance in the case of large-scale search engines which is not based on classical \"PageRank\" and machine learning approaches. We propose a model based on social interactions between communities and individuals that are using or configuring the search engine. We then compare this model with machine learning powered approaches.</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>2019</td>\n      <td>Building an enterprise Natural Language Search Engine with ElasticSearch and Facebook’s DrQA</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Debmalya BiswasLouis Baligand</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/building-enterprise-natural-language-search-engine-elasticsearch-and-facebooks-drqa.html</td>\n      <td>Modern search engines leverage Natural Language Processing (NLP) and Machine Learning (ML) to improve relevance of results. In this presentation, we focus on the specific field of ‘Enterprise Search’, whose primary goal is to make domain specific company data and documents readily accessible to employees to improve their productivity and promote collaboration. Indeed, any large organization produces vast amounts of documentation about their specific systems, technologies and processes. The question then is - “How can we speed up search-driven activities and enhance user experience for the enterprise?”\\nFacebook AI research team has developed and open sourced a tool to answer questions by reading Wikipedia articles, called DrQA (Chen, et al. 2017). DrQA is based on a 2-stage Q&amp;A pipeline: (i) Retriever: retrieve the top-k relevant documents (pages), followed by (ii) Reader: determining the most relevant answer span among the retrieved documents (pages). We applied it on an enterprise use-case to search over machine manuals used by factory operators.\\nWe present an architecture integrating ElasticSearch in the DrQA pipeline, which has been contributed upstream and is now available from the official DrQA github repository (https://github.com/facebookresearch/DrQA). The end result is a very scalable search engine that can be deployed on any document repository in your enterprise containing Microsoft Office docs, ppts, emails, pdf documents, etc. Simply point it to your ElasticSearch index and it will be able to provide ‘very precise’ answers based on your documents, thanks to the pre-trained Deep Learning Q&amp;A model of DrQA. We discuss the learnings along the creation and the limitations of such an engine, e.g. scenarios where it excels by identifying precise answers and how it performs compared to a non-ML approach or a typical keyword based search.\\n</td>\n      <td>Building an enterprise Natural Language Search Engine with ElasticSearch and Facebook’s DrQA Search Modern search engines leverage Natural Language Processing (NLP) and Machine Learning (ML) to improve relevance of results. In this presentation, we focus on the specific field of ‘Enterprise Search’, whose primary goal is to make domain specific company data and documents readily accessible to employees to improve their productivity and promote collaboration. Indeed, any large organization produces vast amounts of documentation about their specific systems, technologies and processes. The question then is - “How can we speed up search-driven activities and enhance user experience for the enterprise?”\\nFacebook AI research team has developed and open sourced a tool to answer questions by reading Wikipedia articles, called DrQA (Chen, et al. 2017). DrQA is based on a 2-stage Q&amp;A pipeline: (i) Retriever: retrieve the top-k relevant documents (pages), followed by (ii) Reader: determining the most relevant answer span among the retrieved documents (pages). We applied it on an enterprise use-case to search over machine manuals used by factory operators.\\nWe present an architecture integrating ElasticSearch in the DrQA pipeline, which has been contributed upstream and is now available from the official DrQA github repository (https://github.com/facebookresearch/DrQA). The end result is a very scalable search engine that can be deployed on any document repository in your enterprise containing Microsoft Office docs, ppts, emails, pdf documents, etc. Simply point it to your ElasticSearch index and it will be able to provide ‘very precise’ answers based on your documents, thanks to the pre-trained Deep Learning Q&amp;A model of DrQA. We discuss the learnings along the creation and the limitations of such an engine, e.g. scenarios where it excels by identifying precise answers and how it performs compared to a non-ML approach or a typical keyword based search.\\n</td>\n    </tr>\n    <tr>\n      <th>269</th>\n      <td>2017</td>\n      <td>The modern architecture of search</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Alaa ElhadbaMikio Braun</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/modern-architecture-search.html</td>\n      <td>Information Retrieval (IR) systems are a vital component in the core of successful modern web platforms.\\nThe main goal of IR systems is to provide a communication layer that enables customers to establish a retrieval dialogue with underlying data.\\nThe immense explosion of unstructured data drives modern search application to go beyond just fuzzy string matching, to invest in deep understanding of user queries through interpretation of user intention in order to respond with a relevant result set. \\nThe modern architecture of search is a design of a data-driven IR system that covers the following: \\n     - Data ingestion pipelines from various sources\\n     - Data retrieval and the lifecycle of a user search query\\n     - Machine learned relevance ranking \\n     - Personalised search\\n     - Search performance tracking and quality assessment\\nThe talk will discuss the components needed to build an eco-system that is designed to solve the problems of IR in web platforms. What role can Machine learning play in search relevancy? how natural language processing can help provide a solid understanding of search phrases? How data can drive a personalized search experience? What are the challenges of maintaining such a complex system?  \\n \\n#ml #nlp #ir #search_relevancy #architecture #big_data\\n____\\nAlae Elhadba\\nResearch Engineer @ Zalando \\n</td>\n      <td>The modern architecture of search Search Information Retrieval (IR) systems are a vital component in the core of successful modern web platforms.\\nThe main goal of IR systems is to provide a communication layer that enables customers to establish a retrieval dialogue with underlying data.\\nThe immense explosion of unstructured data drives modern search application to go beyond just fuzzy string matching, to invest in deep understanding of user queries through interpretation of user intention in order to respond with a relevant result set. \\nThe modern architecture of search is a design of a data-driven IR system that covers the following: \\n     - Data ingestion pipelines from various sources\\n     - Data retrieval and the lifecycle of a user search query\\n     - Machine learned relevance ranking \\n     - Personalised search\\n     - Search performance tracking and quality assessment\\nThe talk will discuss the components needed to build an eco-system that is designed to solve the problems of IR in web platforms. What role can Machine learning play in search relevancy? how natural language processing can help provide a solid understanding of search phrases? How data can drive a personalized search experience? What are the challenges of maintaining such a complex system?  \\n \\n#ml #nlp #ir #search_relevancy #architecture #big_data\\n____\\nAlae Elhadba\\nResearch Engineer @ Zalando \\n</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>2018</td>\n      <td>Your Search Service as a Composable Function</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Stefanie SchirmerAakash Sabharwal</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/your-search-service-composable-function-0.html</td>\n      <td>Modern search systems at scale are often architected as a real-time processing pipeline where the query and its results flow through multiple stages before returning them to the user. Concretely, a  search query flowing through this pipeline might be stemmed by a stemmer, tokenized by a \"parts of speech\" natural language parser, transformed to a query plan conforming to a boolean retrieval model, executed against an inverted index,  and finally the top-k results could be reordered based on an online machine-learned ranker. Many of these transformations of queries and results require performing a network IO to external specialized services. Our proposal is to model the search pipeline as a monad transformer composed of Reader monad and a Future.\\n \\nAbout 1500 people search for handmade and vintage items on Etsy every second. Several different backends power Etsy's search, among them Solr, Elasticsearch, our own key-value-store Arizona, and services for machine learning and inference. How do all these systems work together, present a common interface to Etsy's developers and a coherent search experience to our users?\\n \\nThis problem requires a distributed system that scales very well, and has a state space that is still easy to reason about. We have built a smart proxy in scala with minimal state to solve this problem. It expands on the ideas of the \"Your Server as a Function\" paper. The idea is that basically all program state comes in via the request encoded in a Reader monad, the proxy calls out to the appropriate backend services and combines their responses, behaving like a pure function.\\n \\nThe proxy retrieves search results from a Solr backend, ranks them based on a machine learning model that incorporates the user's context, and returns the ranked results. If a search is unsuccessful, it can decide to kick off alternative search requests and return their results instead, without needing any frontend interaction. Via an intermediate tree representation that encodes a boolean retrieval model which is independent of the backends, we can combine key-value-store, machine learning and search indexes to improve the search result for the end user. The interplay of the different backends to create new services is achieved via a configuration system. It makes heavy use of Scala's type system to lift the types from the data fields in the storage backends into the scala code and ensure consistent types from the incoming thrift request throughout the entire system.\\n \\nTwitter’s Finagle library provides us with RPC clients that abstract over different protocols like thrift, mux or http and lets us interact with them in an asynchronous manner via Futures. Using Futures, we can build custom machine learning pipelines by composing sequentially &amp; concurrently over calls to backend services. It becomes easy to add custom query pipelines for machine learning, such as a recommendation query pipeline that queries a user database, gets the user’s purchase history and favorites, and constructs a recommendation set using a model that takes the user context as input. With experimentation as a first class citizen in our design, A/B testing allows us to test different feature sets and modelling techniques in our machine learning services.\\n \\nIn order to maintain independent failure domains for suggestions and search we run the proxy on Kubernetes as separate services. The Kubernetes autoscaler helps us to react to changing traffic patterns in a flexible way. We will talk about our learnings along the way of building this proxy, and trying to find the right abstraction for the search problem.\\n \\n</td>\n      <td>Your Search Service as a Composable Function Search Modern search systems at scale are often architected as a real-time processing pipeline where the query and its results flow through multiple stages before returning them to the user. Concretely, a  search query flowing through this pipeline might be stemmed by a stemmer, tokenized by a \"parts of speech\" natural language parser, transformed to a query plan conforming to a boolean retrieval model, executed against an inverted index,  and finally the top-k results could be reordered based on an online machine-learned ranker. Many of these transformations of queries and results require performing a network IO to external specialized services. Our proposal is to model the search pipeline as a monad transformer composed of Reader monad and a Future.\\n \\nAbout 1500 people search for handmade and vintage items on Etsy every second. Several different backends power Etsy's search, among them Solr, Elasticsearch, our own key-value-store Arizona, and services for machine learning and inference. How do all these systems work together, present a common interface to Etsy's developers and a coherent search experience to our users?\\n \\nThis problem requires a distributed system that scales very well, and has a state space that is still easy to reason about. We have built a smart proxy in scala with minimal state to solve this problem. It expands on the ideas of the \"Your Server as a Function\" paper. The idea is that basically all program state comes in via the request encoded in a Reader monad, the proxy calls out to the appropriate backend services and combines their responses, behaving like a pure function.\\n \\nThe proxy retrieves search results from a Solr backend, ranks them based on a machine learning model that incorporates the user's context, and returns the ranked results. If a search is unsuccessful, it can decide to kick off alternative search requests and return their results instead, without needing any frontend interaction. Via an intermediate tree representation that encodes a boolean retrieval model which is independent of the backends, we can combine key-value-store, machine learning and search indexes to improve the search result for the end user. The interplay of the different backends to create new services is achieved via a configuration system. It makes heavy use of Scala's type system to lift the types from the data fields in the storage backends into the scala code and ensure consistent types from the incoming thrift request throughout the entire system.\\n \\nTwitter’s Finagle library provides us with RPC clients that abstract over different protocols like thrift, mux or http and lets us interact with them in an asynchronous manner via Futures. Using Futures, we can build custom machine learning pipelines by composing sequentially &amp; concurrently over calls to backend services. It becomes easy to add custom query pipelines for machine learning, such as a recommendation query pipeline that queries a user database, gets the user’s purchase history and favorites, and constructs a recommendation set using a model that takes the user context as input. With experimentation as a first class citizen in our design, A/B testing allows us to test different feature sets and modelling techniques in our machine learning services.\\n \\nIn order to maintain independent failure domains for suggestions and search we run the proxy on Kubernetes as separate services. The Kubernetes autoscaler helps us to react to changing traffic patterns in a flexible way. We will talk about our learnings along the way of building this proxy, and trying to find the right abstraction for the search problem.\\n \\n</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>2019</td>\n      <td>Evolution of Yelp search to a ranking platform</td>\n      <td>Advanced</td>\n      <td>Search</td>\n      <td>Umesh Dangat</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/evolution-yelp-search-ranking-platform.html</td>\n      <td>Yelp’s core business search was one of the oldest systems at Yelp designed well before the advent of Elasticsearch. While this system served Yelp well for a few years, we were getting close to a point where the original search infrastructure architecture was not sufficient to solve modern day search problems.\\nThis talk will detail\\nHow Yelp’s search engineers decoupled the search infrastructure from search relevance.\\nThe challenges associated with transferring complex custom lucene based ranking and text analysis functionality to elasticsearch.\\nThe benefits of offloading search infrastructure to elasticsearch and\\nFinally the dividends it paid by allowing us to further leverage our technological investment in elasticsearch, by hosting machine learning models in elasticsearch by using Learning to Rank plugin, and making contributions to it.\\n</td>\n      <td>Evolution of Yelp search to a ranking platform Search Yelp’s core business search was one of the oldest systems at Yelp designed well before the advent of Elasticsearch. While this system served Yelp well for a few years, we were getting close to a point where the original search infrastructure architecture was not sufficient to solve modern day search problems.\\nThis talk will detail\\nHow Yelp’s search engineers decoupled the search infrastructure from search relevance.\\nThe challenges associated with transferring complex custom lucene based ranking and text analysis functionality to elasticsearch.\\nThe benefits of offloading search infrastructure to elasticsearch and\\nFinally the dividends it paid by allowing us to further leverage our technological investment in elasticsearch, by hosting machine learning models in elasticsearch by using Learning to Rank plugin, and making contributions to it.\\n</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>2018</td>\n      <td>Head-N-Tail Analysis to Increase Engagement</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Amrit Sarkar</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/head-n-tail-analysis-increase-engagement.html</td>\n      <td>Search engines have come a long way since the early days of reliance on information retrieval as user intent drives modern search applications.  Businesses driven by search like eCommerce, content-based companies lose a small but decent amount of traffic to queries getting a lower number of impressions/clicks/conversions as few or irrelevant results are computed and shown. This session focus on identifying poor/low performing queries and terms using Head and Tail Analysis, classify with potential reasons and suggest corrections or improved query models for them. The session concludes with emphasizing on experiments being vital to the entire process of improving search relevance. The session is based on Head-n-Tail Analysis in Fusion 4 by Chao Han, VP Head of Research, Lucidworks.\\nThis talk is presented by Lucidworks.</td>\n      <td>Head-N-Tail Analysis to Increase Engagement Search Search engines have come a long way since the early days of reliance on information retrieval as user intent drives modern search applications.  Businesses driven by search like eCommerce, content-based companies lose a small but decent amount of traffic to queries getting a lower number of impressions/clicks/conversions as few or irrelevant results are computed and shown. This session focus on identifying poor/low performing queries and terms using Head and Tail Analysis, classify with potential reasons and suggest corrections or improved query models for them. The session concludes with emphasizing on experiments being vital to the entire process of improving search relevance. The session is based on Head-n-Tail Analysis in Fusion 4 by Chao Han, VP Head of Research, Lucidworks.\\nThis talk is presented by Lucidworks.</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>158</th>\n      <td>2019</td>\n      <td>Python, Java, or Go: It's Your Choice with Apache Beam</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Maximilian MichelsIsmaël Mejía</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/python-java-or-go-its-your-choice-apache-beam.html</td>\n      <td>With the rise of Tensorflow and libraries like Numpy, Python has become a popular choice for data processing. Applications built with Python are commonly single-node applications and need to be parallelized in order to scale for big amounts of data. Turns out, JVM-based languages are often the only choice to leverage the power of large-scale data processing tools like Apache Flink or Apache Spark.\\nThis talk introduces Apache Beam, an open-source data processing framework for large-scale batch and stream processing which is designed with portability in mind. Apache Beam lets you use languages like Python, Go, Java, and Scala for data processing. Even better, the resulting programs can be run on the execution engine of your choice.\\nWe will show how easy it is to run data processing jobs on Apache Beam and provide insight into different aspects of Apache Beam's portability architecture. In particular how Beam programs\\nexecute on top of different execution engines like Apache Spark, Apache Flink, or Google Cloud Dataflow\\nsupport multiple languages like Python, Go, and Java\\n\\nApache Beam's portability avoids being locked into a single execution engine or programming language. Moreover, portability enables completely new use cases, e.g. to create data processing jobs which mix multiple languages, to reuse Java IO connectors for loading/storing data from a Python job, or to use libraries (e.g. for machine learning) that do not exist in the main language of the data processing job.\\nPlease join us to learn more about the future of data processing where users are free to choose their programming language and execution engine.\\n \\n</td>\n      <td>Python, Java, or Go: It's Your Choice with Apache Beam Stream With the rise of Tensorflow and libraries like Numpy, Python has become a popular choice for data processing. Applications built with Python are commonly single-node applications and need to be parallelized in order to scale for big amounts of data. Turns out, JVM-based languages are often the only choice to leverage the power of large-scale data processing tools like Apache Flink or Apache Spark.\\nThis talk introduces Apache Beam, an open-source data processing framework for large-scale batch and stream processing which is designed with portability in mind. Apache Beam lets you use languages like Python, Go, Java, and Scala for data processing. Even better, the resulting programs can be run on the execution engine of your choice.\\nWe will show how easy it is to run data processing jobs on Apache Beam and provide insight into different aspects of Apache Beam's portability architecture. In particular how Beam programs\\nexecute on top of different execution engines like Apache Spark, Apache Flink, or Google Cloud Dataflow\\nsupport multiple languages like Python, Go, and Java\\n\\nApache Beam's portability avoids being locked into a single execution engine or programming language. Moreover, portability enables completely new use cases, e.g. to create data processing jobs which mix multiple languages, to reuse Java IO connectors for loading/storing data from a Python job, or to use libraries (e.g. for machine learning) that do not exist in the main language of the data processing job.\\nPlease join us to learn more about the future of data processing where users are free to choose their programming language and execution engine.\\n \\n</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2020</td>\n      <td>First Steps with Apache Beam: Writing Portable Pipelines using Java, Python, Go</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Austin Bennett</td>\n      <td>https://2020.berlinbuzzwords.de/session/first-steps-apache-beam-writing-portable-pipelines-using-java-python-go</td>\n      <td>Apache Beam is an open source unified model for defining data processing pipelines (Batch and strEAM), which allows you to write your pipeline in your language of choice and run it with minimal effort on the execution engine (ex: Apache Spark, Apache Flink, Google Cloud Dataflow) of choice.  In this practical session we will get hands-on writing Beam pipelines, and as well as discuss the fundamentals of Beam programming model, and SDKs (Python, Go, Java).\\nPrerequisites\\nYou will need to install IntelliJ IDEA and/or PyCharm with the EduTools plugin, and with the kata(s) installed in the language of their choice to work through exercises in the online platform.\\nInstructions can be found on the blogpost here: https://beam.apache.org/blog/beam-kata-release/</td>\n      <td>First Steps with Apache Beam: Writing Portable Pipelines using Java, Python, Go Stream Apache Beam is an open source unified model for defining data processing pipelines (Batch and strEAM), which allows you to write your pipeline in your language of choice and run it with minimal effort on the execution engine (ex: Apache Spark, Apache Flink, Google Cloud Dataflow) of choice.  In this practical session we will get hands-on writing Beam pipelines, and as well as discuss the fundamentals of Beam programming model, and SDKs (Python, Go, Java).\\nPrerequisites\\nYou will need to install IntelliJ IDEA and/or PyCharm with the EduTools plugin, and with the kata(s) installed in the language of their choice to work through exercises in the online platform.\\nInstructions can be found on the blogpost here: https://beam.apache.org/blog/beam-kata-release/</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>2017</td>\n      <td>Using Apache Beam to create a unified benchmarking framework for streaming and batch systems</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Ismaël Mejía</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/using-apache-beam-create-unified-benchmarking-framework-streaming-and-batch-systems.html</td>\n      <td>In the relational database world, there are different benchmarks to evaluate the correctness and performance of different databases, for example the TPC suites, In this talk we will motivate the need for a benchmark framework to evaluate both stream and batch processing systems.\\n\\nWe will introduce Nexmark, a framework to evaluate queries over data streams and discuss its implementation on Apache Beam, and the properties that make Apache Beam the perfect tool to develop a benchmark framework. Nexmark was an integration test donated by Google as part of the Apache Beam incubation process and we have been working to evolve it since. Nexmark not only bridges the gap for evaluating data processing frameworks, but also serves as a rich integration test of the correct implementation of both the Beam runners (for systems like Apache Spark, Apache Flink and Apache Apex), and the new features of the Beam SDK that we will also present.</td>\n      <td>Using Apache Beam to create a unified benchmarking framework for streaming and batch systems Stream In the relational database world, there are different benchmarks to evaluate the correctness and performance of different databases, for example the TPC suites, In this talk we will motivate the need for a benchmark framework to evaluate both stream and batch processing systems.\\n\\nWe will introduce Nexmark, a framework to evaluate queries over data streams and discuss its implementation on Apache Beam, and the properties that make Apache Beam the perfect tool to develop a benchmark framework. Nexmark was an integration test donated by Google as part of the Apache Beam incubation process and we have been working to evolve it since. Nexmark not only bridges the gap for evaluating data processing frameworks, but also serves as a rich integration test of the correct implementation of both the Beam runners (for systems like Apache Spark, Apache Flink and Apache Apex), and the new features of the Beam SDK that we will also present.</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2019</td>\n      <td>Writing a Distributed Ray Tracer with Apache Beam, Abridged</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Robert Burke</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/writing-distributed-ray-tracer-apache-beam-abridged.html</td>\n      <td>Ray Tracing is an embarrassingly parallel way to render high quality images, but distributing work between machines is hard. Apache Beam is a model for efficient distributed data processing. In this talk I map Ray Tracing onto the Apache Beam Go SDK.\\nApache Beam SDKs portably abstract computations and data into DoFns and PCollections, and allow you to construct a graph of how data flows and connects. Then any compatible Runner can optimize that graph for computation, and distribute work to the runners.\\nI'll give a overview of Ray Tracing  and introduce the Apache Beam model, it's history,  demonstrating mapping one to the other. I'll explain the benefits and limits of the model and how to get the most out of your pipelines, and show it running on compatible runners like Apache Flink and Google Cloud Dataflow. Further, I'll show using the same pipeline running in batch and streaming modes, with minimal changes.\\n</td>\n      <td>Writing a Distributed Ray Tracer with Apache Beam, Abridged Stream Ray Tracing is an embarrassingly parallel way to render high quality images, but distributing work between machines is hard. Apache Beam is a model for efficient distributed data processing. In this talk I map Ray Tracing onto the Apache Beam Go SDK.\\nApache Beam SDKs portably abstract computations and data into DoFns and PCollections, and allow you to construct a graph of how data flows and connects. Then any compatible Runner can optimize that graph for computation, and distribute work to the runners.\\nI'll give a overview of Ray Tracing  and introduce the Apache Beam model, it's history,  demonstrating mapping one to the other. I'll explain the benefits and limits of the model and how to get the most out of your pipelines, and show it running on compatible runners like Apache Flink and Google Cloud Dataflow. Further, I'll show using the same pipeline running in batch and streaming modes, with minimal changes.\\n</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>2019</td>\n      <td>Apache Beam pipelines at 100TB+ scale using Apache Spark.</td>\n      <td>Advanced</td>\n      <td>Scale</td>\n      <td>David Moravek</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/apache-beam-pipelines-100tb-scale-using-apache-spark.html</td>\n      <td>At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.\\nThis talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.\\n</td>\n      <td>Apache Beam pipelines at 100TB+ scale using Apache Spark. Scale At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.\\nThis talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.\\n</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>2019</td>\n      <td>Streaming your shared ride</td>\n      <td>Intermediate</td>\n      <td>Stream</td>\n      <td>Thomas Weise</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/streaming-your-shared-ride.html</td>\n      <td>Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.\\nEnablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.\\nTopics covered in this talk include:\\nOverview of use cases and platform architecture\\nStreaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping\\nStateful streaming computation with scalability, high availability and low latency processing on Apache Flink\\nDevelopment frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python\\nPython with Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink\\nKubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications\\n</td>\n      <td>Streaming your shared ride Stream Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.\\nEnablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.\\nTopics covered in this talk include:\\nOverview of use cases and platform architecture\\nStreaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping\\nStateful streaming computation with scalability, high availability and low latency processing on Apache Flink\\nDevelopment frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python\\nPython with Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink\\nKubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications\\n</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>2016</td>\n      <td>Google Dataflow: The new open model for batch and stream processing</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Felipe Hoffa</td>\n      <td>https://2016.berlinbuzzwords.de/session/google-dataflow-new-open-model-batch-and-stream-processing.html</td>\n      <td>In 2004 Google published the MapReduce paper, a programming model that kick-started big data as we know it. Ten years later, Google introduced Dataflow - a new paradigm for big data, integrating batch and stream processing in one common abstraction. This time the offer was more than a paper, but also an open source Java SDK and a cloud managed service to run it. In 2016 big data players like Cask, Cloudera, Data Artisans, PayPal, Slack, Talend joined Google to propose Dataflow for incubation at the Apache Software Foundation - now accepted as Apache Beam. Dataflow is here, not only unifying batch and streaming, but also the big data world.\\nIn this talk we are going to review Dataflow's differentiating elements and why they matter.  We’ll demonstrate Dataflow’s capabilities through a real-time demo with practical insights on how to manage and visualize streaming data flows.\\n</td>\n      <td>Google Dataflow: The new open model for batch and stream processing Scale In 2004 Google published the MapReduce paper, a programming model that kick-started big data as we know it. Ten years later, Google introduced Dataflow - a new paradigm for big data, integrating batch and stream processing in one common abstraction. This time the offer was more than a paper, but also an open source Java SDK and a cloud managed service to run it. In 2016 big data players like Cask, Cloudera, Data Artisans, PayPal, Slack, Talend joined Google to propose Dataflow for incubation at the Apache Software Foundation - now accepted as Apache Beam. Dataflow is here, not only unifying batch and streaming, but also the big data world.\\nIn this talk we are going to review Dataflow's differentiating elements and why they matter.  We’ll demonstrate Dataflow’s capabilities through a real-time demo with practical insights on how to manage and visualize streaming data flows.\\n</td>\n    </tr>\n    <tr>\n      <th>358</th>\n      <td>2016</td>\n      <td>Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark.</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>David Whiting</td>\n      <td>https://2016.berlinbuzzwords.de/session/big-data-small-code-using-java-8-and-apache-crunch-quickly-develop-concise-efficient.html</td>\n      <td>New execution platforms may be popping up all the time with the intention of being the \"hot new thing\" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. Apache Crunch is a simple framework on top of MapReduce (with support for running on Spark as well) which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.\\n</td>\n      <td>Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark. Scale New execution platforms may be popping up all the time with the intention of being the \"hot new thing\" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. Apache Crunch is a simple framework on top of MapReduce (with support for running on Spark as well) which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.\\n</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>2015</td>\n      <td>Hive on Spark</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Szehon Ho</td>\n      <td>https://2015.berlinbuzzwords.de/session/hive-spark.html</td>\n      <td>Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop.  Until recently, MapReduce was the only Hadoop execution engine for Hive queries. But today, alternative execution engines are available — such as Apache Spark and Apache Tez.  The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.eins zwo\\nIn this talk we'll discuss the Hive on Spark project.  Topics include the motivations, such as improving Hive user experience and streamlining operational management for Spark shops, some background and comparisons of MapRededuce and Spark, and the technical process of porting a complex real-world application from MapReduce to Spark.  A demo will also be presented.\\n</td>\n      <td>Hive on Spark Scale Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop.  Until recently, MapReduce was the only Hadoop execution engine for Hive queries. But today, alternative execution engines are available — such as Apache Spark and Apache Tez.  The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.eins zwo\\nIn this talk we'll discuss the Hive on Spark project.  Topics include the motivations, such as improving Hive user experience and streamlining operational management for Spark shops, some background and comparisons of MapRededuce and Spark, and the technical process of porting a complex real-world application from MapReduce to Spark.  A demo will also be presented.\\n</td>\n    </tr>\n    <tr>\n      <th>266</th>\n      <td>2017</td>\n      <td>Distributed and Native Hybrid optimizations for Machine Learning Workloads</td>\n      <td>Intermediate</td>\n      <td>Scale</td>\n      <td>Suneel Marthi</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/distributed-and-native-hybrid-optimizations-machine-learning-workloads.html</td>\n      <td>Data scientists love tools like R and Scikit-Learn, as they offer a convenient and familiar syntax for analysis tasks. However, these systems are limited to operating serially on data sets that can fit on a single node and do not allow for distributed execution.\\nMahout-Samsara is a linear algebra environment that offers both an easy-to-use Scala DSL and efficient distributed execution for linear algebra operations. Data scientists transitioning from R to Mahout can use the Samsara DSL for large-scale data sets with familiar R-like semantics. Machine Learning and Deep Learning algorithms built with the Mahout-Samsara DSL are automatically parallelized and optimized to execute on distributed processing engines like Apache Spark and Apache Flink accelerated natively by CUDA, OpenCL and OpenMP.\\nIn this talk, we will look at Mahout's distributed linear algebra capabilities and demonstrate an EigenFaces classification using Distributed SSVD executing on a GPU cluster. This talk will also demonstrate the ease with which one can roll-out new Machine Learning Algorithms using Mahout-Samsara DSL. \\nML practitioners will come away from this talk with a better understanding of how Samsara's linear algebra environment can help simplify developing highly scalable, CPU/GPU accelerated ML and DL algorithms by focusing solely on the declarative specification of the algorithm without having to worry about the implementation details of a scalable distributed engine or having to learn to program with native math libraries.\\n</td>\n      <td>Distributed and Native Hybrid optimizations for Machine Learning Workloads Scale Data scientists love tools like R and Scikit-Learn, as they offer a convenient and familiar syntax for analysis tasks. However, these systems are limited to operating serially on data sets that can fit on a single node and do not allow for distributed execution.\\nMahout-Samsara is a linear algebra environment that offers both an easy-to-use Scala DSL and efficient distributed execution for linear algebra operations. Data scientists transitioning from R to Mahout can use the Samsara DSL for large-scale data sets with familiar R-like semantics. Machine Learning and Deep Learning algorithms built with the Mahout-Samsara DSL are automatically parallelized and optimized to execute on distributed processing engines like Apache Spark and Apache Flink accelerated natively by CUDA, OpenCL and OpenMP.\\nIn this talk, we will look at Mahout's distributed linear algebra capabilities and demonstrate an EigenFaces classification using Distributed SSVD executing on a GPU cluster. This talk will also demonstrate the ease with which one can roll-out new Machine Learning Algorithms using Mahout-Samsara DSL. \\nML practitioners will come away from this talk with a better understanding of how Samsara's linear algebra environment can help simplify developing highly scalable, CPU/GPU accelerated ML and DL algorithms by focusing solely on the declarative specification of the algorithm without having to worry about the implementation details of a scalable distributed engine or having to learn to program with native math libraries.\\n</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>level</th>\n      <th>track</th>\n      <th>speaker</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>learn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>306</th>\n      <td>2017</td>\n      <td>BM25 is so Yesterday: Modern Techniques for Better Search Relevance</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Grant Ingersoll</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/bm25-so-yesterday-modern-techniques-better-search-relevance.html</td>\n      <td>Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene.  And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance.  This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it.  We’ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr.  We’ll finish with a sneak peak into using deep learning and word2vec in a search context.\\n</td>\n      <td>BM25 is so Yesterday: Modern Techniques for Better Search Relevance Search Modern relevance in search engines has come a long way since the early days of information retrieval, when the likes of TF-IDF and BM25 scoring models first came on the scene.  And while those core models are still good for a first pass retrieval, more and more search engines are employing machine learning, natural language processing and sophisticated re-ranking techniques to fine tune relevance.  This talk will provide a review of current best practices in relevance tuning, including what to measure and how to measure it.  We’ll then give details on how to use techniques like learning to rank and query intent classification to improve results, with examples in Apache Solr.  We’ll finish with a sneak peak into using deep learning and word2vec in a search context.\\n</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>2018</td>\n      <td>Machine Learning for Apache Solr, Elasticsearch &amp; Vespa</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Mitali Jha</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/machine-learning-apache-solr-elasticsearch-vespa.html</td>\n      <td>Apache Solr, Elasticsearch and Vespa are three of the most popular general purpose search engines that are used in search, recommendations and analytics based applications. Machine learning is a critical aid to improving relevance beyond the native ranking algorithms (e.g. TF-IDF, BM25 etc.) by leveraging editorial judgements and user behaviour and factoring them into ranking of results. This is known as \"learning to rank\" (LTR) or \"machine learned ranking\" (MLR) etc.\\nIn this talk, the speaker presents a comparison of machine learning frameworks across all of these search engines [1]. The comparison is followed by a quick demonstration on how to use all of these frameworks. This talk is aimed at those who are looking to decide upon which search engine to use in their applications based on the machine learned ranking capabilities available for it, and the ease of using such features.\\n[1] 1. LTR based re-ranking module in Solr, https://lucene.apache.org/solr/guide/7_2/learning-to-rank.html       2. Elasticsearch's LTR plugin, https://github.com/o19s/elasticsearch-learning-to-rank       3. Vespa's MLR functionality, http://docs.vespa.ai/documentation/ranking.html\\n \\n</td>\n      <td>Machine Learning for Apache Solr, Elasticsearch &amp; Vespa Search Apache Solr, Elasticsearch and Vespa are three of the most popular general purpose search engines that are used in search, recommendations and analytics based applications. Machine learning is a critical aid to improving relevance beyond the native ranking algorithms (e.g. TF-IDF, BM25 etc.) by leveraging editorial judgements and user behaviour and factoring them into ranking of results. This is known as \"learning to rank\" (LTR) or \"machine learned ranking\" (MLR) etc.\\nIn this talk, the speaker presents a comparison of machine learning frameworks across all of these search engines [1]. The comparison is followed by a quick demonstration on how to use all of these frameworks. This talk is aimed at those who are looking to decide upon which search engine to use in their applications based on the machine learned ranking capabilities available for it, and the ease of using such features.\\n[1] 1. LTR based re-ranking module in Solr, https://lucene.apache.org/solr/guide/7_2/learning-to-rank.html       2. Elasticsearch's LTR plugin, https://github.com/o19s/elasticsearch-learning-to-rank       3. Vespa's MLR functionality, http://docs.vespa.ai/documentation/ranking.html\\n \\n</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>2019</td>\n      <td>Why data-driven methods will shape the future of relevance search</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Pedro Balage</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/why-data-driven-methods-will-shape-future-relevance-search.html</td>\n      <td>You already have your search engine in place, users have started using it and now see that your results could do better. You hire some engineers to get you through this and they included a synonym here, an exception there, and asked you to create a taxonomy to organize your items. After a few months,  everything seems to work fine except for a few minor issues that you got your team working on. \\nNow, your company needs to expand to meet the new demands of your business, so you get a wider range of products in your catalog. Your search results start to show some issues and your team is having a hard time controlling all of your search parameters and maintaining rules and exceptions. You know that improving your results will be the key to the success of your company. You now get yourself the biggest decision to make: should I hire more relevance engineers to handle it?\\nThe first part of this talk will show why it is so complicated to scale an engineering approach of relevance based only on synonyms, taxonomies, rules, and exceptions. As a consequence of these limitations, the second part will focus on the latest advancements of natural language processing and information retrieval to show that all you need are good data scientists responsible for providing new data-driven solutions to your needs without causing any change into your search stack. \\nWith the vast amounts of data collected, you can find applications of it for all your search needs. The talk will guide you through topics such as language modeling for autocompletion, deep neural networks for named-entity recognition, network embeddings for relevance score, and query-product embeddings to improve the discoverability of your most exquisite items. All of this, optimized by your learning-to-rank algorithm and enabling you to personalization capabilities.\\nWith all of these data-driven approaches, your search engine will be actively learning from the interaction of your users and there will be no need to hire an entire department of relevance engineers. \\n</td>\n      <td>Why data-driven methods will shape the future of relevance search Search You already have your search engine in place, users have started using it and now see that your results could do better. You hire some engineers to get you through this and they included a synonym here, an exception there, and asked you to create a taxonomy to organize your items. After a few months,  everything seems to work fine except for a few minor issues that you got your team working on. \\nNow, your company needs to expand to meet the new demands of your business, so you get a wider range of products in your catalog. Your search results start to show some issues and your team is having a hard time controlling all of your search parameters and maintaining rules and exceptions. You know that improving your results will be the key to the success of your company. You now get yourself the biggest decision to make: should I hire more relevance engineers to handle it?\\nThe first part of this talk will show why it is so complicated to scale an engineering approach of relevance based only on synonyms, taxonomies, rules, and exceptions. As a consequence of these limitations, the second part will focus on the latest advancements of natural language processing and information retrieval to show that all you need are good data scientists responsible for providing new data-driven solutions to your needs without causing any change into your search stack. \\nWith the vast amounts of data collected, you can find applications of it for all your search needs. The talk will guide you through topics such as language modeling for autocompletion, deep neural networks for named-entity recognition, network embeddings for relevance score, and query-product embeddings to improve the discoverability of your most exquisite items. All of this, optimized by your learning-to-rank algorithm and enabling you to personalization capabilities.\\nWith all of these data-driven approaches, your search engine will be actively learning from the interaction of your users and there will be no need to hire an entire department of relevance engineers. \\n</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>2019</td>\n      <td>Building an AI/ML powered text search system</td>\n      <td>Beginner</td>\n      <td>Search</td>\n      <td>Nick Burch</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/building-aiml-powered-text-search-system.html</td>\n      <td>There are some great Open Source text search / information retrieval systems, such as Apache SOLR and ElasticSearch. But could an AI / ML powered solution do better? And how would you even go about building one?\\nBased on our experiences building a knowledge base / Q&amp;A system, we'll guide you through the process. Learn how to get your text into a format that AI / ML techniques can work on, and how to build a simple model and recommender. Then it's Deep Learning and Neural Networks, and finally updating the models with real user feedback. Oh, and comparing it to a traditional search engine, to see if it's actually any better...\\nNo AI or hard-core-search experience needed, we'll show you the code and techniques required to create your own!\\n</td>\n      <td>Building an AI/ML powered text search system Search There are some great Open Source text search / information retrieval systems, such as Apache SOLR and ElasticSearch. But could an AI / ML powered solution do better? And how would you even go about building one?\\nBased on our experiences building a knowledge base / Q&amp;A system, we'll guide you through the process. Learn how to get your text into a format that AI / ML techniques can work on, and how to build a simple model and recommender. Then it's Deep Learning and Neural Networks, and finally updating the models with real user feedback. Oh, and comparing it to a traditional search engine, to see if it's actually any better...\\nNo AI or hard-core-search experience needed, we'll show you the code and techniques required to create your own!\\n</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2021</td>\n      <td>Beyond Artificial Intelligence for Search</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Lucian Precup</td>\n      <td>https://2021.berlinbuzzwords.de/session/beyond-artificial-intelligence-search</td>\n      <td>It is proven that for relatively well-structured data, like in e-commerce for example, a hand tailored search configuration can easily outperform machine learning approaches for relevance. The search configuration considers the different searchable fields, a business taxonomy and ontology, some domain related synonyms, a few specific landing pages, boosts and some business numerical criteria.\\nIn the same way, we describe an approach for relevance in the case of large-scale search engines which is not based on classical \"PageRank\" and machine learning approaches. We propose a model based on social interactions between communities and individuals that are using or configuring the search engine. We then compare this model with machine learning powered approaches.</td>\n      <td>Beyond Artificial Intelligence for Search Search It is proven that for relatively well-structured data, like in e-commerce for example, a hand tailored search configuration can easily outperform machine learning approaches for relevance. The search configuration considers the different searchable fields, a business taxonomy and ontology, some domain related synonyms, a few specific landing pages, boosts and some business numerical criteria.\\nIn the same way, we describe an approach for relevance in the case of large-scale search engines which is not based on classical \"PageRank\" and machine learning approaches. We propose a model based on social interactions between communities and individuals that are using or configuring the search engine. We then compare this model with machine learning powered approaches.</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>2019</td>\n      <td>Building an enterprise Natural Language Search Engine with ElasticSearch and Facebook’s DrQA</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Debmalya BiswasLouis Baligand</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/building-enterprise-natural-language-search-engine-elasticsearch-and-facebooks-drqa.html</td>\n      <td>Modern search engines leverage Natural Language Processing (NLP) and Machine Learning (ML) to improve relevance of results. In this presentation, we focus on the specific field of ‘Enterprise Search’, whose primary goal is to make domain specific company data and documents readily accessible to employees to improve their productivity and promote collaboration. Indeed, any large organization produces vast amounts of documentation about their specific systems, technologies and processes. The question then is - “How can we speed up search-driven activities and enhance user experience for the enterprise?”\\nFacebook AI research team has developed and open sourced a tool to answer questions by reading Wikipedia articles, called DrQA (Chen, et al. 2017). DrQA is based on a 2-stage Q&amp;A pipeline: (i) Retriever: retrieve the top-k relevant documents (pages), followed by (ii) Reader: determining the most relevant answer span among the retrieved documents (pages). We applied it on an enterprise use-case to search over machine manuals used by factory operators.\\nWe present an architecture integrating ElasticSearch in the DrQA pipeline, which has been contributed upstream and is now available from the official DrQA github repository (https://github.com/facebookresearch/DrQA). The end result is a very scalable search engine that can be deployed on any document repository in your enterprise containing Microsoft Office docs, ppts, emails, pdf documents, etc. Simply point it to your ElasticSearch index and it will be able to provide ‘very precise’ answers based on your documents, thanks to the pre-trained Deep Learning Q&amp;A model of DrQA. We discuss the learnings along the creation and the limitations of such an engine, e.g. scenarios where it excels by identifying precise answers and how it performs compared to a non-ML approach or a typical keyword based search.\\n</td>\n      <td>Building an enterprise Natural Language Search Engine with ElasticSearch and Facebook’s DrQA Search Modern search engines leverage Natural Language Processing (NLP) and Machine Learning (ML) to improve relevance of results. In this presentation, we focus on the specific field of ‘Enterprise Search’, whose primary goal is to make domain specific company data and documents readily accessible to employees to improve their productivity and promote collaboration. Indeed, any large organization produces vast amounts of documentation about their specific systems, technologies and processes. The question then is - “How can we speed up search-driven activities and enhance user experience for the enterprise?”\\nFacebook AI research team has developed and open sourced a tool to answer questions by reading Wikipedia articles, called DrQA (Chen, et al. 2017). DrQA is based on a 2-stage Q&amp;A pipeline: (i) Retriever: retrieve the top-k relevant documents (pages), followed by (ii) Reader: determining the most relevant answer span among the retrieved documents (pages). We applied it on an enterprise use-case to search over machine manuals used by factory operators.\\nWe present an architecture integrating ElasticSearch in the DrQA pipeline, which has been contributed upstream and is now available from the official DrQA github repository (https://github.com/facebookresearch/DrQA). The end result is a very scalable search engine that can be deployed on any document repository in your enterprise containing Microsoft Office docs, ppts, emails, pdf documents, etc. Simply point it to your ElasticSearch index and it will be able to provide ‘very precise’ answers based on your documents, thanks to the pre-trained Deep Learning Q&amp;A model of DrQA. We discuss the learnings along the creation and the limitations of such an engine, e.g. scenarios where it excels by identifying precise answers and how it performs compared to a non-ML approach or a typical keyword based search.\\n</td>\n    </tr>\n    <tr>\n      <th>269</th>\n      <td>2017</td>\n      <td>The modern architecture of search</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Alaa ElhadbaMikio Braun</td>\n      <td>https://2017.berlinbuzzwords.de/17/session/modern-architecture-search.html</td>\n      <td>Information Retrieval (IR) systems are a vital component in the core of successful modern web platforms.\\nThe main goal of IR systems is to provide a communication layer that enables customers to establish a retrieval dialogue with underlying data.\\nThe immense explosion of unstructured data drives modern search application to go beyond just fuzzy string matching, to invest in deep understanding of user queries through interpretation of user intention in order to respond with a relevant result set. \\nThe modern architecture of search is a design of a data-driven IR system that covers the following: \\n     - Data ingestion pipelines from various sources\\n     - Data retrieval and the lifecycle of a user search query\\n     - Machine learned relevance ranking \\n     - Personalised search\\n     - Search performance tracking and quality assessment\\nThe talk will discuss the components needed to build an eco-system that is designed to solve the problems of IR in web platforms. What role can Machine learning play in search relevancy? how natural language processing can help provide a solid understanding of search phrases? How data can drive a personalized search experience? What are the challenges of maintaining such a complex system?  \\n \\n#ml #nlp #ir #search_relevancy #architecture #big_data\\n____\\nAlae Elhadba\\nResearch Engineer @ Zalando \\n</td>\n      <td>The modern architecture of search Search Information Retrieval (IR) systems are a vital component in the core of successful modern web platforms.\\nThe main goal of IR systems is to provide a communication layer that enables customers to establish a retrieval dialogue with underlying data.\\nThe immense explosion of unstructured data drives modern search application to go beyond just fuzzy string matching, to invest in deep understanding of user queries through interpretation of user intention in order to respond with a relevant result set. \\nThe modern architecture of search is a design of a data-driven IR system that covers the following: \\n     - Data ingestion pipelines from various sources\\n     - Data retrieval and the lifecycle of a user search query\\n     - Machine learned relevance ranking \\n     - Personalised search\\n     - Search performance tracking and quality assessment\\nThe talk will discuss the components needed to build an eco-system that is designed to solve the problems of IR in web platforms. What role can Machine learning play in search relevancy? how natural language processing can help provide a solid understanding of search phrases? How data can drive a personalized search experience? What are the challenges of maintaining such a complex system?  \\n \\n#ml #nlp #ir #search_relevancy #architecture #big_data\\n____\\nAlae Elhadba\\nResearch Engineer @ Zalando \\n</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>2018</td>\n      <td>Your Search Service as a Composable Function</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Stefanie SchirmerAakash Sabharwal</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/your-search-service-composable-function-0.html</td>\n      <td>Modern search systems at scale are often architected as a real-time processing pipeline where the query and its results flow through multiple stages before returning them to the user. Concretely, a  search query flowing through this pipeline might be stemmed by a stemmer, tokenized by a \"parts of speech\" natural language parser, transformed to a query plan conforming to a boolean retrieval model, executed against an inverted index,  and finally the top-k results could be reordered based on an online machine-learned ranker. Many of these transformations of queries and results require performing a network IO to external specialized services. Our proposal is to model the search pipeline as a monad transformer composed of Reader monad and a Future.\\n \\nAbout 1500 people search for handmade and vintage items on Etsy every second. Several different backends power Etsy's search, among them Solr, Elasticsearch, our own key-value-store Arizona, and services for machine learning and inference. How do all these systems work together, present a common interface to Etsy's developers and a coherent search experience to our users?\\n \\nThis problem requires a distributed system that scales very well, and has a state space that is still easy to reason about. We have built a smart proxy in scala with minimal state to solve this problem. It expands on the ideas of the \"Your Server as a Function\" paper. The idea is that basically all program state comes in via the request encoded in a Reader monad, the proxy calls out to the appropriate backend services and combines their responses, behaving like a pure function.\\n \\nThe proxy retrieves search results from a Solr backend, ranks them based on a machine learning model that incorporates the user's context, and returns the ranked results. If a search is unsuccessful, it can decide to kick off alternative search requests and return their results instead, without needing any frontend interaction. Via an intermediate tree representation that encodes a boolean retrieval model which is independent of the backends, we can combine key-value-store, machine learning and search indexes to improve the search result for the end user. The interplay of the different backends to create new services is achieved via a configuration system. It makes heavy use of Scala's type system to lift the types from the data fields in the storage backends into the scala code and ensure consistent types from the incoming thrift request throughout the entire system.\\n \\nTwitter’s Finagle library provides us with RPC clients that abstract over different protocols like thrift, mux or http and lets us interact with them in an asynchronous manner via Futures. Using Futures, we can build custom machine learning pipelines by composing sequentially &amp; concurrently over calls to backend services. It becomes easy to add custom query pipelines for machine learning, such as a recommendation query pipeline that queries a user database, gets the user’s purchase history and favorites, and constructs a recommendation set using a model that takes the user context as input. With experimentation as a first class citizen in our design, A/B testing allows us to test different feature sets and modelling techniques in our machine learning services.\\n \\nIn order to maintain independent failure domains for suggestions and search we run the proxy on Kubernetes as separate services. The Kubernetes autoscaler helps us to react to changing traffic patterns in a flexible way. We will talk about our learnings along the way of building this proxy, and trying to find the right abstraction for the search problem.\\n \\n</td>\n      <td>Your Search Service as a Composable Function Search Modern search systems at scale are often architected as a real-time processing pipeline where the query and its results flow through multiple stages before returning them to the user. Concretely, a  search query flowing through this pipeline might be stemmed by a stemmer, tokenized by a \"parts of speech\" natural language parser, transformed to a query plan conforming to a boolean retrieval model, executed against an inverted index,  and finally the top-k results could be reordered based on an online machine-learned ranker. Many of these transformations of queries and results require performing a network IO to external specialized services. Our proposal is to model the search pipeline as a monad transformer composed of Reader monad and a Future.\\n \\nAbout 1500 people search for handmade and vintage items on Etsy every second. Several different backends power Etsy's search, among them Solr, Elasticsearch, our own key-value-store Arizona, and services for machine learning and inference. How do all these systems work together, present a common interface to Etsy's developers and a coherent search experience to our users?\\n \\nThis problem requires a distributed system that scales very well, and has a state space that is still easy to reason about. We have built a smart proxy in scala with minimal state to solve this problem. It expands on the ideas of the \"Your Server as a Function\" paper. The idea is that basically all program state comes in via the request encoded in a Reader monad, the proxy calls out to the appropriate backend services and combines their responses, behaving like a pure function.\\n \\nThe proxy retrieves search results from a Solr backend, ranks them based on a machine learning model that incorporates the user's context, and returns the ranked results. If a search is unsuccessful, it can decide to kick off alternative search requests and return their results instead, without needing any frontend interaction. Via an intermediate tree representation that encodes a boolean retrieval model which is independent of the backends, we can combine key-value-store, machine learning and search indexes to improve the search result for the end user. The interplay of the different backends to create new services is achieved via a configuration system. It makes heavy use of Scala's type system to lift the types from the data fields in the storage backends into the scala code and ensure consistent types from the incoming thrift request throughout the entire system.\\n \\nTwitter’s Finagle library provides us with RPC clients that abstract over different protocols like thrift, mux or http and lets us interact with them in an asynchronous manner via Futures. Using Futures, we can build custom machine learning pipelines by composing sequentially &amp; concurrently over calls to backend services. It becomes easy to add custom query pipelines for machine learning, such as a recommendation query pipeline that queries a user database, gets the user’s purchase history and favorites, and constructs a recommendation set using a model that takes the user context as input. With experimentation as a first class citizen in our design, A/B testing allows us to test different feature sets and modelling techniques in our machine learning services.\\n \\nIn order to maintain independent failure domains for suggestions and search we run the proxy on Kubernetes as separate services. The Kubernetes autoscaler helps us to react to changing traffic patterns in a flexible way. We will talk about our learnings along the way of building this proxy, and trying to find the right abstraction for the search problem.\\n \\n</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>2019</td>\n      <td>Evolution of Yelp search to a ranking platform</td>\n      <td>Advanced</td>\n      <td>Search</td>\n      <td>Umesh Dangat</td>\n      <td>https://2019.berlinbuzzwords.de/19/session/evolution-yelp-search-ranking-platform.html</td>\n      <td>Yelp’s core business search was one of the oldest systems at Yelp designed well before the advent of Elasticsearch. While this system served Yelp well for a few years, we were getting close to a point where the original search infrastructure architecture was not sufficient to solve modern day search problems.\\nThis talk will detail\\nHow Yelp’s search engineers decoupled the search infrastructure from search relevance.\\nThe challenges associated with transferring complex custom lucene based ranking and text analysis functionality to elasticsearch.\\nThe benefits of offloading search infrastructure to elasticsearch and\\nFinally the dividends it paid by allowing us to further leverage our technological investment in elasticsearch, by hosting machine learning models in elasticsearch by using Learning to Rank plugin, and making contributions to it.\\n</td>\n      <td>Evolution of Yelp search to a ranking platform Search Yelp’s core business search was one of the oldest systems at Yelp designed well before the advent of Elasticsearch. While this system served Yelp well for a few years, we were getting close to a point where the original search infrastructure architecture was not sufficient to solve modern day search problems.\\nThis talk will detail\\nHow Yelp’s search engineers decoupled the search infrastructure from search relevance.\\nThe challenges associated with transferring complex custom lucene based ranking and text analysis functionality to elasticsearch.\\nThe benefits of offloading search infrastructure to elasticsearch and\\nFinally the dividends it paid by allowing us to further leverage our technological investment in elasticsearch, by hosting machine learning models in elasticsearch by using Learning to Rank plugin, and making contributions to it.\\n</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>2018</td>\n      <td>Head-N-Tail Analysis to Increase Engagement</td>\n      <td>Intermediate</td>\n      <td>Search</td>\n      <td>Amrit Sarkar</td>\n      <td>https://2018.berlinbuzzwords.de/18/session/head-n-tail-analysis-increase-engagement.html</td>\n      <td>Search engines have come a long way since the early days of reliance on information retrieval as user intent drives modern search applications.  Businesses driven by search like eCommerce, content-based companies lose a small but decent amount of traffic to queries getting a lower number of impressions/clicks/conversions as few or irrelevant results are computed and shown. This session focus on identifying poor/low performing queries and terms using Head and Tail Analysis, classify with potential reasons and suggest corrections or improved query models for them. The session concludes with emphasizing on experiments being vital to the entire process of improving search relevance. The session is based on Head-n-Tail Analysis in Fusion 4 by Chao Han, VP Head of Research, Lucidworks.\\nThis talk is presented by Lucidworks.</td>\n      <td>Head-N-Tail Analysis to Increase Engagement Search Search engines have come a long way since the early days of reliance on information retrieval as user intent drives modern search applications.  Businesses driven by search like eCommerce, content-based companies lose a small but decent amount of traffic to queries getting a lower number of impressions/clicks/conversions as few or irrelevant results are computed and shown. This session focus on identifying poor/low performing queries and terms using Head and Tail Analysis, classify with potential reasons and suggest corrections or improved query models for them. The session concludes with emphasizing on experiments being vital to the entire process of improving search relevance. The session is based on Head-n-Tail Analysis in Fusion 4 by Chao Han, VP Head of Research, Lucidworks.\\nThis talk is presented by Lucidworks.</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623927561363
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to visualise the ~50 clusters in 2 dimensions\n",
        "\n",
        "def make_tsne():\n",
        "   tsne_init = 'pca'  # could also be 'random'\n",
        "   tsne_perplexity = 20.0\n",
        "   tsne_early_exaggeration = 4.0\n",
        "   tsne_learning_rate = 1000\n",
        "   random_state = 1\n",
        "   return TSNE(n_components=2, random_state=random_state, init=tsne_init, perplexity=tsne_perplexity,\n",
        "               early_exaggeration=tsne_early_exaggeration, learning_rate=tsne_learning_rate)\n",
        "\n",
        "def visualise_km_tsne(centroids):\n",
        "   model = make_tsne()\n",
        "   transformed_centroids = model.fit_transform(centroids)\n",
        "   fig = plt.figure(figsize=(15,9))\n",
        "   plt.scatter(transformed_centroids[:, 0], transformed_centroids[:, 1], marker='x')\n",
        "   plt.show()\n",
        "\n",
        "visualise_km_tsne(best_km.km.cluster_centers_)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# How does that clustering compare to the original TF-IDF data?\n",
        "def visualise_tfidf_tsne(tfidf_matrix):\n",
        "   # Build a TSVD reducer, to get us down to ~50 dimensions\n",
        "   reducer = TruncatedSVD(n_components=50, random_state=0)\n",
        "   print(tfidf_matrix.shape)\n",
        "   tfidf_reduced = reducer.fit_transform(tfidf_matrix)\n",
        "   print(tfidf_reduced.shape)\n",
        "\n",
        "   # Use t-SNE to get down to 2 dimensions\n",
        "   tfidf_emb = make_tsne().fit_transform(tfidf_reduced)\n",
        "\n",
        "   # Plot it\n",
        "   fig = plt.figure(figsize=(15, 10))\n",
        "   ax = plt.axes(frameon=False)\n",
        "   ax.set(title=\"t-SNE visualisation of raw TF-IDF\")\n",
        "   plt.setp(ax, xticks=(), yticks=())\n",
        "   plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9,\n",
        "                       wspace=0.0, hspace=0.0)\n",
        "   plt.scatter(tfidf_emb[:, 0], tfidf_emb[:, 1], c=talk_clusters, marker=\"x\")\n",
        "   plt.show()\n",
        "\n",
        "visualise_tfidf_tsne(tfidf_word.matrix)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Try using OPICS (Ordering Points To Identify the Clustering Structure)\n",
        "#  to try to identify talk clusters\n",
        "# Will need manual review to help identify the hyperparameters, especially\n",
        "#  around the cluster difference\n",
        "# Once clusters, then pull out the top words for each cluster, identify\n",
        "#  likely talks from clusters etc\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# While we could go further with SciKitLearn, it's time to \n",
        "#  swap over to Apache MXNet!"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}