{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Setup step - load all our libraries\n",
        "from mxnet import nd\n",
        "from mxnet.contrib.text import embedding\n",
        "import random\n",
        "import json"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take some time on the first run, as it downloads the\n",
        "#  pre-trained embedding\n",
        "# We use a smaller pre-trained model for speed, you may want to use \n",
        "#  the larger default by skipping the pretrained_file_name option\n",
        "print(\"Loading GloVe embeddings\")\n",
        "glove = embedding.GloVe(pretrained_file_name='glove.6B.50d.txt')\n",
        "print(\"GloVe loaded, contains %d terms\" % len(glove))\n",
        "print(\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the embeddings\n",
        "\n",
        "# For finding cosine-similar embeddings\n",
        "def find_nearest(vectors, wanted, num):\n",
        "    # 1e-9 factor is to avoid zero/negative numbers\n",
        "    cos = nd.dot(vectors, wanted.reshape((-1,))) / (\n",
        "            (nd.sum(vectors * vectors, axis=1) + 1e-9).sqrt() * \n",
        "            nd.sum(wanted * wanted).sqrt())\n",
        "    top_n = nd.topk(cos, k=num, ret_typ='indices').asnumpy().astype('int32')\n",
        "    return top_n, [cos[i].asscalar() for i in top_n]\n",
        "\n",
        "# Looking up some similar words\n",
        "def print_similar_tokens(query_token, num, embed):\n",
        "    top_n, cos = find_nearest(embed.idx_to_vec,\n",
        "                         embed.get_vecs_by_tokens([query_token]), num+1)\n",
        "    print(\"Similar tokens to: %s\" % query_token)\n",
        "    for i, c in zip(top_n[1:], cos[1:]):  # Skip the word itself\n",
        "        print(' - Cosine sim=%.3f: %s' % (c, (embed.idx_to_token[i])))\n",
        "\n",
        "print_similar_tokens(\"search\", 3, glove)\n",
        "print_similar_tokens(\"linux\", 3, glove)\n",
        "print_similar_tokens(\"gpl\", 3, glove)\n",
        "print(\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking up word relationships, to verify the embeddings are working\n",
        "def get_analogy(token_a, token_b, token_c, embed):\n",
        "    vecs = embed.get_vecs_by_tokens([token_a, token_b, token_c])\n",
        "    x = vecs[1] - vecs[0] + vecs[2]\n",
        "    topk, cos = find_nearest(embed.idx_to_vec, x, 1)\n",
        "    return embed.idx_to_token[topk[0]]  # Remove unknown words\n",
        "def print_analogy(token_a, token_b, token_c, embed):\n",
        "    anal = get_analogy('berlin','germany','paris', embed)\n",
        "    print(\"The analogy for %s -> %s of %s is %s\" % \n",
        "                                (token_a, token_b, token_c, anal))\n",
        "\n",
        "print_analogy('berlin','germany','paris', glove)\n",
        "print(\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our talks\n",
        "years = (2021,2020,2019,2018,2017,2016,2015)\n",
        "talks = []\n",
        "for year in years:\n",
        "    filename = \"data/%d/sessions.json\" % year\n",
        "    print(\"Loading %s\" % filename)\n",
        "    with open(filename) as f:\n",
        "       d = json.load(f)\n",
        "       for talk in d:\n",
        "            talk[\"year\"] = year\n",
        "            talks.append(talk)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For each talk title, what are the key words  based on the embedding?\n",
        "# Project the title through the embedding space, and see what words are\n",
        "#  by where we end up\n",
        "# Just run for the first few talks to demo\n",
        "for talk in talks[:15]:\n",
        "    title = talk[\"title\"]\n",
        "\n",
        "    # Get the vectors for each word in the title\n",
        "    # TODO Handle unknown words better\n",
        "    # TODO Tokenize better\n",
        "    title_vectors = glove.get_vecs_by_tokens(title.split(\" \"))\n",
        "    num_words = title_vectors.shape[0]\n",
        "\n",
        "    # Project the title text through the space, and renormalise\n",
        "    # Is mean the best? Checking some research papers recommended...\n",
        "    overall = title_vectors.mean(0)\n",
        "\n",
        "    # What words are near there?\n",
        "    nearby_count = 5\n",
        "    topk, cos = find_nearest(glove.idx_to_vec, overall, nearby_count)\n",
        "    nearby_words = [glove.idx_to_token[idx] for idx in topk]\n",
        "    print(talk)\n",
        "    print(nearby_words)\n",
        "    print(\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text into vectors using the embeddings\n",
        "# Any words not known by the embedding are ignored\n",
        "# For now, just use the titles\n",
        "#\n",
        "# Notes for future improvements\n",
        "#  - Ideally we should weight the words, eg with a TF-IDF approach\n",
        "#  - Is a mean of the embedding vectors really the right way to \n",
        "#    combine? Further research needed\n",
        "#  - We should include the abstract as well as the title\n",
        "title_vectors = nd.zeros( (len(talks), glove.vec_len) )\n",
        "for idx, talk in enumerate(talks):\n",
        "    tokens = talk[\"title\"].split()\n",
        "    token_vectors = glove.get_vecs_by_tokens(tokens)\n",
        "    title_vectors[idx] = token_vectors.sum(0)\n",
        "print(title_vectors)\n",
        "print()\n",
        "\n",
        "\n",
        "# For a given talk, what other talks are nearby?\n",
        "wanted = random.randint(0, len(talks)-1)\n",
        "print(\"Talks with similar embeddings to talk %d\" % wanted)\n",
        "print(talks[wanted])\n",
        "\n",
        "similar, scores = find_nearest(title_vectors, title_vectors[wanted], 5)\n",
        "for num, talk_idx in enumerate(similar):\n",
        "   if talk_idx == wanted:\n",
        "      continue\n",
        "   talk = talks[talk_idx]\n",
        "   print(\"%d%% - %d - %s\" % (scores[num]*100, talk[\"year\"], talk[\"title\"]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TSNE of talk titles\n",
        "# TODO\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Now, you're ready to move onto more advanced things!\n",
        "# Take a look at\n",
        "#  ELMo - https://nlp.gluon.ai/examples/sentence_embedding/elmo_sentence_representation.html\n",
        "#  BERT - https://nlp.gluon.ai/examples/sentence_embedding/bert.html\n",
        "# And start reading some NLP scientific papers!"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}